<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr Gbètoton Nadège Djossou (nadeged2001@yahoo.fr)">

<title>Modélisation d’une variable quantitative - Cours d’économétrie</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Modélisation d’une variable quantitative</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./MOD1_Yquanti_001.html">Introduction</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./MOD1_Yquanti_004.html" aria-current="page">Cours d’économétrie</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./MOD1_Yquanti_002.html">Application n°1</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./MOD1_Yquanti_003.html">Application n°2</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapitre-1-généralité-sur-léconométrie" id="toc-chapitre-1-généralité-sur-léconométrie" class="nav-link active" data-scroll-target="#chapitre-1-généralité-sur-léconométrie">Chapitre 1&nbsp;: Généralité sur l’économétrie</a>
  <ul class="collapse">
  <li><a href="#définition-et-objet-de-léconométrie" id="toc-définition-et-objet-de-léconométrie" class="nav-link" data-scroll-target="#définition-et-objet-de-léconométrie">Définition et objet de l’économétrie</a></li>
  <li><a href="#méthodologie-de-léconométrie" id="toc-méthodologie-de-léconométrie" class="nav-link" data-scroll-target="#méthodologie-de-léconométrie">Méthodologie de l’économétrie</a></li>
  </ul></li>
  <li><a href="#chapitre-2-introduction-à-lanalyse-de-régression-linéaire" id="toc-chapitre-2-introduction-à-lanalyse-de-régression-linéaire" class="nav-link" data-scroll-target="#chapitre-2-introduction-à-lanalyse-de-régression-linéaire">Chapitre 2&nbsp;: Introduction à l’analyse de régression linéaire</a>
  <ul class="collapse">
  <li><a href="#relations-statistiques-versus-relations-déterministes" id="toc-relations-statistiques-versus-relations-déterministes" class="nav-link" data-scroll-target="#relations-statistiques-versus-relations-déterministes">Relations statistiques versus relations déterministes</a></li>
  <li><a href="#régression-versus-corrélation" id="toc-régression-versus-corrélation" class="nav-link" data-scroll-target="#régression-versus-corrélation">Régression versus corrélation</a></li>
  <li><a href="#présentation-du-modèle-de-régression-linéaire" id="toc-présentation-du-modèle-de-régression-linéaire" class="nav-link" data-scroll-target="#présentation-du-modèle-de-régression-linéaire">Présentation du modèle de régression linéaire</a></li>
  <li><a href="#fonction-de-régression-de-léchantillon-fre-et-fonction-de-régression-de-la-population-frp" id="toc-fonction-de-régression-de-léchantillon-fre-et-fonction-de-régression-de-la-population-frp" class="nav-link" data-scroll-target="#fonction-de-régression-de-léchantillon-fre-et-fonction-de-régression-de-la-population-frp">Fonction de régression de l’échantillon (FRE) et fonction de régression de la population (FRP)</a></li>
  </ul></li>
  <li><a href="#chapitre-3-estimation-des-modèles-de-régression-linéaires" id="toc-chapitre-3-estimation-des-modèles-de-régression-linéaires" class="nav-link" data-scroll-target="#chapitre-3-estimation-des-modèles-de-régression-linéaires">Chapitre 3&nbsp;: Estimation des modèles de régression linéaires</a>
  <ul class="collapse">
  <li><a href="#lestimation-par-la-méthode-des-moindres-carrés-ordinaires-mco" id="toc-lestimation-par-la-méthode-des-moindres-carrés-ordinaires-mco" class="nav-link" data-scroll-target="#lestimation-par-la-méthode-des-moindres-carrés-ordinaires-mco">L’estimation par la méthode des moindres carrés ordinaires (MCO)</a></li>
  <li><a href="#la-méthode-du-maximum-de-vraisemblance" id="toc-la-méthode-du-maximum-de-vraisemblance" class="nav-link" data-scroll-target="#la-méthode-du-maximum-de-vraisemblance">La méthode du maximum de vraisemblance</a></li>
  </ul></li>
  <li><a href="#chapitre-4-inférence-statistique" id="toc-chapitre-4-inférence-statistique" class="nav-link" data-scroll-target="#chapitre-4-inférence-statistique">Chapitre 4&nbsp;: Inférence Statistique</a>
  <ul class="collapse">
  <li><a href="#tests-dhypothèses-sur-les-paramètres-de-la-population-test-de-student" id="toc-tests-dhypothèses-sur-les-paramètres-de-la-population-test-de-student" class="nav-link" data-scroll-target="#tests-dhypothèses-sur-les-paramètres-de-la-population-test-de-student">Tests d’hypothèses sur les paramètres de la population&nbsp;: test de Student</a></li>
  <li><a href="#intervalle-de-confiance-des-estimateurs-des-mco" id="toc-intervalle-de-confiance-des-estimateurs-des-mco" class="nav-link" data-scroll-target="#intervalle-de-confiance-des-estimateurs-des-mco">Intervalle de confiance des estimateurs des MCO</a></li>
  <li><a href="#la-prévision" id="toc-la-prévision" class="nav-link" data-scroll-target="#la-prévision">La prévision</a></li>
  <li><a href="#exercices" id="toc-exercices" class="nav-link" data-scroll-target="#exercices">Exercices</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Cours d’économétrie</h1>
<p class="subtitle lead">Cours dispensé aux étudiants de L2 de l’école nationale de statistoques, de planification et de démographe (ENSPD, Parakou, Bénin)</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dr Gbètoton Nadège Djossou <a href="mailto:nadeged2001@yahoo.fr"><em>(nadeged2001@yahoo.fr)</em></a> </p>
          </div>
  </div>
    
    
  </div>
  

</header>

<section id="chapitre-1-généralité-sur-léconométrie" class="level2">
<h2 class="anchored" data-anchor-id="chapitre-1-généralité-sur-léconométrie">Chapitre 1&nbsp;: Généralité sur l’économétrie</h2>
<section id="définition-et-objet-de-léconométrie" class="level3">
<h3 class="anchored" data-anchor-id="définition-et-objet-de-léconométrie">Définition et objet de l’économétrie</h3>
<p>L’objet de la théorie économique est d’expliquer les comportements économiques au travers de modèles décrivant des relations entre des variables économiques : consommation, épargne, revenu, salaire, production, prix, emploi, investissement, taux d’intérêt, etc.</p>
<p>Littéralement, <em>l’économétrie</em> signifie «&nbsp;mesure de l’économie&nbsp;». Ainsi, l’économétrie est souvent décrite comme la partie de l’économie qui s’occupe de la mesure, du quantitatif. C’est une branche de l’économie qui traite de l’estimation pratique des relations économiques.</p>
<p>L’économétrie exprime quantitativement les corrélations pouvant exister entre des phénomènes économiques dont la théorie affirme l’existence. <em>La théorie économique</em> fournit des idées sur les processus qui déterminent les grandeurs économiques. <em>L’économétrie</em> apporte une vérification empirique et établit quantitativement les corrélations qui apparaissent valides. Elle est, d’une part, un outil à la disposition de l’économiste qui lui permet d’infirmer ou de confirmer les théories qu’il construit et d’autre part, elle est à la croisée de l’économie, des mathématiques et des statistiques.</p>
</section>
<section id="méthodologie-de-léconométrie" class="level3">
<h3 class="anchored" data-anchor-id="méthodologie-de-léconométrie">Méthodologie de l’économétrie</h3>
<p>Comment les économétriciens procèdent-ils dans l’analyse d’un problème économique ? Quelle est la méthodologie (démarche) économétrique&nbsp;? Bien qu’il existe plusieurs courants de pensée sur la méthodologie économétrique, nous présentons ici la méthodologie traditionnelle ou classique, qui domine toujours la recherche empirique en économie et dans d’autres sciences sociales et du comportement.</p>
<p>De manière générale, la méthodologie économétrique traditionnelle suit les étapes suivantes&nbsp;:</p>
<ul>
<li><p>Enoncé de la théorie ou d’une hypothèse</p></li>
<li><p>Spécification du modèle mathématique de la théorie</p></li>
<li><p>Spécification du modèle statistique ou économétrique</p></li>
<li><p>Obtention des données</p></li>
<li><p>Estimation des paramètres du modèle économétrique</p></li>
<li><p>Test d’hypothèses</p></li>
<li><p>Prévision</p></li>
<li><p>Utilisation du modèle à des fins de contrôle ou de stratégie</p></li>
</ul>
<p>Examinons les différentes étapes à suivre lors de la construction d’un modèle, ceci à partir de l’exemple du modèle keynésien simplifie.</p>
<section id="enoncé-de-la-théorie-ou-dune-hypothèse" class="level4">
<h4 class="anchored" data-anchor-id="enoncé-de-la-théorie-ou-dune-hypothèse">Enoncé de la théorie ou d’une hypothèse</h4>
<p>Les théories sont des raisonnements destinés à donner une représentation des liens entre les variables économiques. Elles sont souvent fondées sur des hypothèses.</p>
<p>Par exemple, dans la théorie keynésienne, la loi psychologique fondamentale stipule que «&nbsp;en moyenne et la plupart du temps les hommes tendent à accroitre leur consommation à mesure que leur revenu croît, mais cet accroissement est moins que proportionnelle à celui du revenu&nbsp;». En résumé, Keynes a postulé que la propension marginale à consommer pour une unité de revenu est supérieur à zéro mais inférieur à 1.</p>
<p>Plus ou moins contraignantes, les hypothèses sont des simplifications de la réalité destinées à rendre possible la formulation de théories compréhensibles et utilisables. Ainsi, pour analyser la production, on peut partir de l’hypothèse que les entreprises recherchent la maximisation de leur profit, même si cela n’est pas absolument vrai pour toutes les entreprises à tout moment. L’important est que le modèle bâti explique correctement les décisions des entreprises en matière de production.</p>
</section>
<section id="spécification-du-modèle-mathématique-de-la-théorie" class="level4">
<h4 class="anchored">Spécification du modèle mathématique de la théorie</h4>
<p><em>Un modèle est un ensemble de lois et d’hypothèses donnant une représentation théorique des mécanismes économiques.</em> Dans le cadre de l’économétrie, nous pouvons considérer qu’un modèle consiste en une présentation formalisée et simplifiée d’un phénomène sous forme d’équations mathématiques dont les variables sont des grandeurs économiques.</p>
<p>L’objectif du modèle est de représenter les traits les plus marquants d’une réalité qu’il cherche à styliser. Le modèle est donc l’outil que le modélisateur utilise lorsqu’il cherche à comprendre et à expliquer des phénomènes. Pour ce faire, il émet des hypothèses et explicite des relations.</p>
<p>Par exemple, bien que Keynes ait postulé une relation positive entre consommation et revenu, il n’a pas précisé la forme exacte de la relation fonctionnelle entre les deux. Ainsi, bien que des considérations d’ordre théorique nous renseignent sur le signe des dérivées, il existe une multitude de fonctions de formes très différentes et ayant des signes identiques, par exemple <span class="math inline">\(C = a_{0} + a_{1}Y\)</span> et <span class="math inline">\(C = a_{0} + a_{0}Y^{a_{1}}\)</span></p>
<p>Cependant ces deux relations ne reflètent pas le même comportement. Une augmentation du revenu provoque un accroissement proportionnel pour la première relation, alors que, dans la seconde, l’effet s’estompe avec l’augmentation du revenu (<span class="math inline">\(a_{1} &lt; 1\)</span>).</p>
<p>Nous appelons <strong><em>forme fonctionnelle</em></strong> le choix (arbitraire ou fondé) de spécification précise du modèle.</p>
<p>Un économiste pourrait suggérer la forme suivante de la fonction de consommation keynésienne :</p>
<p><span class="math display">\[C = a_{0} + a_{1}Y\text{\ }\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{(1)}\]</span></p>
<p><span class="math display">\[\text{\ }a_{0} &gt; 0\text{\ et\ 0} &lt; a_{1} &lt; 1\]</span></p>
<p><span class="math inline">\(C\)</span> la consommation, <span class="math inline">\(Y\)</span> le revenu. Le coefficient de la pente <span class="math inline">\(a_{1}\)</span> représente la propension marginale à consomme et <span class="math inline">\(a_{0}\)</span> la consommation incompressible. <span class="math inline">\(a_{0}\)</span> et <span class="math inline">\(a_{1}\)</span> sont les paramètres du modèle qui représentent respectivement les coefficients d’interception et de la pente.</p>
<p>L’équation (1) indique que la consommation est liée linéairement au revenu. C’est un exemple de modèle mathématique de la relation entre la consommation et le revenu. Cette relation est appelée <em>fonction de consommation</em> en économie.</p>
<p><strong><em>Un modèle</em></strong> est donc simplement un ensemble d’équations mathématiques.</p>
<ul>
<li><p>Si le modèle n’a qu’une équation, comme dans l’exemple ci-dessus, on parle de <em>modèle à équation unique</em>.</p></li>
<li><p>Si par contre, il a plus d’une équation, il s’agit d’un <em>modèle à équations multiples</em>.</p>
<ol type="1">
<li><h3 id="formalisation-du-modèle-statistique-ou-économétrique" class="anchored" data-anchor-id="spécification-du-modèle-mathématique-de-la-théorie">Formalisation du modèle statistique ou économétrique</h3></li>
</ol></li>
</ul>
<p>Le modèle purement mathématique de la fonction de consommation donné dans l’équation (1) présente un intérêt limité pour l’économètre, car il assure qu’il existe une relation exacte ou déterministe entre consommation et revenu. Mais les relations entre les variables économiques sont généralement inexactes. Ainsi, si nous devions obtenir des données sur les dépenses de consommation et le revenu disponible (c’est-à-dire après impôt) d’un échantillon de 500 familles béninoises, par exemple, et tracer ces données sur un graphique, avec les dépenses de consommation sur l’axe vertical et le revenu disponible sur l’axe horizontal, nous ne nous attendions pas à ce que les 500 observations se situent exactement sur la droite de l’équation (1) ci-dessus.</p>
<p><span class="math display">\[\lbrack Inserer\ un\ graphique\rbrack\]</span></p>
<p>En effet, outre le revenu, d’autres variables affectent les dépenses de consommation. Par exemple, la taille de la famille, l’âge des membres de la famille, la religion de la famille, etc. sont susceptibles d’exercer une influence sur la consommation.</p>
<p>Pour tenir compte des relations inexactes entre les variables économiques, l’économètre modifierait la fonction de consommation déterministe dans l’équation (1) comme suit&nbsp;:</p>
<p><span class="math display">\[C = a_{0} + a_{1}Y + \varepsilon\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{(2)}\]</span></p>
<p>Où <span class="math inline">\(\varepsilon\)</span>, appelé le <strong>terme de perturbation</strong> ou d’<strong>erreur</strong>, est une variable aléatoire (stochastique) qui possède des propriétés probabilistes bien définies. Le terme d’erreur <span class="math inline">\(\varepsilon\)</span> peut très bien représenter tous les facteurs qui influent sur la consommation mais ne sont pas explicitement pris en compte dans le modèle.</p>
<p>L’équation (2) est un exemple de <strong><em>modèle économétrique</em></strong>. Plus techniquement, il s’agit d’un exemple de modèle de régression linéaire que nous étudierons dans ce cours.</p>
</section>
<section id="obtention-des-données" class="level4">
<h4 class="anchored" data-anchor-id="obtention-des-données">Obtention des données</h4>
<p>Pour estimer le modèle économétrique de l’équation (2), c’est-à-dire obtenir les valeurs numériques de <span class="math inline">\(a_{0}\)</span> et <span class="math inline">\(a_{1}\)</span>, nous avons besoin de données.</p>
<section id="les-données" class="level5">
<h5 class="anchored">Les données</h5>
<p>Les données sont utilisées pour analyser le modèle économétrique et donner des recommandations de politique économique. Nous distinguons plusieurs types de données selon que le modèle est spécifié en&nbsp;:</p>
<ul>
<li><p><strong>Série temporelle</strong>&nbsp;: il s’agit des données sur des variables observées a intervalle de temps réguliers. Elles sont indicées par le temps&nbsp;: <span class="math inline">\(y_{t}\)</span>. Par exemple le Produit Intérieur Brut (PIB) du Bénin exprimé en FCFA sur une période de 20 ans.</p></li>
<li><p><strong>Coupe instantanée</strong>&nbsp;: les données sont observées au même instant et concernent les valeurs prises par la variable pour un groupe d’individus spécifiques&nbsp;: <span class="math inline">\(y_{i}\)</span>. Leur indice correspond à l’identifiant d’un individu ou d’une entreprise. Par exemple la consommation en riz d’un échantillon d’étudiant en 2018.</p></li>
<li><p><strong>Panel</strong>&nbsp;: il s’agit de données sur des variables représentant les valeurs prises par un échantillon d’individus à intervalle régulier. Ces variables sont notées&nbsp;: <span class="math inline">\(y_{it}\)</span>. On dispose d’informations sur les individus <span class="math inline">\(i = 1,...,N\)</span> que l’on suit sur plusieurs période, <span class="math inline">\(t = 1,...,T\)</span>. Par exemple, la consommation d’un échantillon de ménages de la ville de Parakou de 2010 à 2020, le PIB des pays de l’UEMOA de 2000 à 2020, etc.</p></li>
<li><p><strong>Cohorte</strong>&nbsp;: très proche des données de panel, les données de cohorte se distinguent de la précédente par la constante de l’échantillon. Les individus sondés sont les mêmes d’une période sur l’autre.</p>
<ol type="1">
<li><h4 id="les-variables" class="anchored" data-anchor-id="les-données">Les variables</h4></li>
</ol></li>
</ul>
<p>L’analyse économétrique d’un ensemble de données a, dans la grande majorité des cas, pour objectif de tester la validité et d’évaluer l’ampleur des explications fournies par l’analyse économique. A ce titre, elle s’intéresse donc à l’effet d’un ensemble de variables, dites <strong>variables explicatives</strong> et notées <span class="math inline">\(X\)</span> sur une ou plusieurs autres variables appelées <strong>variables expliquées</strong>, <span class="math inline">\(Y\)</span>. Le choix de ces variables et leur rôle dans le modèle économétrique est déduit de l’analyse économique du problème auquel on s’intéresse.</p>
<p>Une même variable peut ainsi jouer le rôle de variable expliquée dans un modèle économétrique donnée et le rôle de variable explicative dans un modèle différent. Par exemple, l’éducation dans un modèle d’investissement en capital humain est une variable expliquée tandis que dans un modèle formation des salaires, elle est une variable explicative.</p>
<p>La théorie suggère ainsi une relation de causalité spécifique au problème considéré entre les variables auxquelles on s’intéresse. Pour cette raison, la variable expliquée est également souvent qualifiée de <strong>dépendante</strong> ou <strong>endogène</strong>, au sens où une relation causale la lie aux variables explicatives considérées. Les variables explicatives sont encore qualifiées de variables <strong>indépendantes</strong> ou <strong>exogènes</strong>, au sens où elles peuvent être considéré comme des données (connues) dans le cadre du problème auquel on s’intéresse.</p>
</section>
</section>
<section id="estimation-du-modèle-économétrique" class="level4">
<h4 class="anchored" data-anchor-id="estimation-du-modèle-économétrique">Estimation du modèle économétrique</h4>
<p>Dans l’exemple du modèle keynésien, les estimations numériques des paramètres peuvent donner un contenu empirique à la fonction de consommation. Il existe plusieurs mécanismes d’estimation des paramètres des modèles économiques dont la technique statistique de l’analyse de régression linéaire qui fera l’objet de ce cours.</p>
<p>Par exemple, en supposant que les données collectées ont été soumises à un calcul, nous obtenons les estimations suivantes de <span class="math inline">\(a_{0}\)</span> et <span class="math inline">\(a_{1}\)</span>, à savoir de <span class="math inline">\(144.06\)</span> et <span class="math inline">\(0.8262\)</span>. Ainsi, la fonction de consommation estimée est&nbsp;:</p>
<p><span class="math display">\[\widehat{C} = 144.06 + 0.8262Y\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{(3)}\]</span></p>
<p>Le chapeau sur le <span class="math inline">\(C\)</span> indique qu’il s’agit d’une estimation.</p>
</section>
<section id="test-dhypothèse" class="level4">
<h4 class="anchored" data-anchor-id="test-dhypothèse">Test d’hypothèse</h4>
<p>En supposant que le modèle ajusté soit une approximation raisonnable de la réalité, nous devons élaborer des critères appropriés pour déterminer si les estimations obtenues dans l’équation (3) sont conformes aux attentes de la théorie économique.</p>
<p>Selon la théorie keynésienne, la propension marginale à consommer est positive et inférieure à 1. Dans l’équation (3), la propension marginale à consommer est égale à 0,83. Cependant, avant d’accepter cette estimation comme confirmation de la théorie de la consommation keynésienne, nous devons nous demander si cet estimateur est suffisamment inférieur à l’unité pour nous convaincre qu’il ne s’agit pas d’un événement fortuit ou d’une particularité des données utilisées.</p>
<p>Si, 0,83 est statistiquement inférieur à 1 alors, cela peut conforter la théorie keynésienne. Ce type de confirmation ou de réfutation des théories économiques sur la base d’échantillons repose sur une branche de la théorie statistique appelée inférence statistique (test d’hypothèse<em>)</em>.</p>
<section id="définitions" class="level5">
<h5 class="anchored" data-anchor-id="définitions">Définitions</h5>
<ul>
<li><p><strong>Une hypothèse statistique</strong> est une supposition sur un paramètre de la population. Cette supposition peut être ou ne pas être vraie. Pour prouver qu’une hypothèse est vraie ou fausse avec une certitude absolue, nous aurions besoin d’examiner l’ensemble de la population. Les tests d’hypothèses portent sur la façon d’utiliser un échantillon aléatoire pour déterminer si l’hypothèse faite sur la population est vérifiée.</p></li>
<li><p><strong><em>Un test statistique</em></strong> utilise les données obtenues d’un échantillon pour décider si l’hypothèse nulle doit être rejetée ou non. La valeur numérique obtenue à partir d’un test statistique s’appelle la <strong>valeur de test (valeur empirique)</strong>.</p></li>
</ul>
<p>Le test d’hypothèse est formulé comme suit&nbsp;:</p>
<p><span class="math display">\[H_{0}:hypothèse\ nulle\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ H_{1}:hypothèse\ alternative\]</span></p>
<ul>
<li><p><strong><em>L’hypothèse nulle</em></strong>, symbolisée par <span class="math inline">\(H_{0}\)</span>, est une hypothèse statistique stipulant qu’il n’y a pas de différence entre un paramètre et une valeur spécifique ou qu’il n’y a pas de différence entre deux paramètres.</p></li>
<li><p><strong><em>L’hypothèse alternative</em></strong>, symbolisée par <span class="math inline">\(H_{1}\)</span>, est une hypothèse statistique qui énonce une différence spécifique entre un paramètre et une valeur spécifique ou indique qu’il existe une différence entre deux paramètres.</p></li>
</ul>
<p>Quatre types de résultats sont possibles dans les tests d’hypothèses. L’hypothèse nulle peut être vraie ou pas et une décision est prise de la rejeter ou de ne pas la rejeter sur la base des données obtenues à partir d’un échantillon.</p>
<table class="table">
<colgroup>
<col style="width: 28%">
<col style="width: 29%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;"><span class="math inline">\(H_{0}\)</span> est vraie</th>
<th style="text-align: left;"><span class="math inline">\(H_{1}\)</span> est vraie</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ne pas rejeter <span class="math inline">\(H_{0}\)</span></td>
<td style="text-align: left;">Décision correcte</td>
<td style="text-align: left;">Erreur de Type II</td>
</tr>
<tr class="even">
<td>Rejeter <span class="math inline">\(H_{0}\)</span></td>
<td style="text-align: left;">Erreur de Type I</td>
<td style="text-align: left;">Décision correcte</td>
</tr>
</tbody>
</table>
<p><strong><em>Une erreur de type I</em></strong> se produit si l’on rejette l’hypothèse nulle lorsqu’elle est vraie.</p>
<p><strong><em>Une erreur de type II</em></strong> se produit si l’on ne rejette pas l’hypothèse nulle quand elle est fausse.</p>
<ul>
<li>Le <strong><em>degré de significativité</em></strong> est la probabilité maximale de commettre une erreur de type I, c’est-à-dire la probabilité de rejeter l’hypothèse nulle alors qu’elle est vraie. Cette probabilité est symbolisée par <span class="math inline">\(\alpha\)</span>. En d’autres termes&nbsp;:</li>
</ul>
<p><span class="math display">\[Probabilité\ (erreur\ de\ type\ 1) = \alpha\]</span></p>
<p>Plus le degré de significativité <span class="math inline">\(\alpha\)</span> est faible, moins nous sommes susceptibles de commettre une erreur de type I. Généralement, nous aimerions avoir de petites valeurs de <span class="math inline">\(\alpha\)</span>. Les degrés de significativité typiques sont&nbsp;: 10%, 5% et 1%.</p>
<p>Par exemple, lorsque <span class="math inline">\(\alpha = 10\%\)</span>, il y a 10% de chance de rejeter une hypothèse nulle alors qu’elle est vraie.</p>
<ul>
<li><strong><em>Le degré de liberté</em></strong> correspond au nombre de valeurs aléatoires qui ne peuvent être déterminées ou fixés par une équation (par exemple, pour la variabilité totale, connaissant <span class="math inline">\(n - 1\)</span> valeurs, nous pourrons en déduire la <span class="math inline">\(n\)</span>-ième, puisque nous connaissons la moyenne <span class="math inline">\(y\)</span>). Le degré de liberté est égal au nombre d’observations moins le nombre de relations entre ces observations&nbsp;: on pourrait remplacer l’expression «&nbsp;nombre de relations&nbsp;» par «&nbsp;nombre de paramètres à estimer&nbsp;».</li>
</ul>
<p>Par exemple, si l’on cherche deux nombres dont la somme est 12, aucun des deux nombres ne doit être déterminé par l’équation <span class="math inline">\(x + y = 12\)</span>. <span class="math inline">\(x\)</span> peut être choisi arbitrairement, mais alors pour <span class="math inline">\(y\)</span> il n’y aura alors plus le choix. Ainsi, si vous choisissez 11 comme valeur pour <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> vaut obligatoirement 1. Il y a donc deux variables aléatoires mais un seul degré de liberté.</p>
</section>
<section id="les-différents-types-de-test-dhypothèse" class="level5">
<h5 class="anchored" data-anchor-id="les-différents-types-de-test-dhypothèse">Les différents types de test d’hypothèse</h5>
<ul>
<li><h5 id="les-tests-bilatéraux-test-de-student-sur-les-estimateurs-mco" class="anchored">Les tests bilatéraux&nbsp;: test de Student sur les estimateurs MCO</h5></li>
</ul>
<p>Les tests bilatéraux sont sous la forme&nbsp;:</p>
<p><span class="math display">\[H_{0}:\mu = \ \mu_{0}\ \ \ \ \ \ \ \ \ \ \ H_{1}:\mu \neq \mu_{0}\]</span></p>
<p><strong>Exemple&nbsp;:</strong> Un chercheur en médecine voudrait savoir si un nouveau médicament aura des effets secondaires indésirables. Le chercheur est particulièrement préoccupé par le pouls des patients prenant le médicament. Quelles sont les hypothèses pour vérifier si le pouls sera différent du pouls moyen de 82 battements par minute&nbsp;?</p>
<p><span class="math display">\[H_{0}:\mu = 82\ \ \ \ \ \ \ \ \ \ \ \ \ H_{1}:\mu \neq 82\]</span></p>
<ul>
<li><h5 id="les-tests-unilatéraux" class="anchored">Les tests unilatéraux</h5></li>
</ul>
<p>Ils sont sous la forme suivante&nbsp;:</p>
<p><span class="math display">\[H_{0}:\mu = \ \mu_{0}\ \ \ \ \ H_{1}:\mu &lt; \mu_{0}\]</span></p>
<p>ou</p>
<p><span class="math display">\[H_{0}:\mu = \ \mu_{0}\ \ \ \ \ H_{1}:\mu &gt; \mu_{0}\]</span></p>
<p><strong>Exemple</strong>&nbsp;: Un chimiste invente un additif pour augmenter la durée de vie d’une batterie d’automobile. Si la durée de vie moyenne de la batterie est de 36 mois, alors ses hypothèses sont&nbsp;:</p>
<p><span class="math display">\[H_{0}:\mu = \ 36\ \ \ \ \ H_{1}:\mu &gt; 36\]</span></p>
</section>
<section id="linférence-statistique" class="level5">
<h5 class="anchored" data-anchor-id="linférence-statistique">L’inférence statistique</h5>
<p>L’inférence statistique consiste à effectuer des études sur un échantillon et de transposer (ou généraliser) les résultats sur la population. Elle permet de faire des prédictions sur une population à partir des observations et de l’analyse d’un échantillon. Elle permet de déterminer des intervalles de confiance pour des paramètres du modèle ou de tester si un paramètre est significativement inferieur, supérieur ou simplement différent d’une valeur fixée. Il existe deux méthodes d’inférence statistique&nbsp;: l’estimation des paramètres et le test statistique des hypothèses.</p>
</section>
</section>
<section id="prévision" class="level4">
<h4 class="anchored" data-anchor-id="prévision">Prévision</h4>
<p>La prévision, à partir de l’utilisation des modèles économiques, est utilisée par les pouvoirs publics ou entreprises afin d’anticiper et éventuellement réagir à l’environnement économique. Ainsi, si le modèle que nous choisissons ne réfute pas l’hypothèse ou la théorie considérée, l’on pourra l’utiliser pour prédire la ou les valeurs futures de la variable dépendante, sur la base de la ou des valeurs futures connues ou attendues de la variable explicative.</p>
<p>En se basant par exemple sur l’équation (3), supposons que nous voulons prévoir la dépense de consommation principale pour 2022. La valeur du PIB pour 2019 est par exemple de 1500 milliards de francs CFA. En remplaçant cette valeur dans l’équation (3), on pourra prédire la valeur de la consommation pour 2020 comme suit&nbsp;:</p>
<p><span class="math display">\[\widehat{C_{2019}} = 144.06 + 0.8262(1500\ )\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{(4)}\]</span></p>
<p>En résumé, la démarche économétrique peut être résumée comme suit&nbsp;:</p>
</section>
</section>
</section>
<section id="chapitre-2-introduction-à-lanalyse-de-régression-linéaire" class="level2">
<h2 class="anchored" data-anchor-id="chapitre-2-introduction-à-lanalyse-de-régression-linéaire">Chapitre 2&nbsp;: Introduction à l’analyse de régression linéaire</h2>
<p><strong><em>L’analyse de la régression</em></strong> s’intéresse à l’étude de la dépendance d’une variable (variable dépendante) par rapport à une ou plusieurs variables (variables indépendantes ou explicatives) assortie d’un objectif d’estimer et/ou de prédire la valeur moyenne de la variable dépendante en fonction des valeurs connues ou fixes des variables indépendantes.</p>
<section id="relations-statistiques-versus-relations-déterministes" class="level3">
<h3 class="anchored" data-anchor-id="relations-statistiques-versus-relations-déterministes">Relations statistiques versus relations déterministes</h3>
<p>Dans l’analyse de la régression, nous nous intéressons à ce qu’on nomme la dépendance entre variables. Il s’agit de la dépendance statistique et non fonctionnelle ou déterministe que l’on rencontre par exemple en physique classique. Dans la relation statistique entre variables, nous traitons essentiellement des variables aléatoires ou stochastiques (c’est-à-dire variables avec des distributions de probabilité). Par contre, dans la relation déterministe, on s’intéresse aussi à des variables, mais elles ne sont ni aléatoires ni stochastiques.</p>
<p>Par exemple, la dépendance du rendement d’une récolte par rapport à la température, la pluviométrie, l’ensoleillement et les engrais est, par nature, statistique car les variables explicatives, ne permettent pas à l’agriculteur de prédire la récolte avec précision en raison d’erreurs incluses dans la mesure de ces variables ainsi que de plusieurs autres facteurs (variables) qui influent ensemble sur la récolte mais qui sont difficiles à individualiser.</p>
<p>D’un autre côté, dans les relations déterministes comme celle de la loi de Ohm qui s’énonce comme suit&nbsp;: pour les conducteurs métalliques, sur une plage limitée de température, le courant continu <span class="math inline">\(C\)</span> est proportionnel au voltage <span class="math inline">\(V\)</span>.</p>
<p><span class="math display">\[C = \frac{1}{k}V\ avec\ \frac{1}{k}\ la\ constance\ de\ proportionnalité\]</span></p>
<p>Cependant en cas d’erreur de mesure, par exemple dans <span class="math inline">\(k\)</span> la loi de Ohm, la relation déterministe devient une relation statistique.</p>
</section>
<section id="régression-versus-corrélation" class="level3">
<h3 class="anchored" data-anchor-id="régression-versus-corrélation">Régression versus corrélation</h3>
<p>Bien que très liées, l’analyse de la corrélation est conceptuellement très différente de l’analyse de la régression. L’analyse de la corrélation mesure l’intensité de la liaison entre deux variables.</p>
<p><strong>Exemple</strong>&nbsp;: corrélation entre les notes en statistiques et celles en mathématiques.</p>
<p>L’analyse de la régression quant à elle, a pour objectif d’estimer ou de prévoir la valeur moyenne d’une variable sur la base de valeurs fixées d’autres variables.</p>
<p><strong>Exemple</strong>&nbsp;: Prédire la note moyenne d’une épreuve en statistique en disposant de la note d’un étudiant en mathématiques.</p>
<p>Dans l’analyse de la régression, il existe une asymétrie dans le traitement des variables dépendantes et explicatives. Dans l’analyse de la corrélation, les variables sont traitées de manière symétrique. Il n’existe pas de distinction entre variables explicatives et dépendantes.</p>
</section>
<section id="présentation-du-modèle-de-régression-linéaire" class="level3">
<h3 class="anchored" data-anchor-id="présentation-du-modèle-de-régression-linéaire">Présentation du modèle de régression linéaire</h3>
<p>Le modèle de régression linéaire désigne un modèle dans lequel l’espérance conditionnelle de <span class="math inline">\(y\)</span> sachant <span class="math inline">\(x\)</span> est une transformation affine de <span class="math inline">\(x\)</span>. Le modèle de régression linéaire s’écrit :</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \ldots + \beta_{k}x_{k} + \varepsilon\]</span></p>
<p>On parle de la régression de <span class="math inline">\(y\)</span> sur <span class="math inline">\(x\)</span>, avec <span class="math inline">\(\beta_{j}\)</span> les paramètres (les coefficients) inconnus du modèle. <span class="math inline">\(\varepsilon\)</span> est le terme d’erreur.</p>
<section id="signification-du-terme-linéaire" class="level4">
<h4 class="anchored" data-anchor-id="signification-du-terme-linéaire">Signification du terme «&nbsp;linéaire&nbsp;»</h4>
<p>Puisque nous nous intéressons avant tout aux modèles linéaires, il est fondamental de connaitre ce que signifie réellement l’adjectif «&nbsp;linéaire&nbsp;» car il peut être interprété de deux manières.</p>
<section id="linéarité-dans-les-variables" class="level5">
<h5 class="anchored" data-anchor-id="linéarité-dans-les-variables">Linéarité dans les variables</h5>
<p>Le premier sens, peut-être le plus «&nbsp;naturel&nbsp;» de la linéarité réside dans le fait que l’espérance de <span class="math inline">\(y\)</span> est une fonction linéaire de <span class="math inline">\(x_{i}\)</span>. Géométriquement, la courbe de régression est, dans ce cas, une droite. Dès lors, une fonction de régression telle que <span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}x_{1}^{2} + \ldots + \beta_{k}x_{k} + \varepsilon\)</span> n’est pas une relation linéaire car la variable <span class="math inline">\(x_{1}\)</span> est affectée d’une puissance ou d’un indice de 2.</p>
</section>
<section id="linéarité-dans-les-paramètres" class="level5">
<h5 class="anchored" data-anchor-id="linéarité-dans-les-paramètres">Linéarité dans les paramètres</h5>
<p>La seconde interprétation de la linéarité tient au fait que l’espérance de de <span class="math inline">\(y\)</span> est une fonction linéaire des paramètres <span class="math inline">\(\beta_{i}\)</span>. Cette espérance peut être ou non linéaire dans les variables, c’est-à-dire par rapport à <span class="math inline">\(x\)</span>. Ainsi, <span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}x_{1}^{2} + \ldots + \beta_{k}x_{k} + \varepsilon\)</span> est un modèle de régression linéaire dans les paramètres. Par contre, <span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}^{2}x_{1} + \ldots + \beta_{k}x_{k} + \varepsilon\)</span> n’est pas un modèle de régression linéaire dans les paramètres.</p>
</section>
</section>
<section id="signification-du-terme-aléatoire" class="level4">
<h4 class="anchored" data-anchor-id="signification-du-terme-aléatoire">Signification du terme aléatoire</h4>
<p>Le terme aléatoire que l’on appelle <strong><em>erreur du modèle</em></strong>, tient un rôle très important dans la régression. Il permet de résumer toute l’information qui n’est pas prise en compte dans la relation linéaire que l’on cherche à établir entre <span class="math inline">\(Y\)</span> et <span class="math inline">\(X\)</span> c’est-à-dire les problèmes de spécifications, l’approximation par la linéarité, résumer le rôle des variables explicatives absentes, etc. Le terme d’erreur est un substitut de toutes les variables omises dans le modèle mais qui affectent toutes ensemble <span class="math inline">\(Y\)</span>.</p>
<p>Comme nous le verrons dans la suite du cours, les propriétés des estimateurs du modèle de régression linéaire, reposent en grande partie sur les hypothèses que nous formulerons à propos du terme d’erreur <span class="math inline">\(\varepsilon\)</span>. En pratique, après avoir estimé les paramètres de la régression, les premières vérifications portent sur l’erreur calculée sur les données (on parle de «&nbsp;résidus&nbsp;») lors de la modélisation.</p>
<p>Le terme d’erreur <span class="math inline">\(\varepsilon\)</span> peur regrouper les erreurs suivantes :</p>
<ul>
<li><p><strong><em>omission de variables ou imprécision de la théorie</em></strong>&nbsp;: même si la théorie existe, elle peut être insuffisante pour expliquer le comportement de <span class="math inline">\(Y\)</span>. Nous pouvons tenir pour certain que le revenu influe sur la consommation, mais pouvons ignorer les autres variables affectant la consommation ou douter de leur pertinence. Par conséquent, le terme d’erreur peut être utilisé comme substitut à toutes les variables omises dans le modèle.</p></li>
<li><p><strong><em>erreur de mesure&nbsp;:</em></strong> les données ne représentent pas exactement le phénomène</p></li>
<li><p><strong><em>la nature intrinsèquement aléatoire du comportement humain&nbsp;:</em></strong> même si nous réussissons à introduire toutes les variables pertinentes du modèle, il y a des limites au caractère aléatoire des individuels qui ne peuvent être expliquées quelle que soit la somme de travail que nous fournissons. Les perturbations ou terme d’erreur peuvent fort bien refléter cette nature intrinsèquement aléatoire.</p></li>
</ul>
<!-- -->
<ul>
<li><strong><em>erreur de spécification&nbsp;</em></strong>: même si théoriquement nous disposons des bonnes variables explicatives d’un phénomène et même si nous pouvons nous procurer des données sur ces variables, très souvent nous ne connaissons pas la forme de la relation fonctionnelle entre la variable dépendante et le régresseur.</li>
</ul>
<blockquote class="blockquote">
<p>La consommation est-elle une fonction linéaire ou non linéaire du revenu&nbsp;? Si c’est le premier cas qui prévaut, alors <span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}\)</span> est la relation correcte entre <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> mais si c’est le second cas, <span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}{x^{2}}_{i} + \varepsilon_{i}\)</span> peut être la forme fonctionnelle adaptée. Dans les modèles à deux variables, la forme fonctionnelle de la relation peut souvent être discernée au diagramme de dispersion. Mais dans un modèle de régression multiple, il est plus difficile de déterminer la forme fonctionnelle adéquate, car graphiquement nous ne pouvons pas visualiser les diagrammes de dispersion à plusieurs dimensions.</p>
</blockquote>
<ul>
<li><strong><em>erreur de fluctuation d’échantillonnage&nbsp;:</em></strong> d’un échantillon à l’autre les observations, et donc les estimations, sont légèrement différentes.</li>
</ul>
</section>
</section>
<section id="fonction-de-régression-de-léchantillon-fre-et-fonction-de-régression-de-la-population-frp" class="level3">
<h3 class="anchored" data-anchor-id="fonction-de-régression-de-léchantillon-fre-et-fonction-de-régression-de-la-population-frp">Fonction de régression de l’échantillon (FRE) et fonction de régression de la population (FRP)</h3>
<p>Dans la plupart des situations concrètes nous ne disposons que d’un échantillon de <span class="math inline">\(y\)</span> associé à quelques valeurs données de <span class="math inline">\(x\)</span>. Ainsi, notre tâche est d’estimer la fonction de régression à partir des informations fournies par l’échantillon. Nous obtenons alors la fonction de régression de l’échantillon (FRE) contrairement à la fonction de régression de la population (FRP) qui est <span class="math inline">\(y\)</span> et <span class="math inline">\(x\)</span> sur toute la population.</p>
<p>En résumé, notre objectif est donc d’estimer, la fonction de régression de la population (RP)&nbsp;:</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \ldots + \beta_{k}x_{k} + \varepsilon\]</span></p>
<p>à partir de la fonction de régression de l’échantillon (FRE) obtenue de l’observation de</p>
<p><span class="math display">\[y = {\widehat{\beta}}_{0} + {\widehat{\beta}}_{1}x_{1} + {\widehat{\beta}}_{2}x_{2} + \ldots + {\widehat{\beta}}_{k}x_{k} + e\]</span></p>
<p>où <span class="math inline">\(e\)</span> est un estimateur de <span class="math inline">\(\varepsilon\)</span>.</p>
<p>En raison des fluctuations d’échantillonnage, notre estimation de la FRP basée sur la FRE est une approximation.</p>
</section>
</section>
<section id="chapitre-3-estimation-des-modèles-de-régression-linéaires" class="level2">
<h2 class="anchored" data-anchor-id="chapitre-3-estimation-des-modèles-de-régression-linéaires">Chapitre 3&nbsp;: Estimation des modèles de régression linéaires</h2>
<p>Notre tâche consiste à estimer aussi exactement que possible la fonction de régression de la population (FRP) sur la base d’une fonction de régression de l’échantillon (FRE). Les méthodes d’estimation généralement utilisées sont les <strong>moindres carres ordinaires (MCO) et le maximum de vraisemblance (MV)</strong>.</p>
<section id="lestimation-par-la-méthode-des-moindres-carrés-ordinaires-mco" class="level3">
<h3 class="anchored" data-anchor-id="lestimation-par-la-méthode-des-moindres-carrés-ordinaires-mco">L’estimation par la méthode des moindres carrés ordinaires (MCO)</h3>
<p>La méthode des moindres carrés ordinaires est celle qui est plus largement utilisée dans l’analyse de la régression linéaire parce qu’elle est mathématiquement beaucoup plus simple que le maximum de vraisemblance. Cependant, pour la régression linéaire, les deux méthodes fournissent généralement des résultats analogues.</p>
<p>La méthode des MCO est attribuée à Carl Friedrich Gauss, un mathématicien allemand. La méthode consiste en une prescription (initialement empirique), qui est que la fonction <span class="math inline">\(f(x;\beta)\)</span> qui décrit «&nbsp;le mieux&nbsp;» les données est celle qui minimise la somme quadratique des déviations des mesures aux prédictions de&nbsp;<span class="math inline">\(f(x;\beta)\)</span>.</p>
<section id="les-hypothèses-du-modèle-de-régression-linéaire" class="level4">
<h4 class="anchored" data-anchor-id="les-hypothèses-du-modèle-de-régression-linéaire">Les Hypothèses du modèle de régression linéaire</h4>
<p>L’estimation du modèle de régression linéaire repose sur un certain nombre d’hypothèses que sont&nbsp;:</p>
<ul>
<li><strong>Hypothèse 1&nbsp;: Le modèle est linéaire dans les paramètres</strong></li>
</ul>
<p>Cela ne veut pas dire que <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont linéaires (elles peuvent être non linéaires), mais plutôt que <span class="math inline">\(\beta_{0}\)</span> et <span class="math inline">\(\beta_{1}\)</span> sont linéaires.</p>
<ul>
<li><strong>Hypothèse 2&nbsp;: L’espérance mathématique de l’erreur</strong> <span class="math inline">\(\mathbf{\varepsilon}_{\mathbf{i}}\)</span> <strong>est nulle</strong>.&nbsp;</li>
</ul>
<p>La valeur de <span class="math inline">\(x_{i}\)</span> étant donnée, la moyenne ou l’espérance mathématique du terme d’erreur aléatoire <span class="math inline">\(\varepsilon_{i}\)</span> est nulle.</p>
<p><span class="math display">\[E\left( \varepsilon_{i} \right) = 0\ ou\ E\left( \varepsilon_{i}׀x_{i} \right) = 0\]</span></p>
<p>Ainsi, les erreurs <span class="math inline">\(\varepsilon_{i}\)</span> pour une valeur donnée de <span class="math inline">\(x_{i}\)</span> (dans la population) est symétrique autour de la sa moyenne. (<strong>Figure</strong>)</p>
<ul>
<li><strong>Hypothèse 3</strong> : <strong>L’homoscédasticité ou la constance de la variance de</strong> <span class="math inline">\(\mathbf{\varepsilon}_{\mathbf{i}}\)</span><strong>.</strong></li>
</ul>
<p>La valeur de <span class="math inline">\(X\)</span> étant donnée, la variance de <span class="math inline">\(\varepsilon_{i}\)</span> est identique pour toutes les observations.</p>
<p><span class="math inline">\(E\left( \varepsilon_{i}^{2} \right) = \sigma_{\varepsilon}^{2}\ \ \ ou\ E\left( \varepsilon_{i}^{2}׀x_{i} \right) = \sigma_{\varepsilon}^{2}\)</span></p>
<p>La variance de l’erreur est constante : le risque de l’amplitude de l’erreur est le même quelle que soit l’observation. Cette hypothèse s’appelle&nbsp;: <strong><em>hypothèse d’homoscédasticité</em></strong>. Dans le cas où cette hypothèse n’est pas vérifiée, on parle alors de <strong><em>modèle hétéroscédastique</em></strong>.</p>
<ul>
<li><strong>Hypothèse 4&nbsp;: Absence d’autocorrélation des erreurs</strong></li>
</ul>
<p>Etant donné deux valeurs, <span class="math inline">\(x_{i}\)</span> et <span class="math inline">\(x_{j}\)</span> (<span class="math inline">\(i \neq j)\)</span>, la corrélation entre les erreurs <span class="math inline">\(\varepsilon_{i}\)</span> et <span class="math inline">\(\varepsilon_{j}\)</span> (<span class="math inline">\(i \neq j)\)</span> est nulle.</p>
<p><span class="math display">\[E\left( \varepsilon_{i}\varepsilon_{j} \right) = 0\ ou\ E\left( \varepsilon_{i}\varepsilon_{j}׀x_{i},׀x_{j} \right) = 0\ \ \ \ avec\ (i \neq j)\]</span></p>
<p><span class="math display">\[cov\left( \varepsilon_{i}\varepsilon_{j} \right) = 0\ ou\ cov\left( \varepsilon_{i}\varepsilon_{j}׀x_{i},׀x_{j} \right) = 0\ \ \ \ avec\ (i \neq j)\]</span></p>
<p>Cette hypothèse signifie que la valeur de <span class="math inline">\(x_{i}\)</span> étant donnée, les déviations de deux valeurs quelconques de <span class="math inline">\(Y\)</span> par rapport à leur moyenne ne sont pas corrélées. Autrement dit, les erreurs relatives à 2 observations sont indépendantes. On parle de «&nbsp;<strong><em>non autocorrélation des erreurs</em></strong>&nbsp;».</p>
<ul>
<li><strong>Hypothèse 5 : Covariance nulle entre</strong> <span class="math inline">\(\mathbf{x}_{\mathbf{i}}\mathbf{\ }\)</span><strong>et</strong> <span class="math inline">\(\mathbf{\varepsilon}_{\mathbf{i}}\)</span></li>
</ul>
<p><span class="math display">\[E\left( x_{i}\varepsilon_{i} \right) = 0\ \ ou\ \ Cov\left( x_{i}\varepsilon_{i} \right) = 0\ \ \]</span></p>
<p>Cette hypothèse signifie que l’erreur <span class="math inline">\(\varepsilon_{i}\ \)</span>est indépendante de la variable explicative <span class="math inline">\(x_{i}\)</span>.</p>
<ul>
<li><strong>Hypothèse 6 <em>: </em></strong><span class="math inline">\(\mathbf{n}\mathbf{&gt; k}\)</span></li>
</ul>
<p>Le nombre d’observations <span class="math inline">\(n\)</span> doit être plus élevé que le nombre de paramètres <span class="math inline">\(k\)</span> à estimer (nombre de variables explicatives).</p>
<ul>
<li><strong>Hypothèse 7&nbsp;: Exactitude de la variable indépendante</strong></li>
</ul>
<p>Les valeurs <span class="math inline">\(x_{i}\)</span> sont fixées d’un échantillon à un autre (observées sans erreur). On considère que les données collectées sont contrôlées par le statisticien et sont mesurées avec une marge d’erreur négligeable. En termes techniques <span class="math inline">\(x_{i}\)</span> est supposé non stochastique (non aléatoire). Cependant, la variable dépendante est supposée être statistique, aléatoire ou stochastique, c’est-à-dire ayant une distribution de probabilité. Il existe une distribution de probabilité pour <span class="math inline">\(y\)</span> pour chaque valeur possible de <span class="math inline">\(x\)</span>. Ainsi, l’espérance mathématique et la variance de la distribution sont donnée par&nbsp;:</p>
<p><span class="math display">\[E(y׀x_{i}) = \beta_{0} + \beta_{1}x\]</span></p>
<p><span class="math display">\[E\left( y׀x_{i} \right) = Var\left( \beta_{0} + \beta_{1}x + \varepsilon \right) = \sigma^{2}\]</span></p>
<p>Ainsi, l’esperance mathématique de <span class="math inline">\(y\)</span> est une fonction linéaire de <span class="math inline">\(x\)</span> bien que sa variance ne dépende pas de <span class="math inline">\(x\)</span>.</p>
<ul>
<li><strong>Hypothèse 8<em>&nbsp;</em>: La normalité du terme d’erreur</strong></li>
</ul>
<p>Les hypothèses <strong>H2</strong> et <strong>H3</strong> sur le terme aléatoire <span class="math inline">\(\varepsilon_{i}\)</span>, peuvent se résumer comme&nbsp;: les <span class="math inline">\(\varepsilon_{i}\)</span> sont i.i.d (indépendants et identiquement distribués).</p>
<p><span class="math display">\[\mathbf{\varepsilon}_{\mathbf{i}}\mathcal{↝ N}\mathbf{(0,}\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}\mathbf{\ }\mathbf{)}\mathbf{\ }\]</span></p>
</section>
<section id="estimation-du-modèle-de-régression-linéaire-simple" class="level4">
<h4 class="anchored" data-anchor-id="estimation-du-modèle-de-régression-linéaire-simple">Estimation du modèle de régression linéaire simple</h4>
<p>Nous allons commencer par la méthode d’estimation le plus élémentaire qui est le modèle de régression linéaire simple. Ce modèle cherche à mettre en évidence la relation de dépendance entre une variable aléatoire réelle à expliquer (variable endogène, dépendante ou réponse) <span class="math inline">\(y\)</span> et une variable explicative (variable exogène, indépendante, régresseur, contrôle) <span class="math inline">\(x\)</span>.</p>
<p>Soit, le modèle de régression linéaire simple&nbsp;:</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}\ \ \ \ \ \ \ \ \ avec\ i = 1,\ldots,n\]</span></p>
<p>Nous cherchons à estimer les paramètres <span class="math inline">\(\beta_{0}\)</span> et <span class="math inline">\(\beta_{1}\)</span> du modèle de régression linéaire simple. Pour cela nous avons besoin d’un échantillon issu de la population. Soit un échantillon de taille <span class="math inline">\(n\)</span> issue de la population, tel que nous ayons des <span class="math inline">\(n\)</span> pairs de données <span class="math inline">\((y_{1},x_{1})\)</span>, <span class="math inline">\((y_{2},x_{2})\)</span>, <span class="math inline">\(\ldots,\ (y_{n},x_{n})\)</span> pour chaque observation <span class="math inline">\(i\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/image1.png" style="width:3.1in;height:1.8in" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Mathematics Statistiques</figcaption><p></p>
</figure>
</div>
<p>Les données issues de l’échantillon sont représentées sur le graphique ci-dessus par les nuages de points. Nous voulons partir de ces observations pour estimer la constante <span class="math inline">\(\beta_{0}\)</span> et la pente <span class="math inline">\(\beta_{1}\)</span> de la FRP. Cette FRP est donnée par la droite qui est la plus proche possible de tous les nuages de points (observations). Pour chaque point du nuage de points (FRE), il existe un écart par rapport à la FRP. Ces écarts constituent les <strong><em>erreurs</em></strong> et contiennent tous les facteurs non observées qui affectent <span class="math inline">\(y_{i}\)</span>. Ils sont mesurés par la projection parallèlement à l’axe des ordonnées des points sur la droite de régression (FRP).</p>
<p>La méthode des MCO cherche à calculer ou estimer les paramètres inconnus <span class="math inline">\(\beta_{0}\)</span> et <span class="math inline">\(\beta_{1}\)</span> à partir des pairs de données en minimisant la somme des carrés des écarts entre la valeur observée de la variable endogène et sa variable calculée (la droite de régression). Le carré permet de donner une pénalité plus grande aux observations plus éloignées de la valeur estimée</p>
<p>La résolution analytique est la suivante :</p>
<p><span class="math display">\[Min\sum_{i = 1}^{n}e_{i}^{2} = Min{\sum_{i}^{}\left( y_{i} - \widehat{y_{i}} \right)}^{2} = Min\left( y_{i} - \left( \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} \right) \right)^{2} = Min\ S\left( \beta_{0},\beta_{1} \right)\]</span></p>
<p>Ou le résidu des MCO pour l’observation <span class="math inline">\(i\)</span> est donnée par&nbsp;:</p>
<p><span class="math display">\[e_{i} = y_{i} - \left( \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} \right)\]</span></p>
<p>En annulant les dérivées partielles (condition du premier ordre), nous obtenons un système d’équations appelées « <strong><em>équations normales</em></strong> » :</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\&amp;\frac{\partial S\left( \beta_{0},\beta_{1} \right)}{\partial\beta_{0}} = - 2\sum_{i = 1}^{n}{\left( y_{i} - \widehat{\beta_{0}} - \widehat{\beta_{1}}x_{i} \right) = 0}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1) \\
\&amp;\frac{\partial S\left( \beta_{0},\beta_{1} \right)}{\partial\beta_{1}} = - 2\sum_{i = 1}^{n}{x_{i}\left( y_{i} - \widehat{\beta_{0}} - \widehat{\beta_{1}}x_{i} \right) = 0}\ \ \ \ \ \ \ \ \ \ \ (2) \\
\end{matrix} \right.\ \]</span></p>
<p>La première équation donne&nbsp;:</p>
<p><span class="math display">\[\sum_{i = 1}^{n}{y_{i} -}n\widehat{\beta_{0}} - \widehat{\beta_{1}}\sum_{i = 1}^{n}x_{i} = 0\]</span></p>
<p>or</p>
<p><span class="math display">\[\overline{y} = \frac{1}{N}\sum_{i = 1}^{n}{y_{i}\text{\ et\ }}\overline{x} = \frac{1}{N}\sum_{i = 1}^{n}x_{i}\]</span></p>
<p>On obtient donc&nbsp;:</p>
<p><span class="math display">\[\overline{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}\overline{x}\]</span></p>
<p><span class="math display">\[\widehat{\mathbf{\beta}_{\mathbf{0}}}\mathbf{=}\overline{\mathbf{y}}\mathbf{-}\widehat{\mathbf{\beta}_{\mathbf{1}}}\overline{\mathbf{x}}\]</span></p>
<p>La deuxième équation devient&nbsp;:</p>
<p><span class="math display">\[\sum_{i = 1}^{n}{x_{i}\left( y_{i} - \widehat{\beta_{0}} - \widehat{\beta_{1}}x_{i} \right) =}\sum_{i = 1}^{n}{x_{i}y_{i}} - \widehat{\beta_{0}}\sum_{i = 1}^{n}x_{i} - \widehat{\beta_{1}}\sum_{i = 1}^{n}{x_{i}}^{2} = 0\]</span></p>
<p>En remplaçant <span class="math inline">\(\widehat{\beta_{0}}\)</span> par son expression, on obtient&nbsp;:</p>
<p><span class="math display">\[\sum_{i = 1}^{n}{x_{i}y_{i}} - \left( \overline{y} - \widehat{\beta_{1}}\overline{x} \right)\sum_{i = 1}^{n}x_{i} - \widehat{\beta_{1}}\sum_{i = 1}^{n}{x_{i}}^{2} = 0\]</span></p>
<p><span class="math display">\[\frac{1}{N}\sum_{i = 1}^{n}{x_{i}y_{i}} - \frac{1}{N}\left( \overline{y} - \widehat{\beta_{1}}\overline{x} \right)\sum_{i = 1}^{n}x_{i} - \widehat{\beta_{1}}\frac{1}{N}\sum_{i = 1}^{n}{x_{i}}^{2} = 0\]</span></p>
<p><span class="math display">\[\frac{1}{N}\sum_{i = 1}^{n}{x_{i}y_{i}} - \left( \overline{y} - \widehat{\beta_{1}}\overline{x} \right)\overline{x} - \widehat{\beta_{1}}\frac{1}{N}\sum_{i = 1}^{n}{x_{i}}^{2} = 0\]</span></p>
<p><span class="math display">\[\frac{1}{N}\sum_{i = 1}^{n}{x_{i}y_{i}} - \overline{y}\overline{x} + \widehat{\beta_{1}}{\overline{x}}^{2} - \widehat{\beta_{1}}\frac{1}{N}\sum_{i = 1}^{n}{x_{i}}^{2} = 0\]</span></p>
<p><span class="math display">\[\ \widehat{\beta_{1}}\left( \frac{1}{N}\sum_{i = 1}^{n}{x^{2}}_{i} - {\overline{x}}^{2} \right) = \frac{1}{N}\sum_{i = 1}^{n}{x_{i}y_{i}} - \overline{y}\overline{x}\]</span></p>
<p><span class="math display">\[\widehat{\mathbf{\beta}_{\mathbf{1}}}\mathbf{=}\frac{\frac{1}{N}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{x}_{\mathbf{i}}\mathbf{y}_{\mathbf{i}}}\mathbf{-}\overline{\mathbf{y}}\overline{\mathbf{x}}}{\frac{1}{N}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{x_{i}}^{2}\mathbf{-}{\overline{\mathbf{x}}}^{\mathbf{2}}}\mathbf{=}\frac{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)\mathbf{(}\mathbf{y}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{y}}\mathbf{)}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)^{\mathbf{2}}}\]</span></p>
<p><span class="math display">\[\widehat{\mathbf{\beta}_{\mathbf{1}}}\mathbf{=}\frac{\widehat{\text{Cov}\left( \mathbf{x}\mathbf{,}\mathbf{y} \right)}}{\widehat{\mathbf{Var}\left( \mathbf{x} \right)}}\]</span></p>
<p><strong>Note</strong>&nbsp;: <span class="math inline">\(\widehat{\beta_{0}}\)</span> et <span class="math inline">\(\widehat{\beta_{1}}\)</span> permettent bien de minimiser la somme du carrée des résidus <span class="math inline">\(S\left( \beta_{0},\beta_{1} \right)\)</span>, car&nbsp;:</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\&amp;\frac{\partial^{2}S\left( \beta_{0},\beta_{1} \right)}{\partial\left( \beta_{0} \right)^{2}} = 2 &gt; 0 \\
\&amp;\frac{\partial^{2}S\left( \beta_{0},\beta_{1} \right)}{\partial\left( \beta_{1} \right)^{2}} = 2\sum_{i = 1}^{n}{x_{i}^{2} &gt; 0} \\
\end{matrix} \right.\ \]</span></p>
</section>
<section id="le-modèle-de-régression-linéaire-multiple-avec-mathbfk-variables-indépendantes" class="level4">
<h4 class="anchored" data-anchor-id="le-modèle-de-régression-linéaire-multiple-avec-mathbfk-variables-indépendantes">Le modèle de régression linéaire multiple avec <span class="math inline">\(\mathbf{k}\)</span> variables indépendantes</h4>
<p>Dans le modèle de régression linéaire simple, nous avons vu que la variable dépendante <span class="math inline">\(y\)</span> ne pouvait être expliquée qu’en fonction d’une seule variable indépendante, <span class="math inline">\(x\)</span>. Le principal problème lié à l’utilisation de la régression linéaire simple est qu’il est très difficile de tirer des conclusions concernant l’effet de <span class="math inline">\(x\)</span> sur <span class="math inline">\(y\)</span>, <em>toutes choses étant égales par ailleurs</em>. En règle générale, <strong>l’hypothèse 2</strong> <span class="math inline">\(\mathbf{(}\mathbf{E}\mathbf{(}\mathbf{\varepsilon}_{\mathbf{i}}\mathbf{׀}\mathbf{x}_{\mathbf{i}}\mathbf{) = 0}\)</span><strong>)</strong> ne tient pas, car la variable <span class="math inline">\(x\)</span> est souvent corrélée avec un autre facteur qui influence <span class="math inline">\(y\)</span>.</p>
<p>Le modèle de régression linéaire multiple (RLM), appelé aussi modèle de régression multiple, permet de prendre en compte de façon explicite de nombreux facteurs qui affectent simultanément la variable dépendante. Si l’on ajoute des facteurs utiles pour expliquer la variable dépendante, nous parviendront naturellement à expliquer une plus grande partie de la variation de <span class="math inline">\(y\)</span>. L’utilisation de la régression multiple peut donc conduire à une meilleure prédiction de la variable dépendante.</p>
<p>Le modèle de régression linéaire multiple dans la population peut prendre la forme générale suivante&nbsp;:</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \ldots + \beta_{k}x_{k} + \varepsilon\]</span></p>
<p>Où <span class="math inline">\(\beta_{0}\)</span> est l’ordonnée à l’origine ou la constante, <span class="math inline">\(\beta_{j}\)</span> mesure la <strong><em>variation de</em></strong> <span class="math inline">\(\mathbf{y}\)</span> <strong><em>suite à une variation d’une unité</em></strong> de <span class="math inline">\(x\)</span>, les autres facteurs étant fixés. Comme il y a <span class="math inline">\(k\)</span> variables indépendantes et une ordonnée à l’origine, l’équation de RLM contient <span class="math inline">\(p = k + 1\)</span> paramètres de population (inconnus).</p>
<p><span class="math inline">\(\varepsilon\)</span> est le terme d’erreur ou la perturbation. Il contient les autres facteurs, différents des <span class="math inline">\(x\)</span> qui affectent la variable dépendante <span class="math inline">\(y\)</span>. Quel que soit le nombre de variables explicatives dans notre modèle, il y aura toujours des facteurs que nous ne pourrons pas inclure&nbsp;; ils seront tout alors compris dans <span class="math inline">\(\varepsilon\)</span>.</p>
<section id="estimation-des-paramètres-du-modèle-de-régression-linéaire-multiple-par-les-mco" class="level5">
<h5 class="anchored" data-anchor-id="estimation-des-paramètres-du-modèle-de-régression-linéaire-multiple-par-les-mco">Estimation des paramètres du modèle de régression linéaire multiple par les MCO</h5>
<p>La méthode des MCO peut être utilisée pour estimer les coefficients du l’équation du modèle de RLM. Supposons que nous avons, <span class="math inline">\(n &gt; k\)</span> observations. Soit <span class="math inline">\(y_{i}\)</span> la <span class="math inline">\(i\)</span>ème observation de la variable dépendante (de réponse), <span class="math inline">\(x_{ij}\)</span> est la <span class="math inline">\(i\)</span>ème observation pour la variable dépendante <span class="math inline">\(j\)</span>. Les données se présenteront comme dans le tableau qui suit&nbsp;:</p>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th style="text-align: left;">Variables dépendantes</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>Obersation,</em> <span class="math inline">\(i\)</span></td>
<td><em>Response,</em> <span class="math inline">\(y\)</span></td>
<td style="text-align: left;"><span class="math display">\[x_{1}\]</span></td>
<td><span class="math display">\[x_{2}\]</span></td>
<td><span class="math display">\[\ldots\]</span></td>
<td><span class="math display">\[x_{k}\]</span></td>
</tr>
<tr class="even">
<td><em>1</em></td>
<td><span class="math display">\[y_{1}\]</span></td>
<td style="text-align: left;"><span class="math display">\[x_{11}\]</span></td>
<td><span class="math display">\[x_{12}\]</span></td>
<td><span class="math display">\[\ldots\]</span></td>
<td><span class="math display">\[x_{1k}\]</span></td>
</tr>
<tr class="odd">
<td><em>2</em></td>
<td><span class="math display">\[y_{2}\]</span></td>
<td style="text-align: left;"><span class="math display">\[x_{21}\]</span></td>
<td><span class="math display">\[x_{22}\]</span></td>
<td><span class="math display">\[\ldots\]</span></td>
<td><span class="math display">\[x_{2k}\]</span></td>
</tr>
<tr class="even">
<td><span class="math display">\[\vdots\]</span></td>
<td><span class="math display">\[\vdots\]</span></td>
<td style="text-align: left;"><span class="math display">\[\vdots\]</span></td>
<td><span class="math display">\[\vdots\]</span></td>
<td></td>
<td><span class="math display">\[\vdots\]</span></td>
</tr>
<tr class="odd">
<td><span class="math display">\[n\]</span></td>
<td><span class="math display">\[y_{n}\]</span></td>
<td style="text-align: left;"><span class="math display">\[x_{n1}\]</span></td>
<td><span class="math display">\[x_{n2}\]</span></td>
<td><span class="math display">\[\ldots\]</span></td>
<td><span class="math display">\[x_{nk}\]</span></td>
</tr>
</tbody>
</table>
<p>Le modèle de régression linéaire correspondant, se présente alors comme suit&nbsp;:</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \ldots + \beta_{k}x_{ik} + \varepsilon_{i}\]</span></p>
<p><span class="math display">\[y_{i} = \beta_{0} + \sum_{j = 1}^{k}{\beta_{j}x_{ij}} + \varepsilon_{i},\ \ \ \ i = 1,2,\ldots,\ n\ \ \ et\ j = 1,2,\ldots,\ k\]</span></p>
<p>Sous forme matricielle le modèle s’écrit&nbsp;:</p>
<p><span class="math display">\[Y = X\beta + \varepsilon\]</span></p>
<p><span class="math inline">\(Y = \begin{bmatrix} y_{1} \\ \begin{matrix} y_{2} \\  \vdots \\ \end{matrix} \\ y_{n} \\ \end{bmatrix}\)</span>, <span class="math inline">\(X = \left\lbrack \begin{matrix} \begin{matrix} 1 \\ \begin{matrix} 1 \\  \vdots \\ \end{matrix} \\ \end{matrix} &amp; \begin{matrix} x_{11} \\ \begin{matrix} x_{21} \\  \vdots \\ \end{matrix} \\ \end{matrix} \\ 1 &amp; x_{n1} \\ \end{matrix}\ \ \ \begin{matrix} \begin{matrix} \ldots \\ \begin{matrix} \ldots \\  \vdots \\ \end{matrix} \\ \end{matrix} &amp; \begin{matrix} x_{1k} \\ \begin{matrix} x_{2k} \\  \vdots \\ \end{matrix} \\ \end{matrix} \\ \ldots &amp; x_{nk} \\ \end{matrix} \right\rbrack\)</span> <span class="math inline">\(\beta = \begin{bmatrix} \beta_{0} \\ \begin{matrix} \beta_{2} \\  \vdots \\ \end{matrix} \\ \beta_{k} \\ \end{bmatrix}\)</span> et <span class="math inline">\(\varepsilon = \begin{bmatrix} \varepsilon_{1} \\ \begin{matrix} \varepsilon_{2} \\  \vdots \\ \end{matrix} \\ \varepsilon_{n} \\ \end{bmatrix}\)</span></p>
<ul>
<li><p><span class="math inline">\(Y\)</span> désigne le vecteur à expliquer de taille <span class="math inline">\(n \times 1\)</span></p></li>
<li><p><span class="math inline">\(X\)</span> la matrice explicative de taille <span class="math inline">\(n \times p\)</span></p></li>
<li><p><span class="math inline">\(\beta\)</span> le vecteur des coefficients du modèle. Il est de taille <span class="math inline">\(p \times 1\)</span></p></li>
<li><p><span class="math inline">\(\varepsilon\)</span> le vecteur d’erreurs de taille <span class="math inline">\(n \times 1\)</span></p></li>
</ul>
<p>La fonction des moindres carrés associé au modèle de RLM est donnée par&nbsp;:</p>
<p><span class="math display">\[S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right) = \sum_{i = 1}^{n}e_{i}^{2} = \ \sum_{i = 1}^{n}\left( y_{i} - \beta_{0} - \sum_{j = 1}^{k}{\beta_{j}x_{ij}} \right)^{2}\]</span></p>
<p>Pour trouver les estimateurs des MCO, la fonction <span class="math inline">\(S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right)\)</span> doit être minimisée en fonction de <span class="math inline">\(\beta_{0},\beta_{1},\ \beta_{2},\ldots,\beta_{k}\)</span>. Les estimateurs des MCO <span class="math inline">\({\widehat{\beta}}_{0},{\widehat{\beta}}_{1},\ {\widehat{\beta}}_{2},\ldots,{\widehat{\beta}}_{k}\)</span> satisfont les conditions suivantes&nbsp;:</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\&amp;\frac{\partial S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right)}{\partial\beta_{0}} = - 2\sum_{i = 1}^{n}{\left( y_{i} - \widehat{\beta_{0}} - \sum_{j = 1}^{k}{\widehat{\beta_{j}}x_{ij}} \right) = 0}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1) \\
\&amp;\frac{\partial S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right)}{\partial\beta_{j}} = - 2\sum_{i = 1}^{n}{\left( y_{i} - \widehat{\beta_{0}} - \sum_{j = 1}^{k}{\widehat{\beta_{j}}x_{ij}} \right)x_{ij}} = 0\ \ \ \ \ \ \ j = 1,2,\ldots,\ k\ \ \ \ (2) \\
\end{matrix} \right.\ \]</span></p>
<p>En simplifiant, nous obtenons les équations normales des MCO qui suivent&nbsp;:</p>
<p><span class="math display">\[\sum_{i = 1}^{n}y_{i} = n\widehat{\beta_{0}} - \widehat{\beta_{1}}\sum_{i = 1}^{n}x_{i1} + \widehat{\beta_{2}}\sum_{i = 1}^{n}x_{i2} + \ldots + \widehat{\beta_{k}}\sum_{i = 1}^{n}x_{ik}\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}{x_{i1}y}_{i} = \widehat{\beta_{0}}\sum_{i = 1}^{n}x_{i1} + \widehat{\beta_{1}}\sum_{i = 1}^{n}{x_{i1}}^{2} + \widehat{\beta_{2}}\sum_{i = 1}^{n}{x_{i1}x_{i2}} + \ldots + \widehat{\beta_{k}}\sum_{i = 1}^{n}{x_{i1}x_{ik}}\]</span></p>
<p><span class="math display">\[\vdots .\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \ \ \ \ \ \ \ \  \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \vdots\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}{x_{ik}y}_{i} = \widehat{\beta_{0}}\sum_{i = 1}^{n}x_{ik} + \widehat{\beta_{1}}\sum_{i = 1}^{n}{x_{ik}x_{i1}} + \widehat{\beta_{2}}\sum_{i = 1}^{n}{x_{ik}x_{i2}} + \ldots + \widehat{\beta_{k}}\sum_{i = 1}^{n}{x_{ik}}^{2}\]</span></p>
<p><span class="math inline">\(\begin{bmatrix} \sum_{i = 1}^{n}y_{i} \\ \begin{matrix} \sum_{i = 1}^{n}{x_{i1}y}_{i} \\  \vdots \\ \end{matrix} \\ \sum_{i = 1}^{n}{x_{ik}y}_{i} \\ \end{bmatrix} = \left\lbrack \begin{matrix} \begin{matrix} n \\ \begin{matrix} \sum_{i = 1}^{n}x_{i1} \\  \vdots \\ \end{matrix} \\ \end{matrix} &amp; \begin{matrix} \sum_{i = 1}^{n}x_{i1} \\ \begin{matrix} \sum_{i = 1}^{n}{x_{i1}}^{2} \\  \vdots \\ \end{matrix} \\ \end{matrix} \\ \sum_{i = 1}^{n}x_{ik} &amp; \sum_{i = 1}^{n}{x_{ik}x_{i1}} \\ \end{matrix}\ \ \ \begin{matrix} \begin{matrix} \ldots \\ \begin{matrix} \ldots \\  \vdots \\ \end{matrix} \\ \end{matrix} &amp; \begin{matrix} \sum_{i = 1}^{n}x_{ik} \\ \begin{matrix} \sum_{i = 1}^{n}{x_{i1}x_{ik}} \\  \vdots \\ \end{matrix} \\ \end{matrix} \\ \ldots &amp; \sum_{i = 1}^{n}{x_{ik}}^{2} \\ \end{matrix} \right\rbrack\begin{bmatrix} \widehat{\beta_{0}} \\ \begin{matrix} \widehat{\beta_{1}} \\  \vdots \\ \end{matrix} \\ \widehat{\beta_{k}} \\ \end{bmatrix}\)</span></p>
<p>Pour faciliter la résolution du système d’équation nous allons utiliser la forme matricielle du modèle.</p>
<p><span class="math display">\[S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right) = \sum_{i = 1}^{n}e_{i}^{2} = \varepsilon^{'}\varepsilon = (Y - X\beta)^{'}(Y - X\beta)\]</span></p>
<p><span class="math display">\[S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right) = Y^{'}Y - \beta^{'}X^{'}Y - Y^{'}X\beta + \beta^{'}X^{'}X\beta\]</span></p>
<p><span class="math display">\[S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right) = Y^{'}Y - 2\beta^{'}X^{'}Y + \beta^{'}X^{'}X\beta\]</span></p>
<p>Puisque <span class="math inline">\(\beta^{'}X^{'}Y\)</span> est une matrice (ou un scalaire) <span class="math inline">\(1 \times 1\)</span> et sa transposée <span class="math inline">\(Y^{'}X\beta\)</span> est le même scalaire. La minimisation de <span class="math inline">\(S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right)\)</span> satisfait&nbsp;:</p>
<p><span class="math display">\[\frac{\partial S\left( \beta_{0},\beta_{1},\cdots,\beta_{k} \right)}{\partial\beta} = - 2X^{'}Y + X^{'}X\widehat{\beta}\]</span></p>
<p>On a&nbsp;:</p>
<p><span class="math display">\[X^{'}X\widehat{\beta} = X^{'}Y\]</span></p>
<p>Pour résoudre l’équation, on multiplie les deux côtés de l’équation par <span class="math inline">\({(X^{'}X)}^{- 1}\)</span> en supposant que la matrice <span class="math inline">\({(X^{'}X)}^{- 1}\)</span> existe. La matrice <span class="math inline">\({(X^{'}X)}^{- 1}\)</span> va toujours exister si les variables explicatives sont linéairement indépendantes, c’est-à-dire qu’aucune colonne de la matrice <span class="math inline">\(X\)</span> n’est une combinaison linéaire des autres colonnes.</p>
<p><span class="math display">\[\widehat{\beta} = {(X^{'}X)}^{- 1}X^{'}Y\]</span></p>
</section>
<section id="interprétation-de-la-régression-multiple-en-effet-partiel" class="level5">
<h5 class="anchored" data-anchor-id="interprétation-de-la-régression-multiple-en-effet-partiel">Interprétation de la régression multiple en effet partiel</h5>
<p>L’équation estimée par les MCO de la RLM est comme suit&nbsp;:</p>
<p><span class="math display">\[\widehat{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{1} + \widehat{\beta_{2}}x_{2} + \ldots + \widehat{\beta_{k}}x_{k}\]</span></p>
<p><span class="math inline">\(\widehat{\beta_{0}}\)</span> représente l’ordonnée à l’origine lorsque les <span class="math inline">\(x_{j}\)</span> sont égales à zéro. Dans certains cas, l’estimation de l’ordonnée à l’origine donne des informations intéressantes&nbsp;; dans d’autres, elle n’a pas de sens. Notons, cependant que cette ordonnée à l’origine est indispensable si nous désirons obtenir une estimation <span class="math inline">\(y\)</span> à partir de la régression des MCO lorsque les <span class="math inline">\(x_{j}\)</span> sont égales à zéro. Sans cette ordonnée à l’origine la valeur du <span class="math inline">\(y\)</span> sera toujours nulle lorsque les <span class="math inline">\(x_{j}\)</span> sont égales à zéro.</p>
<p>Les estimations des <span class="math inline">\(\widehat{\beta_{j}}\)</span> s’interprètent comme des <strong>effets marginaux</strong> ou des <strong>effets ceteris paribus</strong>. De l’équation estimée, nous déduisons que&nbsp;:</p>
<p><span class="math display">\[\Delta\widehat{y} = \widehat{\beta_{1}}{\Delta x}_{1} + \widehat{\beta_{2}}{\Delta x}_{2} + \ldots + \widehat{\beta_{k}}{\Delta x}_{k}\]</span></p>
<p>Toutes choses étant égales par ailleurs, c’est-à-dire si toutes les autres variables sont maintenues constantes ou fixées, le coefficient de <span class="math inline">\(x_{1}\)</span> mesure le changement de <span class="math inline">\(\widehat{y}\)</span> dû à une variation d’une unité de <span class="math inline">\(x_{1}\)</span>. Soit&nbsp;:</p>
<p><span class="math display">\[\Delta\widehat{y} = \widehat{\beta_{1}}{\Delta x}_{1}\]</span></p>
<p>En gardant <span class="math inline">\(x_{2},x_{3},\ldots,x_{k}\)</span> constants. De cette manière, nous <em>neutralisons</em> l’effet des variables <span class="math inline">\(x_{2},x_{3},\ldots,x_{k}\)</span> quand nous estimons l’effet de <span class="math inline">\(x_{1}\)</span> sur <span class="math inline">\(y\)</span>. Les autres coefficients ont une interprétation similaire.</p>
</section>
</section>
<section id="cas-particulier-modèle-sans-terme-constant" class="level4">
<h4 class="anchored" data-anchor-id="cas-particulier-modèle-sans-terme-constant">Cas particulier : modèle sans terme constant</h4>
<p>La théorie économique postule parfois des relations dans lesquelles <span class="math inline">\(\beta_{0} = 0\)</span> : c’est le cas par exemple pour une fonction de production de produit industriel où le facteur de production (unique) nul entraîne une production nulle. L’estimation de <span class="math inline">\(\beta_{1}\)</span> est alors donnée par la formule suivante :</p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{N}{x_{i}y_{i}}}{\sum_{i = 1}^{N}x_{i}^{2}}\]</span></p>
<p>Nous remarquons qu’il s’agit de l’application de la formule générale dans laquelle <span class="math inline">\(\overline{x}\)</span> et <span class="math inline">\(\overline{y}\)</span> sont nulles. Dans le cas de variables centrées, c’est donc cette dernière formule qu’il convient d’employer car le terme constant est nul.</p>
<p>Notons que, les données sont centrées lorsque les observations sont centrées sur leur moyenne&nbsp;: <span class="math inline">\(\left( x_{i} - \overline{x} \right)\)</span>, la somme des données centrées est donc par construction nulle.</p>
</section>
<section id="propriétés-des-estimateurs-des-mco" class="level4">
<h4 class="anchored" data-anchor-id="propriétés-des-estimateurs-des-mco">Propriétés des estimateurs des MCO</h4>
<p><strong>Propriétés 1&nbsp;: Les estimateurs MCO sont sans biais&nbsp;</strong></p>
<p>Par définition le biais d’un paramètre <span class="math inline">\(\theta\)</span> dont l’estimateur est <span class="math inline">\(\widehat{\theta}\)</span> est donné par&nbsp;:</p>
<p><span class="math display">\[\mathbf{Biais = E}\left( \widehat{\mathbf{\theta}} \right)\mathbf{- \theta}\]</span></p>
<p>Ainsi<strong>,</strong> l’estimateur des MCO est <strong>sans biais</strong> si&nbsp;<strong>:</strong> <span class="math inline">\(\mathbf{E}\left( \widehat{\mathbf{\beta}_{\mathbf{i}}} \right)\mathbf{=}\mathbf{\beta}_{\mathbf{i}}\)</span></p>
<p><strong><em>NB&nbsp;: Démontrer les équivalences suivantes (Annexe B de Wooldridge)</em></strong></p>
<p><span class="math inline">\(\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right) = \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)y_{i}\)</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}\left( y_{i} - \overline{y} \right)^{2} = \sum_{i = 1}^{N}{y_{i}}^{2} - n{\overline{y}}^{2}\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)x_{i} = \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} = SCE\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right) = 0\]</span></p>
<p><strong>1<sup>er</sup> cas&nbsp;: L’intercepte</strong></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)(y_{i} - \overline{y})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)y_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)(\beta_{0} + \beta_{1}x_{i} + \varepsilon_{i})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)(\beta_{0} + \beta_{1}x_{i} + \varepsilon_{i})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\beta_{0}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right) + \beta_{1}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)x_{i} + \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p>Sachant que <span class="math inline">\(\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right) = 0\)</span> et <span class="math inline">\(\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)x_{i} = \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\)</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \beta_{1} + \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[E(\widehat{\beta_{1})} = E(\beta_{1}) + \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)E(\varepsilon_{i})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[\mathbf{E}\left( \widehat{\mathbf{\beta}_{\mathbf{1}}} \right)\mathbf{=}\mathbf{\beta}_{\mathbf{1}}\]</span></p>
<p><strong>2<sup>e</sup> cas&nbsp;: La constante</strong></p>
<p><span class="math display">\[\widehat{\beta_{0}} = \overline{y} - \widehat{\beta_{1}}\overline{x}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{0}} = \beta_{0} + \beta_{1}\overline{x} + \overline{\varepsilon} - \widehat{\beta_{1}}\overline{x}\]</span></p>
<p><span class="math display">\[\widehat{\beta_{0}} = \beta_{0} - (\widehat{\beta_{1}} - \beta_{1})\overline{x} + \overline{\varepsilon}\]</span></p>
<p>Or <span class="math inline">\(E\left( \widehat{\beta_{1}} - \beta_{1} \right) = 0\)</span>, donc&nbsp;:</p>
<p><span class="math display">\[E(\widehat{\beta_{0})} = {E(\beta}_{0}) + E(\overline{\varepsilon})\]</span></p>
<p><span class="math display">\[\mathbf{E(}\widehat{\mathbf{\beta}_{\mathbf{0}}}\mathbf{) =}\mathbf{\beta}_{\mathbf{0}}\]</span></p>
<p>En conclusion, les estimateurs des MCO sont sans biais si et seulement si les deux hypothèses suivantes sont respectées :</p>
<ul>
<li><p><strong>Hypothèse 2</strong>&nbsp;: L’exogène <span class="math inline">\(\mathbf{x}\)</span> n’est pas stochastique (X est non aléatoire) ;</p></li>
<li><p><strong>Hypothèse 3</strong>&nbsp;: <span class="math inline">\(E\left( \varepsilon_{i} \right) = 0\)</span>, l’espérance de l’erreur est nulle.</p></li>
</ul>
<p><strong>Propriété2&nbsp;: Les estimateurs MCO sont convergents&nbsp;</strong></p>
<p>Un estimateur <span class="math inline">\(\widehat{\theta}\)</span> est <strong>convergent si&nbsp;:</strong> <span class="math inline">\(\mathbf{Lim\ V}\left( \widehat{\theta} \right)\mathbf{\rightarrow 0\ lorsque\ n \rightarrow \infty}\)</span></p>
<p><strong>1<sup>er</sup> cas&nbsp;: L’intercepte</strong></p>
<p>Calculons la Variance de <span class="math inline">\(\widehat{\beta_{1}}\ \)</span>:</p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)(y_{i} - \overline{y})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p>Or <span class="math inline">\(\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right) = \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)y_{i}\)</span></p>
<p>On a donc&nbsp;:</p>
<p><span class="math display">\[\widehat{\beta_{1}} = \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)y_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[Var\left( \widehat{\beta_{1}} \right) = Var\left( \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)y_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}} \right) = \left( \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}} \right)^{2}Var\left( y_{i} \right)\]</span></p>
<p><span class="math display">\[\mathbf{V}\left( \widehat{\mathbf{\beta}_{\mathbf{1}}} \right)\mathbf{=}\frac{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)^{\mathbf{2}}}\mathbf{=}\frac{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}{\mathbf{SCE}}\]</span></p>
<p><strong>Une autre méthode de calculer la variance de</strong> <span class="math inline">\(\widehat{\mathbf{\beta}_{\mathbf{1}}}\)</span></p>
<p><span class="math display">\[\widehat{\beta_{1}} = \beta_{1} + \ \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[V\left( \widehat{\beta_{1}} \right) = Var\left( \beta_{1} + \ \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}} \right)\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \  = Var\left( \beta_{1} \right) + Var\left( \frac{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}} \right)\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \  = 0 + \frac{\frac{1}{n^{2}}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}Var(\varepsilon_{i})}{\frac{1}{n^{2}}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{4}} = \frac{\frac{1}{n}Var(x)Var(\varepsilon_{i})}{{(Var(x))}^{2}}\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \  = \frac{Var(\varepsilon_{i})}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p>Or d’après les hypothèses MCO&nbsp;: <span class="math inline">\(Var(\varepsilon_{i}) = \ \sigma_{\varepsilon}^{2}\)</span>, donc&nbsp;:</p>
<p><span class="math display">\[\mathbf{V}\left( \widehat{\mathbf{\beta}_{\mathbf{1}}} \right)\mathbf{=}\frac{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)^{\mathbf{2}}}\mathbf{=}\frac{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}{\mathbf{SCE}}\]</span></p>
<p><strong>Convergence de</strong> <span class="math inline">\(\widehat{\mathbf{\beta}_{\mathbf{1}}}\)</span></p>
<p><span class="math display">\[lorsque\ n \rightarrow \infty\ alors\ \sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\ tend\ également\ vers\ \infty,\ d^{'}où\ V\left( \widehat{\beta_{1}} \right) \rightarrow 0\ car\ \sigma^{2} = cst\]</span></p>
<p>Il s’en suit donc que <span class="math inline">\(\widehat{\mathbf{\beta}_{\mathbf{1}}}\)</span> <strong>est estimateur convergent</strong>.</p>
<p><strong>2<sup>e</sup> cas&nbsp;: La constante</strong></p>
<p>Calculons la Variance de <span class="math inline">\(\widehat{\beta_{0}}\ \)</span>:</p>
<p><span class="math display">\[Var(\widehat{\beta_{0})} = Var(\overline{y} - \widehat{\beta_{1}}\overline{x})\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = Var\left( \overline{y} \right) + Var(\widehat{\beta_{1})}{\overline{x}}^{2} - 2\overline{x}cov\left( \overline{y},\widehat{\beta_{1}} \right)\]</span></p>
<p>On montre que <span class="math inline">\(cov\left( \overline{y},\widehat{\beta_{1}} \right) = 0\)</span>. On a donc&nbsp;:</p>
<p><span class="math display">\[\ Var(\widehat{\beta_{0})} = Var(\frac{1}{n}\sum_{i = 1}^{n}y_{i}) - \frac{\sigma_{\varepsilon}^{2}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}{\overline{x}}^{2}\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \frac{1}{n^{2}}\sum_{i = 1}^{n}{y_{i}}^{2} - \frac{\sigma_{\varepsilon}^{2}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}{\overline{x}}^{2}\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \frac{1}{n}Var(y_{i}) - \frac{\sigma_{\varepsilon}^{2}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}{\overline{x}}^{2}\]</span></p>
<p><span class="math display">\[\mathbf{V}\left( \widehat{\mathbf{\beta}_{\mathbf{0}}} \right)\mathbf{=}\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}\left( \frac{\mathbf{1}}{\mathbf{n}}\mathbf{+}\frac{{\overline{\mathbf{x}}}^{\mathbf{2}}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)^{\mathbf{2}}} \right)\mathbf{=}\mathbf{\sigma}^{\mathbf{2}}\left( \frac{\mathbf{1}}{\mathbf{n}}\mathbf{+}\frac{{\overline{\mathbf{x}}}^{\mathbf{2}}}{\mathbf{SCE}} \right)\]</span></p>
<p><span class="math display">\[\mathbf{lorsque\ n \rightarrow \infty\ alors\ Lim\ }\mathbf{V}\left( \widehat{\mathbf{\beta}_{\mathbf{0}}} \right)\mathbf{\rightarrow 0}\]</span></p>
<p>On démontre ainsi que <span class="math inline">\(\widehat{\mathbf{\beta}_{\mathbf{0}}}\)</span> est aussi convergent comme <span class="math inline">\(\widehat{\mathbf{\beta}_{\mathbf{1}}}\)</span>.</p>
<p><strong>Théorème de Gauss-Markov</strong></p>
<p>Selon le théorème de Gauss-Markov, les estimateurs des MCO de la régression sont sans biais et convergents. On peut même aller plus loin et prouver que parmi les estimateurs linéaires sans biais de la régression, les estimateurs MCO sont à variance minimale c.-à-d.&nbsp;il n’existe pas d’autres estimateurs linéaires sans biais présentant une plus petite variance. Les estimateurs des MCO sont BLUE (Best Linear Unbiased Estimator). On dit qu’ils sont efficaces.</p>
</section>
<section id="autres-propriétés-algébriques-des-mco" class="level4">
<h4 class="anchored" data-anchor-id="autres-propriétés-algébriques-des-mco">Autres propriétés algébriques des MCO</h4>
<p><strong>Propriété 1 : La moyenne des résidus est nulle :</strong></p>
<p><span class="math display">\[\frac{1}{N}\sum_{i = 1}^{N}{e_{i} = 0}\]</span></p>
<p>En effet</p>
<p><span class="math display">\[\frac{1}{N}\sum_{i = 1}^{N}e_{i} = \frac{1}{N}\sum_{i = 1}^{N}\left\lbrack y_{i} - \left( \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} \right) \right\rbrack = \overline{y} - \widehat{\beta_{0}} - \widehat{\beta_{1}}\overline{x} = \overline{y} - \left( \overline{y} - \widehat{\beta_{1}}\overline{x} \right) - \widehat{\beta_{1}}\overline{x} = 0\]</span></p>
<p><strong>Propriété 2</strong> : La covariance entre les résidus et les valeurs de la variable explicative est nulle :</p>
<p><span class="math display">\[\sum_{i = 1}^{N}{x_{i}e_{i} = 0}\]</span></p>
<p><strong>Propriété 3</strong> : La régression passe par le point moyen de l’échantillon :</p>
<p><span class="math display">\[\overline{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}\overline{x}\]</span></p>
<p><strong>Propriété 4</strong> : La somme des valeurs observées de <span class="math inline">\(y_{i}\)</span> est égale à la somme des valeurs ajustées <span class="math inline">\(\widehat{y_{i}}\)</span>.</p>
<p><span class="math display">\[\sum_{i = 1}^{N}y_{i} = \sum_{i = 1}^{N}\widehat{y_{i}}\]</span></p>
<p><strong>Propriété 5</strong> : La covariance entre les résidus et les valeurs prédites sont nulle :</p>
<p><span class="math display">\[\sum_{i = 1}^{N}{\widehat{y_{i}}e_{i}} = \sum_{i = 1}^{N}\left( \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} \right)e_{i} = \widehat{\beta_{0}}\sum_{i = 1}^{N}e_{i} + \widehat{\beta_{1}}\sum_{i = 1}^{N}{x_{i}e_{i}} = 0\]</span></p>
<p><strong>Propriété 6&nbsp;: L’estimateur de la variance des erreurs est sans biais</strong></p>
<p>Nous allons commencer par montrer la différence entre les <em>erreurs</em> (ou perturbations) et les <em>résidus</em>.</p>
<p>A partir des MCO, la valeur prédite de <span class="math inline">\(\widehat{y_{i}}\)</span> conditionnellement à <span class="math inline">\(x_{i}\)</span> est :</p>
<p><span class="math display">\[\widehat{y_{i}} = \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i}\]</span></p>
<p>Le modèle de régression simple peut s’écrire sous deux formes selon qu’il s’agit<br>
du modèle théorique spécifié par l’économiste ou du modèle estimé à partir d’un<br>
échantillon.</p>
<p>Modèle théorique spécifié par l’économiste avec <span class="math inline">\(\varepsilon_{i}\)</span> l’erreur inconnue :</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}\]</span></p>
<p>Modèle estimé à partir d’un échantillon d’observations :</p>
<p><span class="math display">\[y_{i} = \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} + e_{i}\]</span></p>
<p><span class="math display">\[\mathbf{y}_{\mathbf{i}}\mathbf{=}\widehat{\mathbf{y}_{\mathbf{i}}}\mathbf{+}\mathbf{e}_{\mathbf{i}}\]</span></p>
<p>La droite de régression des MCO est :</p>
<p><span class="math display">\[\widehat{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i}\]</span></p>
<p>Le résidu des MCO est :</p>
<p><span class="math display">\[e_{i} = y_{i} - \widehat{y_{i}} = \left( \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i} \right) - \left( \widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i} \right)\]</span></p>
<p><span class="math display">\[\mathbf{e}_{\mathbf{i}}\mathbf{=}\mathbf{\varepsilon}_{\mathbf{i}}\mathbf{+}\left( \mathbf{\beta}_{\mathbf{0}}\mathbf{-}\widehat{\mathbf{\beta}_{\mathbf{0}}} \right)\mathbf{+ (}\mathbf{\beta}_{\mathbf{1}}\mathbf{-}\mathbf{\beta}_{\mathbf{1}}\mathbf{)}\mathbf{x}_{\mathbf{i}}\]</span></p>
<p>Le résidu observé <span class="math inline">\(e_{i}\)</span> est donc la différence entre les valeurs observées de la variable à expliquer et les valeurs ajustées à l’aide des estimations des coefficients du modèle. Les erreurs ne peuvent jamais être observées alors que les résidus sont calculés à partir d’une base de données.</p>
<p>On constate que le résidu <span class="math inline">\(e_{i}\)</span> n’est pas égal à l’erreur <span class="math inline">\(\varepsilon_{i}\)</span>. C’est la différence <em>attendue</em> entre ces deux termes qui est égale à zéro comme pour <span class="math inline">\(\beta_{0}\)</span> et <span class="math inline">\(\widehat{\beta_{0}}\)</span>, d’une part et <span class="math inline">\(\beta_{1}\)</span> et <span class="math inline">\(\widehat{\beta_{1}}\)</span>, d’autre part.</p>
<p>Maintenant que nous comprenons la différence entre les erreurs et les résidus, nous pouvons estimer la variance de l’erreur <span class="math inline">\(\sigma^{2}\)</span>. Comme <span class="math inline">\(\mathbf{\sigma}^{\mathbf{2}}\mathbf{= E(}{\varepsilon_{i}}^{\mathbf{2}}\mathbf{)}\)</span>, on pourrait penser que <span class="math inline">\(\frac{1}{n}\sum_{i = 1}^{n}{\varepsilon_{i}}^{\mathbf{2}}\)</span> est un estimateur sans biais de <span class="math inline">\(\sigma^{2}\)</span>. Ce n’est malheureusement pas le cas pour la simple raison qu’il est impossible d’observer les erreurs <span class="math inline">\(\varepsilon_{i}\)</span>. La bonne nouvelle est que nous pouvons remplacer les erreurs par les résidus. Nous obtenons alors&nbsp;:</p>
<p><span class="math display">\[\frac{1}{n}\sum_{i = 1}^{n}{\mathbf{e}_{\mathbf{i}}}^{\mathbf{2}} = \frac{SCR}{n}\]</span></p>
<p>Il s’agit d’un «&nbsp;vrai estimateur&nbsp;» car il offre une règle de calcul qui s’applique à n’importe quel échantillon de données. L’inconvénient de cet estimateur est qu’il est biaisé, bien que ce biais soit négligeable lorsque <span class="math inline">\(n\)</span> est grand. L’estimateur <span class="math inline">\(\frac{SCR}{n}\)</span> est biaisé pour la principale raison qu’il ne tient pas compte de deux contraintes que les résidus des MCO doivent respecter, à savoir&nbsp;:</p>
<p><span class="math inline">\(\sum_{i = 1}^{N}{e_{i} = 0}\)</span> et <span class="math inline">\(\sum_{i = 1}^{N}{x_{i}e_{i} = 0}\)</span></p>
<p>En tenant compte de ces deux contraintes, nous perdons deux degrés de liberté. Si nous connaissons la valeur des <span class="math inline">\(n - 2\)</span> résidus dans notre échantillon, nous sommes contraints de choisir les deux derniers résidus de sorte que ces conditions soient respectées. C’est la raison pour laquelle, il n’y a que <span class="math inline">\(n - 2\)</span> degrés de liberté dans les résidus. Ainsi, l’estimateur sans biais de <span class="math inline">\(\sigma^{2}\)</span>que nous allons utiliser doit tenir compte de l’ajustement relatif aux degrés de liberté.</p>
<p><span class="math display">\[\widehat{\sigma^{2}} = \frac{1}{n - 2}\sum_{i = 1}^{n}{\mathbf{e}_{\mathbf{i}}}^{\mathbf{2}} = \frac{SCR}{n - 2}\]</span></p>
<p>On a&nbsp;:</p>
<p><span class="math display">\[SCR = \sum_{i = 1}^{N}{e_{i}}^{2} = \sum_{i = 1}^{N}{(y_{i} - \widehat{y_{i}})}^{2}\]</span></p>
<p><span class="math display">\[SCR = \sum_{i = 1}^{N}{e_{i}}^{2} = \sum_{i = 1}^{N}{(y_{i} - (\widehat{\beta_{0}} + \widehat{\beta_{1}}x_{i}))}^{2} = \sum_{i = 1}^{N}{y_{i}}^{2} - n{\overline{y}}^{2} - \widehat{\beta_{1}}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}\]</span></p>
<p>Or,</p>
<p><span class="math display">\[\sum_{i = 1}^{N}{y_{i}}^{2} - n{\overline{y}}^{2} = \sum_{i = 1}^{n}\left( y_{i} - \overline{y} \right)^{2} = SCT\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} = SCE\]</span></p>
<p><span class="math display">\[SCR = SCT - \widehat{\beta_{1}}SCE\]</span></p>
<p><span class="math display">\[\widehat{\mathbf{\sigma}^{\mathbf{2}}}\mathbf{=}\frac{\mathbf{SCT}\mathbf{-}\widehat{\beta_{1}}\mathbf{SCE}}{\mathbf{n - 2}}\]</span></p>
</section>
</section>
<section id="la-méthode-du-maximum-de-vraisemblance" class="level3">
<h3 class="anchored" data-anchor-id="la-méthode-du-maximum-de-vraisemblance">La méthode du maximum de vraisemblance</h3>
<p>La méthode du <strong>maximum de vraisemblance (MV</strong>) (en anglais « Maximum Likelihood method » ou ML method) permet aussi d’estimer les paramètres d’un modèle de régression, sous l’hypothèse que la vraie (loi) distribution desdits paramètres est connue.</p>
<p>Si le principe pour les MCO est de trouver le paramètre qui minimise la somme des carrés des erreurs, la méthode du maximum de vraisemblance cherche par contre à trouver le paramètre à même (avec une forte probabilité) de reproduire les vraies valeurs de l’échantillon (celles réellement observées). Autrement dit, l’estimation par la méthode du maximum de vraisemblance cherche à trouver les valeurs des paramètres qui rendent l’observation de l’échantillon le plus vraisemblable.</p>
<p>L’on notera aussi que, sous l’hypothèse que les erreurs sont normalement distribuées <strong><em><br>
</em></strong><span class="math inline">\(\mathbf{\varepsilon}\mathcal{↝ N}\mathbf{(0,}\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}\mathbf{\ }\mathbf{)}\)</span>, les estimateurs des MCO et ceux du maximum de vraisemblance sont identiques.</p>
<p>Supposons que, dans le modèle de régression linéaire simple comme suit&nbsp;:</p>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}\]</span></p>
<p><span class="math inline">\(Y_{i}\)</span> est linéairement indépendants du terme d’erreur <span class="math inline">\(\varepsilon_{i}\)</span> et suit une distribution normale, avec une moyenne&nbsp;: <span class="math inline">\(E\left( Y_{i} \right) = \beta_{0} + \beta_{1}X_{i}\)</span> et une variance = <span class="math inline">\(V\left( Y_{i} \right) = \sigma^{2}\)</span></p>
<p>On sait que&nbsp;: <span class="math inline">\(\varepsilon_{i}\mathcal{↝ N}(0,\sigma_{\varepsilon}^{2}\mathbf{)}\)</span><strong>,</strong> alors <span class="math inline">\(E\left( \varepsilon_{i} \right) = 0\)</span>. On en déduit que&nbsp;:</p>
<p><span class="math display">\[\mathbf{E}\left( \mathbf{Y}_{\mathbf{i}} \right)\mathbf{= E}\left( \mathbf{\beta}_{\mathbf{0}}\mathbf{+}\mathbf{\beta}_{\mathbf{1}}\mathbf{X}_{\mathbf{i}}\mathbf{+}\mathbf{\varepsilon}_{\mathbf{i}} \right)\mathbf{=}\mathbf{\beta}_{\mathbf{0}}\mathbf{+}\mathbf{\beta}_{\mathbf{1}}\mathbf{X}_{\mathbf{i}}\]</span></p>
<p>Puisque&nbsp;:</p>
<p><span class="math display">\[\mathbf{Y}_{\mathbf{i}}\mathbf{- E}\left( \mathbf{Y}_{\mathbf{i}} \right)\mathbf{=}\left( \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i} \right)\mathbf{-}\left( \beta_{0} + \beta_{1}X_{i} \right)\mathbf{=}\varepsilon_{i}\]</span></p>
<p>Alors&nbsp;:</p>
<p><span class="math display">\[\mathbf{Var}\left( \mathbf{Y}_{\mathbf{i}} \right)\mathbf{=}{\mathbf{E}\left\lbrack \mathbf{Y}_{\mathbf{i}}\mathbf{- E}\left( \mathbf{Y}_{\mathbf{i}} \right) \right\rbrack}^{\mathbf{2}}\mathbf{=}{\mathbf{E}\left\lbrack \varepsilon_{i} \right\rbrack}^{\mathbf{2}}\mathbf{=}\sigma_{\varepsilon}^{2}\]</span></p>
<p>D’où, la distribution de <span class="math inline">\(\mathbf{Y}_{\mathbf{i}}\mathbf{\ }\)</span>est comme suit&nbsp;:</p>
<p><span class="math display">\[Y_{i}\mathcal{↝ N}\left\lbrack \left( \beta_{0} + \beta_{1}X_{i} \right)\mathbf{;}\sigma_{\varepsilon}^{2} \right\rbrack\]</span></p>
<p>On définit une fonction de densité de probabilité jointe qui, compte tenu de l'indépendance des Y, peut s'écrire comme un produit de n fonctions de densité individuelles, soit :</p>
<p><span class="math display">\[f\left( Y_{1},Y_{2},\ldots Y_{n} \middle| \beta_{0} + \beta_{1}X_{i},\sigma_{\varepsilon}^{2} \right) = f\left( Y_{1} \middle| \beta_{0} + \beta_{1}X_{i},\ \sigma_{\varepsilon}^{2} \right) \times \left( Y_{2} \middle| \beta_{0} + \beta_{1}X_{i},\ \sigma_{\varepsilon}^{2} \right) \times \ldots \times \left( Y_{n} \middle| \beta_{0} + \beta_{1}X_{i},\ \sigma_{\varepsilon}^{2} \right)\]</span></p>
<p><span class="math display">\[= \prod_{i = 1}^{n}{f\left( Y_{i} \right)\ }\]</span></p>
<p>Où la <span class="math inline">\(f\left( Y_{i} \right)\)</span> est la fonction de densité de la loi normale de moyenne <span class="math inline">\(\beta_{0} + \beta_{1}X_{i}\)</span> et de variance <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>.</p>
<p><span class="math display">\[f\left( Y_{i} \right) = \frac{1}{\sqrt{2\pi\sigma_{\varepsilon}}}\exp\left\{ - \frac{1}{2}\frac{\left( Y_{i} - \beta_{0} - \beta_{1}X_{i} \right)²}{\sigma_{\varepsilon}^{2}} \right\}\]</span></p>
<p>La fonction de densité de probabilité jointe devient alors&nbsp;:</p>
<p><span class="math display">\[\begin{matrix}
f\left( Y_{1},Y_{2},\ldots Y_{n} \middle| \beta_{0} + \beta_{1}X_{i},\sigma_{\varepsilon}^{2} \right) = \prod_{i = 1}^{n}{\frac{1}{\sqrt{2\pi\sigma_{\varepsilon}}}\exp\left\{ - \frac{1}{2}\frac{\left( Y_{i} - \beta_{0} - \beta_{1}X_{i} \right)²}{\sigma^{2}} \right\}} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \frac{1}{{\sqrt{2\pi\sigma_{\varepsilon}}}^{n}}\exp\left\{ - \frac{1}{2}\sum_{}^{}\frac{\left( Y_{i} - \beta_{0} - \beta_{1}X_{i} \right)²}{\sigma_{\varepsilon}^{2}} \right\} \\
\end{matrix}\]</span></p>
<p>La fonction précédente est appelée <strong>fonction de vraisemblance</strong>, notée <span class="math inline">\(L\left( \beta_{0},\beta_{1},\sigma_{\varepsilon}^{2} \right)\)</span> que l’on écrit :</p>
<p><span class="math display">\[L\left( \beta_{0},\beta_{1},\sigma_{\varepsilon}^{2} \right) = \frac{1}{{\sqrt{2\pi\sigma_{\varepsilon}}}^{n}}\exp\left\{ - \frac{1}{2}\sum_{}^{}\frac{\left( Y_{i} - \beta_{0} - \beta_{1}X_{i} \right)²}{\sigma_{\varepsilon}^{2}} \right\}\]</span></p>
<p>Lorsqu’on effectue une transformation logarithmique de la fonction de vraisemblance ci-dessus, l’on obtient la <strong>fonction</strong> <strong>log-vraisemblance</strong> qui servira de base à l’estimation des paramètres <span class="math inline">\(\widetilde{\mathbf{\beta}_{\mathbf{0}}}\ \)</span>et <span class="math inline">\(\widetilde{\mathbf{\beta}_{\mathbf{1}}}\)</span> du <strong>maximum de vraisemblance.</strong> La fonction log-vraisemblance s’écrit&nbsp;:</p>
<p><span class="math display">\[\ln{L\left( \beta_{0},\beta_{1},\sigma_{\varepsilon}^{2} \right)} = - n\ln\sigma_{\varepsilon} - \frac{n}{2}\ln{(2\pi}) - \frac{1}{2}\sum_{}^{}\frac{\left( Y_{i} - \beta_{0} - \beta_{1}X_{i} \right)²}{\sigma_{\varepsilon}^{2}}\]</span></p>
<p>Considérant que&nbsp;: <span class="math inline">\(\ln\sigma_{\varepsilon}^{2} = 2\ln\sigma_{\varepsilon}\)</span> alors, <span class="math inline">\(\frac{1}{2}\ln\sigma^{2} = \ln\sigma_{\varepsilon}\)</span>, on peut alors fonction log-vraisemblance comme suit&nbsp;:</p>
<p><span class="math display">\[\mathbf{\ln}{\mathbf{L}\left( \mathbf{\beta}_{\mathbf{0}}\mathbf{,}\mathbf{\beta}_{\mathbf{1}}\mathbf{,}\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}} \right)}\mathbf{= -}\frac{\mathbf{n}}{\mathbf{2}}\mathbf{\ln}\mathbf{\sigma}^{\mathbf{2}}\mathbf{-}\frac{\mathbf{n}}{\mathbf{2}}\mathbf{\ln}{\mathbf{(2}\mathbf{\pi}}\mathbf{) -}\frac{\mathbf{1}}{\mathbf{2}}\sum_{}^{}\frac{\left( \mathbf{Y}_{\mathbf{i}}\mathbf{-}\mathbf{\beta}_{\mathbf{0}}\mathbf{-}\mathbf{\beta}_{\mathbf{1}}\mathbf{X}_{\mathbf{i}} \right)\mathbf{²}}{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}\]</span></p>
<p>Pour estimer les paramètres <span class="math inline">\(\widetilde{\beta_{0}},\widetilde{\beta_{1}}\ \)</span>et <span class="math inline">\({\widetilde{\sigma}}_{\varepsilon}^{2}\)</span> par le maximum de vraisemblance, la démarche va consister à maximiser la fonction log-vraisemblance ci-dessus. Ce qui revient à annuler ses dérivées premières par rapport aux arguments <span class="math inline">\(\beta_{0},\beta_{1}\)</span> et <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>. On obtient&nbsp;:</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\frac{\partial\ln{L(.)}}{\partial\beta_{0}} = - \frac{1}{{\widetilde{\sigma}}_{\varepsilon}^{2}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right) = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)\ \ \ } \\
\frac{\partial\ln{L(.)}}{\partial\beta_{1}} = - \frac{1}{{\widetilde{\sigma}}_{\varepsilon}^{2}}\sum_{}^{}\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)X_{i}\  = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)\  \\
\frac{\partial\ln{L(.)}}{\partial\sigma_{\varepsilon}^{2}} = - \frac{n}{2{\widetilde{\sigma}}_{\varepsilon}^{2}} + \frac{1}{2{\widetilde{\sigma}}_{\varepsilon}^{4}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)^{2} = 0}\ \ \ \ \ \ \ \ \ \ \ (3) \\
\end{matrix} \right.\
\]</span></p>
<p><span class="math display">\[\left\{ \begin{matrix}
\frac{1}{{\widetilde{\sigma}}_{\varepsilon}^{2}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right) = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)\ \ \ } \\
\frac{1}{{\widetilde{\sigma}}_{\varepsilon}^{2}}\sum_{}^{}\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)X_{i}\  = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)\  \\
- \frac{n}{2{\widetilde{\sigma}}_{\varepsilon}^{2}} + \frac{1}{2{\widetilde{\sigma}}_{\varepsilon}^{4}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)^{2} = 0}\ \ \ \ \ \ \ \ \ \ \ (3) \\
\end{matrix} \right.\ \]</span></p>
<p>Alors simplification, les équations 1 et 2 donnent :</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\sum_{}^{}Y_{i} = n\widetilde{\beta_{0}} + \widetilde{\beta_{1}}\sum_{}^{}X_{i}\ \ \ \ \ \ \ \ \ \ \ \ \ \  \\
\sum_{}^{}{Y_{i}X_{i}} = \widetilde{\beta_{0}}\sum_{}^{}X_{i} + \widetilde{\beta_{1}}\sum_{}^{}X_{i}^{2} \\
\end{matrix} \right.\ \]</span></p>
<p>On a donc&nbsp;:</p>
<p><span class="math display">\[\widetilde{\mathbf{\beta}_{\mathbf{0}}}\mathbf{=}\overline{\mathbf{y}}\mathbf{-}\widetilde{\mathbf{\beta}_{\mathbf{1}}}\overline{\mathbf{x}}\]</span></p>
<p>et</p>
<p><span class="math display">\[\widetilde{\mathbf{\beta}_{\mathbf{1}}}\mathbf{=}\frac{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)\mathbf{(}\mathbf{y}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{y}}\mathbf{)}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}\left( \mathbf{x}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{x}} \right)^{\mathbf{2}}}\]</span></p>
<p>On en déduit que les estimateurs <span class="math inline">\(\widetilde{\mathbf{\beta}_{\mathbf{0}}}\ \)</span>et <span class="math inline">\(\widetilde{\mathbf{\beta}_{\mathbf{1}}}\)</span> du maximum de vraisemblance sont identiques à ceux obtenus par la méthode des MCO.</p>
<p>Après simplification de l'équation 3, on obtient l'estimateur MV de <span class="math inline">\({\widetilde{\sigma}}_{\varepsilon}^{2}\ \)</span>:</p>
<p><span class="math display">\[\frac{n}{2{\widetilde{\sigma}}_{\varepsilon}^{2}} + \frac{1}{2{\widetilde{\sigma}}_{\varepsilon}^{4}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)^{2} = 0}\]</span></p>
<p><span class="math display">\[- n + \frac{1}{2{\widetilde{\sigma}}_{\varepsilon}^{2}}\sum_{}^{}{\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)^{2} = 0}\]</span></p>
<p><span class="math display">\[{\widetilde{\sigma}}_{\varepsilon}^{2} = \frac{1}{n}\sum_{}^{}\left( Y_{i} - \widetilde{\beta_{0}} - \widetilde{\beta_{1}}X_{i} \right)^{2}\]</span></p>
<p><span class="math display">\[{\widetilde{\sigma}}_{\varepsilon}^{2} = \frac{1}{n}\sum_{}^{}\left( Y_{i} - \widetilde{Y_{i}} \right)^{2}\]</span></p>
<p><span class="math display">\[{\widetilde{\mathbf{\sigma}}}_{\mathbf{\varepsilon}}^{\mathbf{2}}\mathbf{=}\frac{\mathbf{1}}{\mathbf{n}}\sum_{}^{}{\mathbf{e}_{\mathbf{i}}^{\mathbf{2}}\mathbf{\ }}\]</span></p>
<p>Or l’estimateur MCO de la variance des erreurs est donné par&nbsp;:</p>
<p><span class="math display">\[{\widetilde{\sigma}}_{\varepsilon}^{2} = \frac{\mathbf{1}}{\mathbf{n - 2}}\sum_{}^{}{\mathbf{e}_{\mathbf{i}}^{\mathbf{2}}\mathbf{\ }}\]</span></p>
<p>On remarque l’estimateur du maximum de vraisemblance <span class="math inline">\({\widetilde{\mathbf{\sigma}}}_{\mathbf{\varepsilon}}^{\mathbf{2}}\)</span> est différent de l’estimateur MCO <span class="math inline">\({\widehat{\sigma}}_{\varepsilon}^{2}\)</span> qui était sans biais. On en déduit donc que l'estimateur MV est biaisé, mais il reste convergent. Autrement dit, que lorsque la taille <em>n</em> de l'échantillon augmente, les deux estimateurs tendent, à s'égaliser. Par conséquent, asymptotiquement (lorsque <em>n</em> croît indéfiniment), l'estimateur MV de <span class="math inline">\({\widetilde{\sigma}}_{\varepsilon}^{2}\)</span> est également non biaisé.</p>
<p><strong>Remarque&nbsp;:</strong></p>
<p>La méthode du Maximum de Vraisemblance est une alternative à la méthode des MCO. Elle est généralement appelée la <em>méthode des grands échantillons.</em> Elle est d'une application plus large car elle s'applique aussi aux modèles de régression non linéaires dans les paramètres. Cependant, son utilisation nécessite une bonne spécification de la forme fonctionnelle de la distribution des observations.</p>
</section>
</section>
<section id="chapitre-4-inférence-statistique" class="level2">
<h2 class="anchored" data-anchor-id="chapitre-4-inférence-statistique">Chapitre 4&nbsp;: Inférence Statistique</h2>
<section id="tests-dhypothèses-sur-les-paramètres-de-la-population-test-de-student" class="level3">
<h3 class="anchored" data-anchor-id="tests-dhypothèses-sur-les-paramètres-de-la-population-test-de-student">Tests d’hypothèses sur les paramètres de la population&nbsp;: test de Student</h3>
<p>Nous nous intéressons maintenant au problème de tests d’hypothèses sur les paramètres de la population du modèle de régression linéaire. Pour ce faire, nous devons connaitre la loi de distribution de <span class="math inline">\({\widehat{\mathbf{\beta}}}_{j}\)</span>. Selon les hypothèses du modèle de régression linéaire, nous savons que le terme d’erreur <span class="math inline">\(\varepsilon_{i}\)</span> dans la population est indépendante des variables explicatives <span class="math inline">\(x_{j}\ (j = 1,\ldots,k)\)</span> et suit une distribution normale de moyenne nulle et de variance <span class="math inline">\(\sigma^{2}\)</span><sub>,</sub> soit&nbsp;:</p>
<p><span class="math display">\[\varepsilon_{i}\ \mathcal{↝ N(}0,\sigma^{2})\]</span></p>
<p>On en déduit que&nbsp;:</p>
<p><span class="math display">\[y_{i}׀x_{i}\mathcal{↝ N(}\beta_{0} + \beta_{1}x_{i},\sigma^{2})\]</span></p>
<p>Chaque <span class="math inline">\(\widehat{\beta_{j}}\)</span> peut s’écrire comme une combinaison linéaire des erreurs <span class="math inline">\(\varepsilon_{i}\)</span> et des variables indépendantes&nbsp;:</p>
<p><span class="math display">\[\widehat{\beta_{j}} = \beta_{j} + \ \frac{\sum_{i = 1}^{n}\left( x_{ij} - \overline{x} \right)\varepsilon_{i}}{\sum_{i = 1}^{n}\left( x_{ij} - \overline{x} \right)^{2}}\]</span></p>
<p>Puisque les <span class="math inline">\(x_{i}\)</span>sont supposées non aléatoires, et étant donné que les erreurs sont indépendantes et identiquement distribuées selon une loi normale <span class="math inline">\(\mathcal{N(}0,\sigma^{2})\)</span>, il en ressort que conditionnellement aux valeurs prises dans l’échantillon par les variables indépendantes du modèle, on a&nbsp;:</p>
<p><span class="math display">\[\widehat{\beta_{j}}\mathcal{↝ N}\left( \beta_{j},Var\left( \widehat{\beta_{j}} \right) \right)\ \]</span></p>
<p>avec <span class="math inline">\(Var\left( \widehat{\beta_{j}} \right) = \frac{\mathbf{\sigma}_{\mathbf{\varepsilon}}^{\mathbf{2}}}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\)</span> dès lors, sous l’hypothèse nulle, on a&nbsp;:</p>
<p><span class="math display">\[\frac{\widehat{\beta_{j}} - \beta_{j}}{\sqrt{Var\left( \widehat{\beta_{j}} \right)}} = \frac{\widehat{\beta_{j}} - \beta_{j}}{\sigma\left( \widehat{\beta_{j}} \right)}\ \mathcal{↝ N(}0,1)\]</span></p>
<p>Or <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> est inconnu, on va donc le remplacer par son estimateur <span class="math inline">\(\widehat{\sigma^{2}}\)</span>. On a donc&nbsp;:</p>
<p><span class="math display">\[\mathbf{t}_{{\widehat{\mathbf{\beta}}}_{\mathbf{j}}}\mathbf{=}\frac{\widehat{\beta_{j}} - \beta_{j}}{\widehat{\sigma}\left( \widehat{\beta_{j}} \right)} ↝ \mathbf{t}_{\mathbf{(}\mathbf{n}\mathbf{-}\mathbf{p}\mathbf{)}}\]</span></p>
<p>Où <span class="math inline">\(p\)</span> est le nombre de paramètres du modèle.</p>
<p>Supposons que nous voulons tester l’hypothèse selon laquelle l’intercepte est différente de zéro. Ce test est également appelé test de significativité du paramètre <span class="math inline">\(\beta_{j}\)</span>. Il permet de vérifier l’influence réelle de l’exogène <span class="math inline">\(x_{i}\)</span> sur l’endogène <span class="math inline">\(y_{i}\)</span>. Les hypothèses correspondantes à ce test sont données par&nbsp;:</p>
<p><span class="math display">\[\left\{ \begin{matrix}
\mathbf{H}_{\mathbf{0}}\mathbf{:\ }\mathbf{\beta}_{\mathbf{j}}\mathbf{=}\mathbf{0} \\
\mathbf{H}_{\mathbf{1}}\mathbf{:\ }\mathbf{\beta}_{\mathbf{j}}\mathbf{\neq}\mathbf{0} \\
\end{matrix} \right.\ \]</span></p>
<p>Sous <span class="math inline">\(Ho\)</span> la statistique ou ratio de Student est donné par&nbsp;:</p>
<p><span class="math display">\[\mathbf{t}_{{\widehat{\mathbf{\beta}}}_{\mathbf{i}}}\mathbf{=}\frac{{\widehat{\mathbf{\beta}}}_{\mathbf{j}}}{{\widehat{\mathbf{\sigma}}}_{{\widehat{\mathbf{\beta}}}_{\mathbf{i}}}}\mathbf{\rightarrow}\mathbf{t}_{\mathbf{((}\mathbf{n}\mathbf{- 2)}}\]</span></p>
<p>Soit <span class="math inline">\(\alpha\ \)</span>le seuil de significativité ou risque d’erreur. La règle de décision est comme suit&nbsp;:</p>
<ul>
<li><p>Si <span class="math inline">\(t_{cal} &gt; t_{lue}(n - p)\)</span> alors on rejette <span class="math inline">\(H_{0}\)</span></p></li>
<li><p>Si <span class="math inline">\(t_{cal} &lt; t_{lue}(n - p)\)</span>, on ne rejette pas <span class="math inline">\(H_{0}\)</span></p></li>
</ul>
<p>Bien que <span class="math inline">\(H_{0}:\ \beta_{j} = 0\)</span> soit l’hypothèse la plus courante, il arrive que nous soyons amenés à tester d’autres hypothèses relatives à <span class="math inline">\(\beta_{j}\)</span> et notamment la possibilité que le paramètre prenne d’autres valeurs constantes. Dans ce cas, les hypothèses posées sont comme suit&nbsp;:</p>
<p><span class="math display">\[\left\{ \begin{matrix}
H_{0}:\ \beta_{j} = a_{j} \\
H_{1}:\ \beta_{j} \neq a_{j} \\
\end{matrix} \right.\ \]</span></p>
<p>Avec <span class="math inline">\(a_{j}\)</span> la valeur hypothétique de <span class="math inline">\(\beta_{j}\)</span> que nous stipulons. Dans ce cas, la statistique de Student, s’écrit&nbsp;:</p>
<p><span class="math display">\[t_{{\widehat{\mathbf{\beta}}}_{i}} = \frac{{\widehat{\mathbf{\beta}}}_{j} - a_{j}}{{\widehat{\sigma}}_{{\widehat{\mathbf{\beta}}}_{i}}} \rightarrow t_{((n - 2)}\]</span></p>
<p>Comme précédemment, la statistique de <span class="math inline">\(t_{{\widehat{\mathbf{\beta}}}_{i}}\)</span> mesure simplement le nombre d’écart-type qui sépare <span class="math inline">\({\widehat{\mathbf{\beta}}}_{j}\)</span> de sa valeur hypothétique <span class="math inline">\(\beta_{j}\)</span>. La formulation générale de la statistique <span class="math inline">\(t_{{\widehat{\mathbf{\beta}}}_{i}}\)</span> est donné par&nbsp;:</p>
<p><span class="math display">\[t_{{\widehat{\mathbf{\beta}}}_{i}} = \frac{(estimateur - valeur\ hypothétique)}{écart - type\ estimé}\]</span></p>
</section>
<section id="intervalle-de-confiance-des-estimateurs-des-mco" class="level3">
<h3 class="anchored">Intervalle de confiance des estimateurs des MCO</h3>
<p>Sous les hypothèses du modèle de régression linéaire, nous pouvons également facilement construire un intervalle de confiance (IC) pour un paramètre de la population <span class="math inline">\(\beta_{j}\)</span>. Cet intervalle de confiance fournit l’ensemble des valeurs possibles du paramètre de la population et pas simplement une estimation ponctuelle de cette valeur.</p>
<p>Ainsi, sous l’hypothèse alternative <span class="math inline">\(\mathbf{H}_{\mathbf{1}}:\ \mathbf{\beta}_{\mathbf{j}} \neq \mathbf{0}\)</span>, l’intervalle de confiance (<span class="math inline">\(IC\)</span>) au niveau (<span class="math inline">\(\mathbf{1 - \alpha})\)</span> c’est-à-dire avec 95% de chance de contenir le paramètre inconnu <span class="math inline">\(\mathbf{\beta}_{\mathbf{j}}\)</span> est donné par&nbsp;:</p>
<p><span class="math display">\[\mathbf{IC = \lbrack}{\widehat{\mathbf{\beta}}}_{\mathbf{j}}\mathbf{-}\mathbf{t}_{\left( \frac{\mathbf{\alpha}}{\mathbf{2}}\mathbf{;n - 2} \right)}\mathbf{\times}{\widehat{\mathbf{\sigma}}}_{{\widehat{\mathbf{\beta}}}_{\mathbf{j}}}\mathbf{\ ;\ }{\widehat{\mathbf{\beta}}}_{\mathbf{j}}\mathbf{-}\mathbf{t}_{\left( \frac{\mathbf{\alpha}}{\mathbf{2}}\mathbf{;n - 2} \right)}\mathbf{\times}{\widehat{\mathbf{\sigma}}}_{{\widehat{\mathbf{\beta}}}_{\mathbf{j}}}\mathbf{\rbrack}\]</span></p>
<p>Avec <span class="math inline">\({\widehat{\sigma}}_{{\widehat{\beta}}_{i}}\)</span>, l’estimateur de l’écart type de <span class="math inline">\({\widehat{\beta}}_{j}\)</span><em>.</em></p>
<ol start="3" type="1">
<li><h2 id="décomposition-de-la-variance-et-coefficient-de-détermination" class="anchored" data-anchor-id="intervalle-de-confiance-des-estimateurs-des-mco">Décomposition de la variance et coefficient de détermination</h2>
<ol type="1">
<li><h3 id="décomposition-de-la-variance---équation-danalyse-de-variance" class="anchored">Décomposition de la variance - Équation d’analyse de variance</h3></li>
</ol></li>
</ol>
<p>L’analyse de la variance encore appelé ANOVA (Analysis of Variance) consiste à expliquer la variance totale sur l’ensemble des échantillons en fonction de la variance due à l’interaction entre les variables du modèle (la variance expliquée par le modèle) et de la variance résiduelle aléatoire (la variance non expliquée par le modèle). Elle est fondée sur l’orthogonalité entre le vecteur des résidus estimés et de la variable prédite.</p>
<p><span class="math display">\[y_{i} = \widehat{y_{i}} + e_{i}\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}y_{i} = \sum_{i = 1}^{n}\widehat{y_{i}} + \sum_{i = 1}^{n}e_{i}\]</span></p>
<p>On a donc (en divisant par <span class="math inline">\(\frac{1}{n}\)</span>)</p>
<p><span class="math display">\[\overline{y_{i}} = \overline{\widehat{y_{i}}}\]</span></p>
<p>Car&nbsp;: <span class="math inline">\(\sum_{i = 1}^{n}{e_{i} = 0}\)</span></p>
<p><span class="math display">\[y_{i} - \overline{y_{i}} = \widehat{y_{i}} - \overline{\widehat{y_{i}}} + e_{i}\]</span></p>
<p><span class="math display">\[{(y}_{i} - \overline{y_{i}}) = (\widehat{y_{i}} - \overline{y_{i}}) + (y_{i} - \widehat{y_{i}})\]</span></p>
<p><span class="math display">\[\sum_{i = 1}^{n}{(y_{i} - \overline{y_{i}})}^{2} = \sum_{i = 1}^{n}{(\widehat{y_{i}} - \overline{y_{i}})}^{2} + \sum_{i = 1}^{n}{(y_{i} - \widehat{y_{i}})}^{2} + 2\sum_{i = 1}^{n}{(\widehat{y_{i}} - \overline{y_{i}})(y_{i} - \widehat{y_{i}})}\]</span></p>
<p>Or,</p>
<p><span class="math display">\[2\sum_{i = 1}^{n}{(\widehat{y_{i}} - \overline{y_{i}})(y_{i} - \widehat{y_{i}})} = 2\sum_{i = 1}^{n}{\widehat{y_{i}}(y_{i} - \widehat{y_{i}})} - 2\overline{y_{i}}\sum_{i = 1}^{n}{(y_{i} - \widehat{y_{i}})}\]</span></p>
<p><span class="math display">\[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = 2\sum_{i = 1}^{n}{\widehat{y_{i}}e_{i}} - 2\overline{y_{i}}\sum_{i = 1}^{n}e_{i} = 0\ (voir\ les\ hypothèses\ des\ MCO)\]</span></p>
<p>On a donc,</p>
<p><span class="math display">\[\sum_{i = 1}^{n}{(y_{i} - \overline{y_{i}})}^{2} = \sum_{i = 1}^{n}{(\widehat{y_{i}} - \overline{y_{i}})}^{2} + \sum_{i = 1}^{n}{(y_{i} - \widehat{y_{i}})}^{2}\]</span></p>
<p><span class="math display">\[\mathbf{SCT}\mathbf{=}\mathbf{SCE}\mathbf{+}\mathbf{SCR}\]</span></p>
<p>Cette équation est l’équation fondamentale de l’analyse de la variance pour les modèles de régression. La variance totale est égale à la somme de la variance expliquée et de la variance des résidus ou encore la somme des carrés totaux (SCT) est égale à la somme des carrés expliqués (SCE) et de la somme des carrés des résidus (SCR). SCT indique la variabilité expliquée par le modèle, c’est-à-dire la variation de <span class="math inline">\(y\)</span> expliquée par <span class="math inline">\(x\)</span>. SCR indique la variabilité non-expliquée par le modèle, c’est-à-dire l’écart entre les valeurs observées de <span class="math inline">\(y\)</span> et celle prédites par le modèle.</p>
<p>Deux situations extrêmes peuvent survenir&nbsp;:</p>
<ul>
<li><p>Dans le meilleur des cas, <span class="math inline">\(SCR = 0\ \)</span>: les variations de <span class="math inline">\(y\)</span> sont complétement expliquées par celles de <span class="math inline">\(x\)</span>. On a un modèle parfait. La droite de régression passe exactement par tous les points du nuage (<span class="math inline">\(\widehat{y_{i}} = y_{i}\)</span>).</p></li>
<li><p>Dans le pire des cas, <span class="math inline">\(SCE = 0\ \)</span>: <span class="math inline">\(x\)</span> n’apporte aucune information sur <span class="math inline">\(y\)</span>. Ainsi, <span class="math inline">\(\widehat{y_{i}} = \overline{y_{i}}\)</span>, la meilleure prédiction de <span class="math inline">\(y\)</span> est sa propre moyenne.</p>
<ol type="1">
<li><h3 id="le-coefficient-de-détermination" class="anchored">Le coefficient de détermination</h3></li>
</ol></li>
</ul>
<p>Il est possible de déduire un indicateur synthétique à partir de l’équation d’analyse de variance. C’est le coefficient de détermination <span class="math inline">\(R^{2}\)</span> qui permet de mesurer la qualité de l’ajustement. Il est donné par&nbsp;:</p>
<p><span class="math display">\[\mathbf{R}^{\mathbf{2}}\mathbf{=}\frac{\mathbf{SCE}}{\mathbf{SCT}}\mathbf{= 1}\mathbf{-}\frac{\mathbf{SCR}}{\mathbf{SCT}}\]</span></p>
<p><span class="math display">\[\mathbf{R}^{\mathbf{2}}\mathbf{=}\frac{\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\widehat{\mathbf{y}_{\mathbf{i}}}\mathbf{-}\overline{y_{i}}\mathbf{)}}^{\mathbf{2}}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\mathbf{y}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{y}_{\mathbf{i}}}\mathbf{)}}^{\mathbf{2}}}\mathbf{= 1}\mathbf{-}\frac{\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\mathbf{e}_{\mathbf{i}}\mathbf{)}}^{\mathbf{2}}}{\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\mathbf{y}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{y}_{\mathbf{i}}}\mathbf{)}}^{\mathbf{2}}}\]</span></p>
<p>En effet, plus la variance expliquée est proche de la variance totale, meilleur est l’ajustement du nuage de points par la droite des moindres carrés. <span class="math inline">\(R^{2}\)</span> indique la proportion de variance de <span class="math inline">\(y\)</span> expliquée par le modèle. Autrement dit, <span class="math inline">\(100*R^{2}\)</span> est le pourcentage de la variance de <span class="math inline">\(y\)</span> expliquée par <span class="math inline">\(x\)</span>.</p>
<p>Propriété&nbsp;:</p>
<ul>
<li><p><span class="math inline">\(R^{2}\)</span> est une quantité positive,</p></li>
<li><p><span class="math inline">\(0 \leq r^{2} \leq 1\)</span></p></li>
<li><p>Plus <span class="math inline">\(R^{2}\)</span> est proche de 1, meilleur sera le modèle. La connaissance des valeurs de <span class="math inline">\(x\)</span> permet de deviner avec précision celle de <span class="math inline">\(y\)</span>.</p></li>
<li><p>Lorsque <span class="math inline">\(R^{2}\)</span> est proche de 0, cela veut dire <span class="math inline">\(x\)</span> n’apporte pas d’informations utiles sur <span class="math inline">\(y\)</span>.</p></li>
</ul>
<p>Cependant, il est important de noter qu’on le peut juger de la qualité d’une régression uniquement que la base de <span class="math inline">\(R^{2}\)</span>.</p>
<section id="évaluation-globale-de-la-régression" class="level4">
<h4 class="anchored" data-anchor-id="évaluation-globale-de-la-régression">Évaluation globale de la régression</h4>
<p>Nous avions mis en avant la décomposition de la variance et le coefficient de détermination <span class="math inline">\(R^{2}\)</span> pour évaluer la qualité de l’ajustement. Le <span class="math inline">\(R^{2}\)</span> indique dans quelle proportion la variabilité de <span class="math inline">\(Y\)</span> pouvait être expliquée par <span class="math inline">\(X\)</span>. En revanche, il ne répond pas à la question : est-ce que la régression est globalement significative ? En d’autres termes, est-ce que les <span class="math inline">\(X\)</span> (il n’y en a qu’un seul pour l’instant dans la régression simple) emmènent significativement de l’information sur <span class="math inline">\(Y\)</span> représentative d’une relation linéaire réelle dans la population, et qui va au-delà̀ des simples fluctuations d’échantillonnage ?</p>
<p>Pour répondre à cette question, nous allons étendre l’étude de la décomposition de la variance par l’analyse du tableau de la variance.</p>
</section>
<section id="tableau-danalyse-de-la-variance" class="level4">
<h4 class="anchored" data-anchor-id="tableau-danalyse-de-la-variance">Tableau d’analyse de la variance</h4>
<p>Le tableau ci-dessous présente l’analyse de la variance pour un modèle de régression simple.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 64%">
<col style="width: 6%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Source de variation</th>
<th style="text-align: left;">Somme des carrés</th>
<th style="text-align: left;">Degré de liberté</th>
<th style="text-align: left;">Carrés moyens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math display">\[x\]</span></td>
<td style="text-align: left;"><span class="math display">\[\mathbf{SCE =}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\widehat{\mathbf{y}_{\mathbf{i}}}\mathbf{-}\overline{\widehat{\mathbf{y}_{\mathbf{i}}}}\mathbf{)}}^{\mathbf{2}}\]</span></td>
<td style="text-align: left;"><span class="math display">\[1\]</span></td>
<td style="text-align: left;"><span class="math display">\[\frac{\mathbf{SCE}}{\mathbf{1}}\]</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Résidus</td>
<td style="text-align: left;"><span class="math display">\[\mathbf{SCR =}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\mathbf{e}_{\mathbf{i}}\mathbf{)}}^{\mathbf{2}}\]</span></td>
<td style="text-align: left;"><span class="math display">\[n - 2\]</span></td>
<td style="text-align: left;"><span class="math display">\[\frac{\mathbf{SCR}}{\mathbf{(n}\mathbf{-}\mathbf{2)}}\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;"><span class="math display">\[\mathbf{SCT =}\sum_{\mathbf{i = 1}}^{\mathbf{n}}{\mathbf{(}\mathbf{y}_{\mathbf{i}}\mathbf{-}\overline{\mathbf{y}_{\mathbf{i}}}\mathbf{)}}^{\mathbf{2}}\]</span></td>
<td style="text-align: left;"><span class="math display">\[n - 1\]</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>À partir du tableau de l’ANOVA, nous effectuons le test de la linéarité de la régression en calculant la statistique de Fisher <span class="math inline">\(F\)</span> qui suit une loi de Fisher <span class="math inline">\(F(1,\ n - 2)\)</span> degré de liberté. Il revient à tester si la variable explicative <span class="math inline">\(x\)</span> contribue pas à l’explication du modèle. Le test d’hypothèse est le suivant&nbsp;:</p>
<p><span class="math display">\[H_{0}:\beta_{1} = 0\ \  \leftrightarrow \ H_{0}:SCE = 0\]</span></p>
<p>La statistique de Ficher est donnée par :</p>
<p><span class="math display">\[\mathbf{F}^{\mathbf{*}}\mathbf{=}\frac{\frac{\mathbf{SCE}}{\mathbf{1}}}{\frac{\mathbf{SCR}}{\mathbf{(n}\mathbf{-}\mathbf{2)}}}\mathbf{=}\left( \mathbf{n}\mathbf{-}\mathbf{2} \right)\frac{\mathbf{SCE}}{\mathbf{SCR}}\mathbf{= (n - 2)}\frac{\mathbf{R}^{\mathbf{2}}}{\mathbf{(1 -}\mathbf{R}^{\mathbf{2}}\mathbf{)}}\]</span></p>
<p>La statistique de <span class="math inline">\(F^{*}\)</span> est le rapport de la somme des carrés expliqués par <span class="math inline">\(x\)</span> et de la somme des carrés des résidus, chacune de ces sommes étant divisée par son degré de liberté respectif. Ainsi, si la variance expliquée est significativement supérieure à la variance résiduelle, la variable <span class="math inline">\(x\)</span> est considérée comme étant une variable réellement explicative.</p>
<p>La statistique <span class="math inline">\(F^{*}\)</span> suit une loi de Ficher à <span class="math inline">\(1\ et\ n - 2\)</span> degré de liberté.</p>
<ul>
<li><p>Si <span class="math inline">\(\mathbf{F}^{\mathbf{*}}\mathbf{\geq}\mathbf{F}_{\mathbf{(1,\ n}\mathbf{-}\mathbf{2)}}^{\mathbf{\alpha}}\)</span> alors nous décidons de rejeter l’hypothèse nulle <span class="math inline">\(H_{0}\)</span> et par conséquent d’accepter l’hypothèse alternative <span class="math inline">\(H_{1}\)</span> au risque <span class="math inline">\(\alpha\)</span>, c’est-à-dire qu’il existe une liaison linéaire significative entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</p></li>
<li><p>Si <span class="math inline">\(\mathbf{F}^{\mathbf{*}}\mathbf{\leq}\mathbf{F}_{\mathbf{(1,\ n}\mathbf{-}\mathbf{2)}}^{\mathbf{\alpha}}\)</span> alors nous décidons de ne pas rejeter l’hypothèse nulle <span class="math inline">\(H_{0}\)</span> et par conséquent de l’accepter, c’est-à-dire qu’il n’existe pas une liaison linéaire significative entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</p></li>
</ul>
<p>En effet, si l’hypothèse nulle <span class="math inline">\(H_{0}\)</span> est vérifiée alors cela implique que <span class="math inline">\(\rho(x,y) = 0\)</span> c’est-à-dire <span class="math inline">\(Cov(x,y) = 0\)</span>. Donc il n’existe aucune liaison linéaire entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</p>
<p>On peut encore écrire <span class="math inline">\(F^{*}\)</span> comme suit&nbsp;:</p>
<p><span class="math display">\[F^{*} = \left( t_{\widehat{\beta_{1}}}^{*} \right)^{2} = \left( \frac{{\widehat{\beta}}_{1}}{{\widehat{\sigma}}_{{\widehat{\beta}}_{1}}} \right)^{2} = \frac{\widehat{\beta_{1}}}{\frac{{\widehat{\sigma}}_{\varepsilon}^{2}}{\sum_{i}^{}\left( x_{i} - \overline{x} \right)^{2}}} = \frac{{\widehat{\beta}}_{1}^{2}\sum_{i}^{}\left( x_{i} - \overline{x} \right)^{2}}{\frac{\sum_{i}^{}e_{i}^{2}}{(n - 2)}}\]</span></p>
<p>La statistique <span class="math inline">\(F^{*}\)</span> permet de tester la <strong><em>significativité globale de la régression ou encore d’effectuer une évaluation globale de la régression</em></strong>, sous l’hypothèse nulle d’absence de liaison linéaire entre la variable endogène et les variables exogènes. Cette statistique indique si la variance expliquée est significativement supérieure à la variance résiduelle. Dans ce cas, on peut considérer que l’explication donnée par la régression traduit une relation qui existe réellement dans la population.</p>
</section>
</section>
<section id="la-prévision" class="level3">
<h3 class="anchored" data-anchor-id="la-prévision">La prévision</h3>
<p>Après l’estimation des paramètres, la question que l’on se pose est : <strong>quelle utilisation faire de cette régression passée ?</strong> L’une des utilisations est la « <strong>prédiction</strong>» de la valeur future de la variable expliquée étant donné un certain niveau de la variable explicative.</p>
<p>La prévision suppose donc que l’on connait la valeur de la variable explicative à un instant <strong>(</strong><span class="math inline">\(\mathbf{t}\mathbf{+ h}\)</span>) c’est-à-dire <span class="math inline">\(\mathbf{X}_{\mathbf{t + h}}\)</span> est connu et l’on cherche à prévoir <span class="math inline">\(\mathbf{Y}_{\mathbf{t + h}}\mathbf{.}\ t\)</span> est appelé l’origine de la prévision et <span class="math inline">\(h\)</span>, l’horizon de la prévision.</p>
<p>Nous savons que la valeur prédite de <span class="math inline">\(Y_{t}\)</span> est donné par&nbsp;:</p>
<p><span class="math display">\[{\widehat{Y}}_{t} = {\widehat{\beta}}_{0} + {\widehat{\beta}}_{1}X_{t}\]</span></p>
<p>La valeur prédite <span class="math inline">\(Y\)</span> à l’instant <span class="math inline">\(\mathbf{t}\mathbf{+ h}\)</span> sera donc donnée par&nbsp;:</p>
<p><span class="math display">\[{\widehat{Y}}_{t + h} = {\widehat{\beta}}_{0} + {\widehat{\beta}}_{1}X_{t + h}\]</span></p>
<p>Puisque <span class="math inline">\({\widehat{Y}}_{t + h}\ \)</span>est un estimateur, il est probablement différent de sa valeur réelle <span class="math inline">\(Y_{t + h}\)</span>. L’erreur de prédiction est définie par&nbsp;: <span class="math inline">\({\mathbf{Y}_{\mathbf{t}}\mathbf{-}\widehat{\mathbf{Y}}}_{t}\)</span>. On peut montrer que sous les hypothèses du modèle (incluant l’hypothèse de normalité), on a :</p>
<p><span class="math display">\[{\mathbf{Y}_{\mathbf{t}}\mathbf{-}\widehat{\mathbf{Y}}}_{t} ↝ N(0,\sigma_{\varepsilon_{t + h}}^{2}(1 + \frac{1}{n} + \frac{\left( X_{t + h} - \overline{X} \right)}{\sum_{t = 1}^{T}\left( X_{t} - \overline{X} \right)^{2}})\]</span></p>
<p>L’intervalle de prédiction est un intervalle dans lequel une future observation <span class="math inline">\(\mathbf{Y}_{\mathbf{t}}\)</span> va tomber avec une certaine probabilité (différent d’un intervalle de confiance).</p>
<p>On montre que <span class="math inline">\({\widehat{Y}}_{t + h\ }\)</span> suit une distribution normale de moyenne (<span class="math inline">\(\beta_{0} + \beta_{1}X_{t + h}\)</span>) et de variance&nbsp;:</p>
<p><span class="math display">\[V\left( {\widehat{Y}}_{t + h} \right) = \sigma_{\varepsilon_{t + h}}^{2}(1 + \frac{1}{n} + \frac{\left( X_{t + h} - \overline{X} \right)}{\sum_{t = 1}^{T}\left( X_{t} - \overline{X} \right)^{2}}\]</span></p>
<p>En remplaçant l’inconnue <span class="math inline">\(\sigma_{\varepsilon_{t + h}}^{2}\ \)</span>par son estimateur sans biais <span class="math inline">\({\widehat{\sigma}}_{\varepsilon_{t + h}}^{2}\)</span>, on peut construire un intervalle de confiance pour le vrai <span class="math inline">\(Y_{t + h}\)</span> et tester les hypothèses le concernant.</p>
<p>Pour ce faire, on définit :</p>
<p><span class="math display">\[\frac{Y_{t + h} - {\widehat{Y}}_{t + h}}{{\widehat{\sigma}}_{\varepsilon_{t + h}}^{2}} \rightarrow t(n - 2)\]</span></p>
<p>Soit <span class="math inline">\({IC}_{p}\ \)</span>l’intervalle de prédiction de<span class="math inline">\(\ {\widehat{\mathbf{Y}}}_{t + h}\)</span> au niveau de confiance (<span class="math inline">\(1 - \alpha)\)</span>. Il est donné par&nbsp;:</p>
<p><span class="math display">\[{IC}_{p} = \left\lbrack {\widehat{\mathbf{Y}}}_{t + h}\  \pm \mathbf{t}_{\left( \frac{\mathbf{\alpha}}{\mathbf{2}}\mathbf{;n - 2} \right)} \times \left\lbrack \sigma_{\varepsilon_{t + h}}^{2}(1 + \frac{1}{n} + \frac{\left( X_{t + h} - \overline{X} \right)}{\sum_{t = 1}^{T}\left( X_{t} - \overline{X} \right)^{2}} \right\rbrack^{\frac{1}{2}} \right\rbrack\]</span></p>
</section>
<section id="exercices" class="level3">
<h3 class="anchored" data-anchor-id="exercices">Exercices</h3>
<p><strong>Exercice 1</strong></p>
<p>A partir des données sur le niveau d’éducation (mesuré par le nombre d’années d’études) et les salaires horaires moyens perçus par les individus pour chaque niveau d’éducation, on obtient la régression suivante :</p>
<p><span class="math display">\[\begin{matrix}
{Salaire\ moyen}_{i} = 0,7437 + 0,641{Education}_{i}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\
Ecart - type = (0,8355)\ \ \ (\ \ )\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t = (\ )\ \ \ \ \ \ \ \ \ \ (9,6536)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ r^{2} = 0,8944\ \ \ \ \ \ \ \ \ \ \ \ n = 13 \\
\end{matrix}\]</span></p>
<p>1- Complétez les chiffres manquants.</p>
<p>2-Comment interpréter le coefficient 0,6416 ?</p>
<p>3-Rejetteriez-vous l’hypothèse selon laquelle l’éducation n’a pas d’influence sur les salaires ?</p>
<p>Quel test utiliseriez-vous ? Pourquoi ?</p>
<p>4 - Construisez la table ANOVA pour cet exemple et testez l’hypothèse d’un coefficient de pente nul. Quel test utiliseriez-vous et Pourquoi ?</p>
<p>5 - Supposez que dans la régression donnée ci-dessus le <span class="math inline">\(r^{2}\)</span> ne vous était pas fourni. Auriez-vous pu l’obtenir des autres informations contenues dans la régression ?</p>
<p><strong>Exercice 2</strong></p>
<p>Soit les résultats d’une estimation économétrique :</p>
<p><span class="math display">\[\begin{matrix}
{\widehat{Y}}_{i} = - 32,95 + 1,25X_{i}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\
n = 20\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ R^{2} = 0,23\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\widehat{ó}}_{{\widehat{u}}_{i}}^{2} = 10,66 \\
\end{matrix}\]</span></p>
<p>1 - A partir des informations connues, on demande de retrouver les statistiques suivantes : la somme des carrés des résidus (SCR), la somme des carrés totaux (SCT), la somme des carrés expliqués (SCE), la statistique F de Fisher et l’écart-type de la pente.</p>
<p>2- La pente est-elle significativement supérieur à 1?</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>