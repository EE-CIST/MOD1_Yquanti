[
  {
    "objectID": "MOD1_Yquanti_cours.html",
    "href": "MOD1_Yquanti_cours.html",
    "title": "Cours d’économétrie",
    "section": "",
    "text": "L’objet de la théorie économique est d’expliquer les comportements économiques au travers de modèles décrivant des relations entre des variables économiques : consommation, épargne, revenu, salaire, production, prix, emploi, investissement, taux d’intérêt, etc.\nLittéralement, l’économétrie signifie « mesure de l’économie ». Ainsi, l’économétrie est souvent décrite comme la partie de l’économie qui s’occupe de la mesure, du quantitatif. C’est une branche de l’économie qui traite de l’estimation pratique des relations économiques.\nL’économétrie exprime quantitativement les corrélations pouvant exister entre des phénomènes économiques dont la théorie affirme l’existence. La théorie économique fournit des idées sur les processus qui déterminent les grandeurs économiques. L’économétrie apporte une vérification empirique et établit quantitativement les corrélations qui apparaissent valides. Elle est, d’une part, un outil à la disposition de l’économiste qui lui permet d’infirmer ou de confirmer les théories qu’il construit et d’autre part, elle est à la croisée de l’économie, des mathématiques et des statistiques.\n\n\n\nComment les économétriciens procèdent-ils dans l’analyse d’un problème économique ? Quelle est la méthodologie (démarche) économétrique ? Bien qu’il existe plusieurs courants de pensée sur la méthodologie économétrique, nous présentons ici la méthodologie traditionnelle ou classique, qui domine toujours la recherche empirique en économie et dans d’autres sciences sociales et du comportement.\nDe manière générale, la méthodologie économétrique traditionnelle suit les étapes suivantes :\n\nEnoncé de la théorie ou d’une hypothèse\nSpécification du modèle mathématique de la théorie\nSpécification du modèle statistique ou économétrique\nObtention des données\nEstimation des paramètres du modèle économétrique\nTest d’hypothèses\nPrévision\nUtilisation du modèle à des fins de contrôle ou de stratégie\n\nExaminons les différentes étapes à suivre lors de la construction d’un modèle, ceci à partir de l’exemple du modèle keynésien simplifie.\n\n\nLes théories sont des raisonnements destinés à donner une représentation des liens entre les variables économiques. Elles sont souvent fondées sur des hypothèses.\nPar exemple, dans la théorie keynésienne, la loi psychologique fondamentale stipule que « en moyenne et la plupart du temps les hommes tendent à accroitre leur consommation à mesure que leur revenu croît, mais cet accroissement est moins que proportionnelle à celui du revenu ». En résumé, Keynes a postulé que la propension marginale à consommer pour une unité de revenu est supérieur à zéro mais inférieur à 1.\nPlus ou moins contraignantes, les hypothèses sont des simplifications de la réalité destinées à rendre possible la formulation de théories compréhensibles et utilisables. Ainsi, pour analyser la production, on peut partir de l’hypothèse que les entreprises recherchent la maximisation de leur profit, même si cela n’est pas absolument vrai pour toutes les entreprises à tout moment. L’important est que le modèle bâti explique correctement les décisions des entreprises en matière de production.\n\n\n\nUn modèle est un ensemble de lois et d’hypothèses donnant une représentation théorique des mécanismes économiques. Dans le cadre de l’économétrie, nous pouvons considérer qu’un modèle consiste en une présentation formalisée et simplifiée d’un phénomène sous forme d’équations mathématiques dont les variables sont des grandeurs économiques.\nL’objectif du modèle est de représenter les traits les plus marquants d’une réalité qu’il cherche à styliser. Le modèle est donc l’outil que le modélisateur utilise lorsqu’il cherche à comprendre et à expliquer des phénomènes. Pour ce faire, il émet des hypothèses et explicite des relations.\nPar exemple, bien que Keynes ait postulé une relation positive entre consommation et revenu, il n’a pas précisé la forme exacte de la relation fonctionnelle entre les deux. Ainsi, bien que des considérations d’ordre théorique nous renseignent sur le signe des dérivées, il existe une multitude de fonctions de formes très différentes et ayant des signes identiques, par exemple \\(C = a_{0} + a_{1}Y\\) et \\(C = a_{0} + a_{0}Y^{a_{1}}\\)\nCependant ces deux relations ne reflètent pas le même comportement. Une augmentation du revenu provoque un accroissement proportionnel pour la première relation, alors que, dans la seconde, l’effet s’estompe avec l’augmentation du revenu (\\(a_{1} < 1\\)).\nNous appelons forme fonctionnelle le choix (arbitraire ou fondé) de spécification précise du modèle.\nUn économiste pourrait suggérer la forme suivante de la fonction de consommation keynésienne :\n\\[C = a_{0} + a_{1}Y\\text{\\ }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{(1)}\\]\n\\[\\text{\\ }a_{0} > 0\\text{\\ et\\ 0} < a_{1} < 1\\]\n\\(C\\) la consommation, \\(Y\\) le revenu. Le coefficient de la pente \\(a_{1}\\) représente la propension marginale à consomme et \\(a_{0}\\) la consommation incompressible. \\(a_{0}\\) et \\(a_{1}\\) sont les paramètres du modèle qui représentent respectivement les coefficients d’interception et de la pente.\nL’équation (1) indique que la consommation est liée linéairement au revenu. C’est un exemple de modèle mathématique de la relation entre la consommation et le revenu. Cette relation est appelée fonction de consommation en économie.\nUn modèle est donc simplement un ensemble d’équations mathématiques.\n\nSi le modèle n’a qu’une équation, comme dans l’exemple ci-dessus, on parle de modèle à équation unique.\nSi par contre, il a plus d’une équation, il s’agit d’un modèle à équations multiples.\n\n\n\n\nLe modèle purement mathématique de la fonction de consommation donné dans l’équation (1) présente un intérêt limité pour l’économètre, car il assure qu’il existe une relation exacte ou déterministe entre consommation et revenu. Mais les relations entre les variables économiques sont généralement inexactes. Ainsi, si nous devions obtenir des données sur les dépenses de consommation et le revenu disponible (c’est-à-dire après impôt) d’un échantillon de 500 familles béninoises, par exemple, et tracer ces données sur un graphique, avec les dépenses de consommation sur l’axe vertical et le revenu disponible sur l’axe horizontal, nous ne nous attendions pas à ce que les 500 observations se situent exactement sur la droite de l’équation (1) ci-dessus.\n\\[\\lbrack Inserer\\ un\\ graphique\\rbrack\\]\nEn effet, outre le revenu, d’autres variables affectent les dépenses de consommation. Par exemple, la taille de la famille, l’âge des membres de la famille, la religion de la famille, etc. sont susceptibles d’exercer une influence sur la consommation.\nPour tenir compte des relations inexactes entre les variables économiques, l’économètre modifierait la fonction de consommation déterministe dans l’équation (1) comme suit :\n\\[C = a_{0} + a_{1}Y + \\varepsilon\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{(2)}\\]\nOù \\(\\varepsilon\\), appelé le terme de perturbation ou d’erreur, est une variable aléatoire (stochastique) qui possède des propriétés probabilistes bien définies. Le terme d’erreur \\(\\varepsilon\\) peut très bien représenter tous les facteurs qui influent sur la consommation mais ne sont pas explicitement pris en compte dans le modèle.\nL’équation (2) est un exemple de modèle économétrique. Plus techniquement, il s’agit d’un exemple de modèle de régression linéaire que nous étudierons dans ce cours.\n\n\n\nPour estimer le modèle économétrique de l’équation (2), c’est-à-dire obtenir les valeurs numériques de \\(a_{0}\\) et \\(a_{1}\\), nous avons besoin de données.\n\n\nLes données sont utilisées pour analyser le modèle économétrique et donner des recommandations de politique économique. Nous distinguons plusieurs types de données selon que le modèle est spécifié en :\n\nSérie temporelle : il s’agit des données sur des variables observées a intervalle de temps réguliers. Elles sont indicées par le temps : \\(y_{t}\\). Par exemple le Produit Intérieur Brut (PIB) du Bénin exprimé en FCFA sur une période de 20 ans.\nCoupe instantanée : les données sont observées au même instant et concernent les valeurs prises par la variable pour un groupe d’individus spécifiques : \\(y_{i}\\). Leur indice correspond à l’identifiant d’un individu ou d’une entreprise. Par exemple la consommation en riz d’un échantillon d’étudiant en 2018.\nPanel : il s’agit de données sur des variables représentant les valeurs prises par un échantillon d’individus à intervalle régulier. Ces variables sont notées : \\(y_{it}\\). On dispose d’informations sur les individus \\(i = 1,...,N\\) que l’on suit sur plusieurs période, \\(t = 1,...,T\\). Par exemple, la consommation d’un échantillon de ménages de la ville de Parakou de 2010 à 2020, le PIB des pays de l’UEMOA de 2000 à 2020, etc.\nCohorte : très proche des données de panel, les données de cohorte se distinguent de la précédente par la constante de l’échantillon. Les individus sondés sont les mêmes d’une période sur l’autre.\n\n\n\n\nL’analyse économétrique d’un ensemble de données a, dans la grande majorité des cas, pour objectif de tester la validité et d’évaluer l’ampleur des explications fournies par l’analyse économique. A ce titre, elle s’intéresse donc à l’effet d’un ensemble de variables, dites variables explicatives et notées \\(X\\) sur une ou plusieurs autres variables appelées variables expliquées, \\(Y\\). Le choix de ces variables et leur rôle dans le modèle économétrique est déduit de l’analyse économique du problème auquel on s’intéresse.\nUne même variable peut ainsi jouer le rôle de variable expliquée dans un modèle économétrique donnée et le rôle de variable explicative dans un modèle différent. Par exemple, l’éducation dans un modèle d’investissement en capital humain est une variable expliquée tandis que dans un modèle formation des salaires, elle est une variable explicative.\nLa théorie suggère ainsi une relation de causalité spécifique au problème considéré entre les variables auxquelles on s’intéresse. Pour cette raison, la variable expliquée est également souvent qualifiée de dépendante ou endogène, au sens où une relation causale la lie aux variables explicatives considérées. Les variables explicatives sont encore qualifiées de variables indépendantes ou exogènes, au sens où elles peuvent être considéré comme des données (connues) dans le cadre du problème auquel on s’intéresse.\n\n\n\n\nDans l’exemple du modèle keynésien, les estimations numériques des paramètres peuvent donner un contenu empirique à la fonction de consommation. Il existe plusieurs mécanismes d’estimation des paramètres des modèles économiques dont la technique statistique de l’analyse de régression linéaire qui fera l’objet de ce cours.\nPar exemple, en supposant que les données collectées ont été soumises à un calcul, nous obtenons les estimations suivantes de \\(a_{0}\\) et \\(a_{1}\\), à savoir de \\(144.06\\) et \\(0.8262\\). Ainsi, la fonction de consommation estimée est :\n\\[\\widehat{C} = 144.06 + 0.8262Y\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{(3)}\\]\nLe chapeau sur le \\(C\\) indique qu’il s’agit d’une estimation.\n\n\n\nEn supposant que le modèle ajusté soit une approximation raisonnable de la réalité, nous devons élaborer des critères appropriés pour déterminer si les estimations obtenues dans l’équation (3) sont conformes aux attentes de la théorie économique.\nSelon la théorie keynésienne, la propension marginale à consommer est positive et inférieure à 1. Dans l’équation (3), la propension marginale à consommer est égale à 0,83. Cependant, avant d’accepter cette estimation comme confirmation de la théorie de la consommation keynésienne, nous devons nous demander si cet estimateur est suffisamment inférieur à l’unité pour nous convaincre qu’il ne s’agit pas d’un événement fortuit ou d’une particularité des données utilisées.\nSi, 0,83 est statistiquement inférieur à 1 alors, cela peut conforter la théorie keynésienne. Ce type de confirmation ou de réfutation des théories économiques sur la base d’échantillons repose sur une branche de la théorie statistique appelée inférence statistique (test d’hypothèse).\n\n\n\nUne hypothèse statistique est une supposition sur un paramètre de la population. Cette supposition peut être ou ne pas être vraie. Pour prouver qu’une hypothèse est vraie ou fausse avec une certitude absolue, nous aurions besoin d’examiner l’ensemble de la population. Les tests d’hypothèses portent sur la façon d’utiliser un échantillon aléatoire pour déterminer si l’hypothèse faite sur la population est vérifiée.\nUn test statistique utilise les données obtenues d’un échantillon pour décider si l’hypothèse nulle doit être rejetée ou non. La valeur numérique obtenue à partir d’un test statistique s’appelle la valeur de test (valeur empirique).\n\nLe test d’hypothèse est formulé comme suit :\n\\[H_{0}:hypothèse\\ nulle\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_{1}:hypothèse\\ alternative\\]\n\nL’hypothèse nulle, symbolisée par \\(H_{0}\\), est une hypothèse statistique stipulant qu’il n’y a pas de différence entre un paramètre et une valeur spécifique ou qu’il n’y a pas de différence entre deux paramètres.\nL’hypothèse alternative, symbolisée par \\(H_{1}\\), est une hypothèse statistique qui énonce une différence spécifique entre un paramètre et une valeur spécifique ou indique qu’il existe une différence entre deux paramètres.\n\nQuatre types de résultats sont possibles dans les tests d’hypothèses. L’hypothèse nulle peut être vraie ou pas et une décision est prise de la rejeter ou de ne pas la rejeter sur la base des données obtenues à partir d’un échantillon.\n\n\n\n\n\n\n\n\n\n\\(H_{0}\\) est vraie\n\\(H_{1}\\) est vraie\n\n\n\n\nNe pas rejeter \\(H_{0}\\)\nDécision correcte\nErreur de Type II\n\n\nRejeter \\(H_{0}\\)\nErreur de Type I\nDécision correcte\n\n\n\nUne erreur de type I se produit si l’on rejette l’hypothèse nulle lorsqu’elle est vraie.\nUne erreur de type II se produit si l’on ne rejette pas l’hypothèse nulle quand elle est fausse.\n\nLe degré de significativité est la probabilité maximale de commettre une erreur de type I, c’est-à-dire la probabilité de rejeter l’hypothèse nulle alors qu’elle est vraie. Cette probabilité est symbolisée par \\(\\alpha\\). En d’autres termes :\n\n\\[Probabilité\\ (erreur\\ de\\ type\\ 1) = \\alpha\\]\nPlus le degré de significativité \\(\\alpha\\) est faible, moins nous sommes susceptibles de commettre une erreur de type I. Généralement, nous aimerions avoir de petites valeurs de \\(\\alpha\\). Les degrés de significativité typiques sont : 10%, 5% et 1%.\nPar exemple, lorsque \\(\\alpha = 10\\%\\), il y a 10% de chance de rejeter une hypothèse nulle alors qu’elle est vraie.\n\nLe degré de liberté correspond au nombre de valeurs aléatoires qui ne peuvent être déterminées ou fixés par une équation (par exemple, pour la variabilité totale, connaissant \\(n - 1\\) valeurs, nous pourrons en déduire la \\(n\\)-ième, puisque nous connaissons la moyenne \\(y\\)). Le degré de liberté est égal au nombre d’observations moins le nombre de relations entre ces observations : on pourrait remplacer l’expression « nombre de relations » par « nombre de paramètres à estimer ».\n\nPar exemple, si l’on cherche deux nombres dont la somme est 12, aucun des deux nombres ne doit être déterminé par l’équation \\(x + y = 12\\). \\(x\\) peut être choisi arbitrairement, mais alors pour \\(y\\) il n’y aura alors plus le choix. Ainsi, si vous choisissez 11 comme valeur pour \\(x\\), \\(y\\) vaut obligatoirement 1. Il y a donc deux variables aléatoires mais un seul degré de liberté.\n\n\n\n\n\n\nLes tests bilatéraux sont sous la forme :\n\\[H_{0}:\\mu = \\ \\mu_{0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_{1}:\\mu \\neq \\mu_{0}\\]\nExemple : Un chercheur en médecine voudrait savoir si un nouveau médicament aura des effets secondaires indésirables. Le chercheur est particulièrement préoccupé par le pouls des patients prenant le médicament. Quelles sont les hypothèses pour vérifier si le pouls sera différent du pouls moyen de 82 battements par minute ?\n\\[H_{0}:\\mu = 82\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_{1}:\\mu \\neq 82\\]\n\n\n\nIls sont sous la forme suivante :\n\\[H_{0}:\\mu = \\ \\mu_{0}\\ \\ \\ \\ \\ H_{1}:\\mu < \\mu_{0}\\]\nou\n\\[H_{0}:\\mu = \\ \\mu_{0}\\ \\ \\ \\ \\ H_{1}:\\mu > \\mu_{0}\\]\nExemple : Un chimiste invente un additif pour augmenter la durée de vie d’une batterie d’automobile. Si la durée de vie moyenne de la batterie est de 36 mois, alors ses hypothèses sont :\n\\[H_{0}:\\mu = \\ 36\\ \\ \\ \\ \\ H_{1}:\\mu > 36\\]\n\n\n\nL’inférence statistique consiste à effectuer des études sur un échantillon et de transposer (ou généraliser) les résultats sur la population. Elle permet de faire des prédictions sur une population à partir des observations et de l’analyse d’un échantillon. Elle permet de déterminer des intervalles de confiance pour des paramètres du modèle ou de tester si un paramètre est significativement inferieur, supérieur ou simplement différent d’une valeur fixée. Il existe deux méthodes d’inférence statistique : l’estimation des paramètres et le test statistique des hypothèses.\n\n\n\n\nLa prévision, à partir de l’utilisation des modèles économiques, est utilisée par les pouvoirs publics ou entreprises afin d’anticiper et éventuellement réagir à l’environnement économique. Ainsi, si le modèle que nous choisissons ne réfute pas l’hypothèse ou la théorie considérée, l’on pourra l’utiliser pour prédire la ou les valeurs futures de la variable dépendante, sur la base de la ou des valeurs futures connues ou attendues de la variable explicative.\nEn se basant par exemple sur l’équation (3), supposons que nous voulons prévoir la dépense de consommation principale pour 2022. La valeur du PIB pour 2019 est par exemple de 1500 milliards de francs CFA. En remplaçant cette valeur dans l’équation (3), on pourra prédire la valeur de la consommation pour 2020 comme suit :\n\\[\\widehat{C_{2019}} = 144.06 + 0.8262(1500\\ )\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{(4)}\\]\nEn résumé, la démarche économétrique peut être résumée comme suit :"
  },
  {
    "objectID": "MOD1_Yquanti_cours.html#chapitre-2-introduction-à-lanalyse-de-régression-linéaire",
    "href": "MOD1_Yquanti_cours.html#chapitre-2-introduction-à-lanalyse-de-régression-linéaire",
    "title": "Cours d’économétrie",
    "section": "Chapitre 2 : Introduction à l’analyse de régression linéaire",
    "text": "Chapitre 2 : Introduction à l’analyse de régression linéaire\nL’analyse de la régression s’intéresse à l’étude de la dépendance d’une variable (variable dépendante) par rapport à une ou plusieurs variables (variables indépendantes ou explicatives) assortie d’un objectif d’estimer et/ou de prédire la valeur moyenne de la variable dépendante en fonction des valeurs connues ou fixes des variables indépendantes.\n\nRelations statistiques versus relations déterministes\nDans l’analyse de la régression, nous nous intéressons à ce qu’on nomme la dépendance entre variables. Il s’agit de la dépendance statistique et non fonctionnelle ou déterministe que l’on rencontre par exemple en physique classique. Dans la relation statistique entre variables, nous traitons essentiellement des variables aléatoires ou stochastiques (c’est-à-dire variables avec des distributions de probabilité). Par contre, dans la relation déterministe, on s’intéresse aussi à des variables, mais elles ne sont ni aléatoires ni stochastiques.\nPar exemple, la dépendance du rendement d’une récolte par rapport à la température, la pluviométrie, l’ensoleillement et les engrais est, par nature, statistique car les variables explicatives, ne permettent pas à l’agriculteur de prédire la récolte avec précision en raison d’erreurs incluses dans la mesure de ces variables ainsi que de plusieurs autres facteurs (variables) qui influent ensemble sur la récolte mais qui sont difficiles à individualiser.\nD’un autre côté, dans les relations déterministes comme celle de la loi de Ohm qui s’énonce comme suit : pour les conducteurs métalliques, sur une plage limitée de température, le courant continu \\(C\\) est proportionnel au voltage \\(V\\).\n\\[C = \\frac{1}{k}V\\ avec\\ \\frac{1}{k}\\ la\\ constance\\ de\\ proportionnalité\\]\nCependant en cas d’erreur de mesure, par exemple dans \\(k\\) la loi de Ohm, la relation déterministe devient une relation statistique.\n\n\nRégression versus corrélation\nBien que très liées, l’analyse de la corrélation est conceptuellement très différente de l’analyse de la régression. L’analyse de la corrélation mesure l’intensité de la liaison entre deux variables.\nExemple : corrélation entre les notes en statistiques et celles en mathématiques.\nL’analyse de la régression quant à elle, a pour objectif d’estimer ou de prévoir la valeur moyenne d’une variable sur la base de valeurs fixées d’autres variables.\nExemple : Prédire la note moyenne d’une épreuve en statistique en disposant de la note d’un étudiant en mathématiques.\nDans l’analyse de la régression, il existe une asymétrie dans le traitement des variables dépendantes et explicatives. Dans l’analyse de la corrélation, les variables sont traitées de manière symétrique. Il n’existe pas de distinction entre variables explicatives et dépendantes.\n\n\nPrésentation du modèle de régression linéaire\nLe modèle de régression linéaire désigne un modèle dans lequel l’espérance conditionnelle de \\(y\\) sachant \\(x\\) est une transformation affine de \\(x\\). Le modèle de régression linéaire s’écrit :\n\\[y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\]\nOn parle de la régression de \\(y\\) sur \\(x\\), avec \\(\\beta_{j}\\) les paramètres (les coefficients) inconnus du modèle. \\(\\varepsilon\\) est le terme d’erreur.\n\nSignification du terme « linéaire »\nPuisque nous nous intéressons avant tout aux modèles linéaires, il est fondamental de connaitre ce que signifie réellement l’adjectif « linéaire » car il peut être interprété de deux manières.\n\nLinéarité dans les variables\nLe premier sens, peut-être le plus « naturel » de la linéarité réside dans le fait que l’espérance de \\(y\\) est une fonction linéaire de \\(x_{i}\\). Géométriquement, la courbe de régression est, dans ce cas, une droite. Dès lors, une fonction de régression telle que \\(y_{i} = \\beta_{0} + \\beta_{1}x_{1}^{2} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\) n’est pas une relation linéaire car la variable \\(x_{1}\\) est affectée d’une puissance ou d’un indice de 2.\n\n\nLinéarité dans les paramètres\nLa seconde interprétation de la linéarité tient au fait que l’espérance de de \\(y\\) est une fonction linéaire des paramètres \\(\\beta_{i}\\). Cette espérance peut être ou non linéaire dans les variables, c’est-à-dire par rapport à \\(x\\). Ainsi, \\(y_{i} = \\beta_{0} + \\beta_{1}x_{1}^{2} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\) est un modèle de régression linéaire dans les paramètres. Par contre, \\(y_{i} = \\beta_{0} + \\beta_{1}^{2}x_{1} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\) n’est pas un modèle de régression linéaire dans les paramètres.\n\n\n\nSignification du terme aléatoire\nLe terme aléatoire que l’on appelle erreur du modèle, tient un rôle très important dans la régression. Il permet de résumer toute l’information qui n’est pas prise en compte dans la relation linéaire que l’on cherche à établir entre \\(Y\\) et \\(X\\) c’est-à-dire les problèmes de spécifications, l’approximation par la linéarité, résumer le rôle des variables explicatives absentes, etc. Le terme d’erreur est un substitut de toutes les variables omises dans le modèle mais qui affectent toutes ensemble \\(Y\\).\nComme nous le verrons dans la suite du cours, les propriétés des estimateurs du modèle de régression linéaire, reposent en grande partie sur les hypothèses que nous formulerons à propos du terme d’erreur \\(\\varepsilon\\). En pratique, après avoir estimé les paramètres de la régression, les premières vérifications portent sur l’erreur calculée sur les données (on parle de « résidus ») lors de la modélisation.\nLe terme d’erreur \\(\\varepsilon\\) peur regrouper les erreurs suivantes :\n\nomission de variables ou imprécision de la théorie : même si la théorie existe, elle peut être insuffisante pour expliquer le comportement de \\(Y\\). Nous pouvons tenir pour certain que le revenu influe sur la consommation, mais pouvons ignorer les autres variables affectant la consommation ou douter de leur pertinence. Par conséquent, le terme d’erreur peut être utilisé comme substitut à toutes les variables omises dans le modèle.\nerreur de mesure : les données ne représentent pas exactement le phénomène\nla nature intrinsèquement aléatoire du comportement humain : même si nous réussissons à introduire toutes les variables pertinentes du modèle, il y a des limites au caractère aléatoire des individuels qui ne peuvent être expliquées quelle que soit la somme de travail que nous fournissons. Les perturbations ou terme d’erreur peuvent fort bien refléter cette nature intrinsèquement aléatoire.\n\n\n\nerreur de spécification : même si théoriquement nous disposons des bonnes variables explicatives d’un phénomène et même si nous pouvons nous procurer des données sur ces variables, très souvent nous ne connaissons pas la forme de la relation fonctionnelle entre la variable dépendante et le régresseur.\n\n\nLa consommation est-elle une fonction linéaire ou non linéaire du revenu ? Si c’est le premier cas qui prévaut, alors \\(y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i}\\) est la relation correcte entre \\(X\\) et \\(Y\\) mais si c’est le second cas, \\(y_{i} = \\beta_{0} + \\beta_{1}{x^{2}}_{i} + \\varepsilon_{i}\\) peut être la forme fonctionnelle adaptée. Dans les modèles à deux variables, la forme fonctionnelle de la relation peut souvent être discernée au diagramme de dispersion. Mais dans un modèle de régression multiple, il est plus difficile de déterminer la forme fonctionnelle adéquate, car graphiquement nous ne pouvons pas visualiser les diagrammes de dispersion à plusieurs dimensions.\n\n\nerreur de fluctuation d’échantillonnage : d’un échantillon à l’autre les observations, et donc les estimations, sont légèrement différentes.\n\n\n\n\nFonction de régression de l’échantillon (FRE) et fonction de régression de la population (FRP)\nDans la plupart des situations concrètes nous ne disposons que d’un échantillon de \\(y\\) associé à quelques valeurs données de \\(x\\). Ainsi, notre tâche est d’estimer la fonction de régression à partir des informations fournies par l’échantillon. Nous obtenons alors la fonction de régression de l’échantillon (FRE) contrairement à la fonction de régression de la population (FRP) qui est \\(y\\) et \\(x\\) sur toute la population.\nEn résumé, notre objectif est donc d’estimer, la fonction de régression de la population (RP) :\n\\[y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\]\nà partir de la fonction de régression de l’échantillon (FRE) obtenue de l’observation de\n\\[y = {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}x_{1} + {\\widehat{\\beta}}_{2}x_{2} + \\ldots + {\\widehat{\\beta}}_{k}x_{k} + e\\]\noù \\(e\\) est un estimateur de \\(\\varepsilon\\).\nEn raison des fluctuations d’échantillonnage, notre estimation de la FRP basée sur la FRE est une approximation."
  },
  {
    "objectID": "MOD1_Yquanti_cours.html#chapitre-3-estimation-des-modèles-de-régression-linéaires",
    "href": "MOD1_Yquanti_cours.html#chapitre-3-estimation-des-modèles-de-régression-linéaires",
    "title": "Cours d’économétrie",
    "section": "Chapitre 3 : Estimation des modèles de régression linéaires",
    "text": "Chapitre 3 : Estimation des modèles de régression linéaires\nNotre tâche consiste à estimer aussi exactement que possible la fonction de régression de la population (FRP) sur la base d’une fonction de régression de l’échantillon (FRE). Les méthodes d’estimation généralement utilisées sont les moindres carres ordinaires (MCO) et le maximum de vraisemblance (MV).\n\nL’estimation par la méthode des moindres carrés ordinaires (MCO)\nLa méthode des moindres carrés ordinaires est celle qui est plus largement utilisée dans l’analyse de la régression linéaire parce qu’elle est mathématiquement beaucoup plus simple que le maximum de vraisemblance. Cependant, pour la régression linéaire, les deux méthodes fournissent généralement des résultats analogues.\nLa méthode des MCO est attribuée à Carl Friedrich Gauss, un mathématicien allemand. La méthode consiste en une prescription (initialement empirique), qui est que la fonction \\(f(x;\\beta)\\) qui décrit « le mieux » les données est celle qui minimise la somme quadratique des déviations des mesures aux prédictions de \\(f(x;\\beta)\\).\n\nLes Hypothèses du modèle de régression linéaire\nL’estimation du modèle de régression linéaire repose sur un certain nombre d’hypothèses que sont :\n\nHypothèse 1 : Le modèle est linéaire dans les paramètres\n\nCela ne veut pas dire que \\(X\\) et \\(Y\\) sont linéaires (elles peuvent être non linéaires), mais plutôt que \\(\\beta_{0}\\) et \\(\\beta_{1}\\) sont linéaires.\n\nHypothèse 2 : L’espérance mathématique de l’erreur \\(\\mathbf{\\varepsilon}_{\\mathbf{i}}\\) est nulle. \n\nLa valeur de \\(x_{i}\\) étant donnée, la moyenne ou l’espérance mathématique du terme d’erreur aléatoire \\(\\varepsilon_{i}\\) est nulle.\n\\[E\\left( \\varepsilon_{i} \\right) = 0\\ ou\\ E\\left( \\varepsilon_{i}׀x_{i} \\right) = 0\\]\nAinsi, les erreurs \\(\\varepsilon_{i}\\) pour une valeur donnée de \\(x_{i}\\) (dans la population) est symétrique autour de la sa moyenne. (Figure)\n\nHypothèse 3 : L’homoscédasticité ou la constance de la variance de \\(\\mathbf{\\varepsilon}_{\\mathbf{i}}\\).\n\nLa valeur de \\(X\\) étant donnée, la variance de \\(\\varepsilon_{i}\\) est identique pour toutes les observations.\n\\(E\\left( \\varepsilon_{i}^{2} \\right) = \\sigma_{\\varepsilon}^{2}\\ \\ \\ ou\\ E\\left( \\varepsilon_{i}^{2}׀x_{i} \\right) = \\sigma_{\\varepsilon}^{2}\\)\nLa variance de l’erreur est constante : le risque de l’amplitude de l’erreur est le même quelle que soit l’observation. Cette hypothèse s’appelle : hypothèse d’homoscédasticité. Dans le cas où cette hypothèse n’est pas vérifiée, on parle alors de modèle hétéroscédastique.\n\nHypothèse 4 : Absence d’autocorrélation des erreurs\n\nEtant donné deux valeurs, \\(x_{i}\\) et \\(x_{j}\\) (\\(i \\neq j)\\), la corrélation entre les erreurs \\(\\varepsilon_{i}\\) et \\(\\varepsilon_{j}\\) (\\(i \\neq j)\\) est nulle.\n\\[E\\left( \\varepsilon_{i}\\varepsilon_{j} \\right) = 0\\ ou\\ E\\left( \\varepsilon_{i}\\varepsilon_{j}׀x_{i},׀x_{j} \\right) = 0\\ \\ \\ \\ avec\\ (i \\neq j)\\]\n\\[cov\\left( \\varepsilon_{i}\\varepsilon_{j} \\right) = 0\\ ou\\ cov\\left( \\varepsilon_{i}\\varepsilon_{j}׀x_{i},׀x_{j} \\right) = 0\\ \\ \\ \\ avec\\ (i \\neq j)\\]\nCette hypothèse signifie que la valeur de \\(x_{i}\\) étant donnée, les déviations de deux valeurs quelconques de \\(Y\\) par rapport à leur moyenne ne sont pas corrélées. Autrement dit, les erreurs relatives à 2 observations sont indépendantes. On parle de « non autocorrélation des erreurs ».\n\nHypothèse 5 : Covariance nulle entre \\(\\mathbf{x}_{\\mathbf{i}}\\mathbf{\\ }\\)et \\(\\mathbf{\\varepsilon}_{\\mathbf{i}}\\)\n\n\\[E\\left( x_{i}\\varepsilon_{i} \\right) = 0\\ \\ ou\\ \\ Cov\\left( x_{i}\\varepsilon_{i} \\right) = 0\\ \\ \\]\nCette hypothèse signifie que l’erreur \\(\\varepsilon_{i}\\ \\)est indépendante de la variable explicative \\(x_{i}\\).\n\nHypothèse 6 : \\(\\mathbf{n}\\mathbf{> k}\\)\n\nLe nombre d’observations \\(n\\) doit être plus élevé que le nombre de paramètres \\(k\\) à estimer (nombre de variables explicatives).\n\nHypothèse 7 : Exactitude de la variable indépendante\n\nLes valeurs \\(x_{i}\\) sont fixées d’un échantillon à un autre (observées sans erreur). On considère que les données collectées sont contrôlées par le statisticien et sont mesurées avec une marge d’erreur négligeable. En termes techniques \\(x_{i}\\) est supposé non stochastique (non aléatoire). Cependant, la variable dépendante est supposée être statistique, aléatoire ou stochastique, c’est-à-dire ayant une distribution de probabilité. Il existe une distribution de probabilité pour \\(y\\) pour chaque valeur possible de \\(x\\). Ainsi, l’espérance mathématique et la variance de la distribution sont donnée par :\n\\[E(y׀x_{i}) = \\beta_{0} + \\beta_{1}x\\]\n\\[E\\left( y׀x_{i} \\right) = Var\\left( \\beta_{0} + \\beta_{1}x + \\varepsilon \\right) = \\sigma^{2}\\]\nAinsi, l’esperance mathématique de \\(y\\) est une fonction linéaire de \\(x\\) bien que sa variance ne dépende pas de \\(x\\).\n\nHypothèse 8 : La normalité du terme d’erreur\n\nLes hypothèses H2 et H3 sur le terme aléatoire \\(\\varepsilon_{i}\\), peuvent se résumer comme : les \\(\\varepsilon_{i}\\) sont i.i.d (indépendants et identiquement distribués).\n\\[\\mathbf{\\varepsilon}_{\\mathbf{i}}\\mathcal{↝ N}\\mathbf{(0,}\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}\\mathbf{\\ }\\mathbf{)}\\mathbf{\\ }\\]\n\n\nEstimation du modèle de régression linéaire simple\nNous allons commencer par la méthode d’estimation le plus élémentaire qui est le modèle de régression linéaire simple. Ce modèle cherche à mettre en évidence la relation de dépendance entre une variable aléatoire réelle à expliquer (variable endogène, dépendante ou réponse) \\(y\\) et une variable explicative (variable exogène, indépendante, régresseur, contrôle) \\(x\\).\nSoit, le modèle de régression linéaire simple :\n\\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ avec\\ i = 1,\\ldots,n\\]\nNous cherchons à estimer les paramètres \\(\\beta_{0}\\) et \\(\\beta_{1}\\) du modèle de régression linéaire simple. Pour cela nous avons besoin d’un échantillon issu de la population. Soit un échantillon de taille \\(n\\) issue de la population, tel que nous ayons des \\(n\\) pairs de données \\((y_{1},x_{1})\\), \\((y_{2},x_{2})\\), \\(\\ldots,\\ (y_{n},x_{n})\\) pour chaque observation \\(i\\).\n\n\n\nMathematics Statistiques\n\n\nLes données issues de l’échantillon sont représentées sur le graphique ci-dessus par les nuages de points. Nous voulons partir de ces observations pour estimer la constante \\(\\beta_{0}\\) et la pente \\(\\beta_{1}\\) de la FRP. Cette FRP est donnée par la droite qui est la plus proche possible de tous les nuages de points (observations). Pour chaque point du nuage de points (FRE), il existe un écart par rapport à la FRP. Ces écarts constituent les erreurs et contiennent tous les facteurs non observées qui affectent \\(y_{i}\\). Ils sont mesurés par la projection parallèlement à l’axe des ordonnées des points sur la droite de régression (FRP).\nLa méthode des MCO cherche à calculer ou estimer les paramètres inconnus \\(\\beta_{0}\\) et \\(\\beta_{1}\\) à partir des pairs de données en minimisant la somme des carrés des écarts entre la valeur observée de la variable endogène et sa variable calculée (la droite de régression). Le carré permet de donner une pénalité plus grande aux observations plus éloignées de la valeur estimée\nLa résolution analytique est la suivante :\n\\[Min\\sum_{i = 1}^{n}e_{i}^{2} = Min{\\sum_{i}^{}\\left( y_{i} - \\widehat{y_{i}} \\right)}^{2} = Min\\left( y_{i} - \\left( \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} \\right) \\right)^{2} = Min\\ S\\left( \\beta_{0},\\beta_{1} \\right)\\]\nOu le résidu des MCO pour l’observation \\(i\\) est donnée par :\n\\[e_{i} = y_{i} - \\left( \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} \\right)\\]\nEn annulant les dérivées partielles (condition du premier ordre), nous obtenons un système d’équations appelées « équations normales » :\n\\[\\left\\{ \\begin{matrix}\n\\&\\frac{\\partial S\\left( \\beta_{0},\\beta_{1} \\right)}{\\partial\\beta_{0}} = - 2\\sum_{i = 1}^{n}{\\left( y_{i} - \\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}x_{i} \\right) = 0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1) \\\\\n\\&\\frac{\\partial S\\left( \\beta_{0},\\beta_{1} \\right)}{\\partial\\beta_{1}} = - 2\\sum_{i = 1}^{n}{x_{i}\\left( y_{i} - \\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}x_{i} \\right) = 0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2) \\\\\n\\end{matrix} \\right.\\ \\]\nLa première équation donne :\n\\[\\sum_{i = 1}^{n}{y_{i} -}n\\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}x_{i} = 0\\]\nor\n\\[\\overline{y} = \\frac{1}{N}\\sum_{i = 1}^{n}{y_{i}\\text{\\ et\\ }}\\overline{x} = \\frac{1}{N}\\sum_{i = 1}^{n}x_{i}\\]\nOn obtient donc :\n\\[\\overline{y} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}\\overline{x}\\]\n\\[\\widehat{\\mathbf{\\beta}_{\\mathbf{0}}}\\mathbf{=}\\overline{\\mathbf{y}}\\mathbf{-}\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\overline{\\mathbf{x}}\\]\nLa deuxième équation devient :\n\\[\\sum_{i = 1}^{n}{x_{i}\\left( y_{i} - \\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}x_{i} \\right) =}\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\widehat{\\beta_{0}}\\sum_{i = 1}^{n}x_{i} - \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}{x_{i}}^{2} = 0\\]\nEn remplaçant \\(\\widehat{\\beta_{0}}\\) par son expression, on obtient :\n\\[\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\left( \\overline{y} - \\widehat{\\beta_{1}}\\overline{x} \\right)\\sum_{i = 1}^{n}x_{i} - \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}{x_{i}}^{2} = 0\\]\n\\[\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\frac{1}{N}\\left( \\overline{y} - \\widehat{\\beta_{1}}\\overline{x} \\right)\\sum_{i = 1}^{n}x_{i} - \\widehat{\\beta_{1}}\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}}^{2} = 0\\]\n\\[\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\left( \\overline{y} - \\widehat{\\beta_{1}}\\overline{x} \\right)\\overline{x} - \\widehat{\\beta_{1}}\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}}^{2} = 0\\]\n\\[\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\overline{y}\\overline{x} + \\widehat{\\beta_{1}}{\\overline{x}}^{2} - \\widehat{\\beta_{1}}\\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}}^{2} = 0\\]\n\\[\\ \\widehat{\\beta_{1}}\\left( \\frac{1}{N}\\sum_{i = 1}^{n}{x^{2}}_{i} - {\\overline{x}}^{2} \\right) = \\frac{1}{N}\\sum_{i = 1}^{n}{x_{i}y_{i}} - \\overline{y}\\overline{x}\\]\n\\[\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\mathbf{=}\\frac{\\frac{1}{N}\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{x}_{\\mathbf{i}}\\mathbf{y}_{\\mathbf{i}}}\\mathbf{-}\\overline{\\mathbf{y}}\\overline{\\mathbf{x}}}{\\frac{1}{N}\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{x_{i}}^{2}\\mathbf{-}{\\overline{\\mathbf{x}}}^{\\mathbf{2}}}\\mathbf{=}\\frac{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)\\mathbf{(}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{y}}\\mathbf{)}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}\\]\n\\[\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\mathbf{=}\\frac{\\widehat{\\text{Cov}\\left( \\mathbf{x}\\mathbf{,}\\mathbf{y} \\right)}}{\\widehat{\\mathbf{Var}\\left( \\mathbf{x} \\right)}}\\]\nNote : \\(\\widehat{\\beta_{0}}\\) et \\(\\widehat{\\beta_{1}}\\) permettent bien de minimiser la somme du carrée des résidus \\(S\\left( \\beta_{0},\\beta_{1} \\right)\\), car :\n\\[\\left\\{ \\begin{matrix}\n\\&\\frac{\\partial^{2}S\\left( \\beta_{0},\\beta_{1} \\right)}{\\partial\\left( \\beta_{0} \\right)^{2}} = 2 > 0 \\\\\n\\&\\frac{\\partial^{2}S\\left( \\beta_{0},\\beta_{1} \\right)}{\\partial\\left( \\beta_{1} \\right)^{2}} = 2\\sum_{i = 1}^{n}{x_{i}^{2} > 0} \\\\\n\\end{matrix} \\right.\\ \\]\n\n\nLe modèle de régression linéaire multiple avec \\(\\mathbf{k}\\) variables indépendantes\nDans le modèle de régression linéaire simple, nous avons vu que la variable dépendante \\(y\\) ne pouvait être expliquée qu’en fonction d’une seule variable indépendante, \\(x\\). Le principal problème lié à l’utilisation de la régression linéaire simple est qu’il est très difficile de tirer des conclusions concernant l’effet de \\(x\\) sur \\(y\\), toutes choses étant égales par ailleurs. En règle générale, l’hypothèse 2 \\(\\mathbf{(}\\mathbf{E}\\mathbf{(}\\mathbf{\\varepsilon}_{\\mathbf{i}}\\mathbf{׀}\\mathbf{x}_{\\mathbf{i}}\\mathbf{) = 0}\\)) ne tient pas, car la variable \\(x\\) est souvent corrélée avec un autre facteur qui influence \\(y\\).\nLe modèle de régression linéaire multiple (RLM), appelé aussi modèle de régression multiple, permet de prendre en compte de façon explicite de nombreux facteurs qui affectent simultanément la variable dépendante. Si l’on ajoute des facteurs utiles pour expliquer la variable dépendante, nous parviendront naturellement à expliquer une plus grande partie de la variation de \\(y\\). L’utilisation de la régression multiple peut donc conduire à une meilleure prédiction de la variable dépendante.\nLe modèle de régression linéaire multiple dans la population peut prendre la forme générale suivante :\n\\[y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\ldots + \\beta_{k}x_{k} + \\varepsilon\\]\nOù \\(\\beta_{0}\\) est l’ordonnée à l’origine ou la constante, \\(\\beta_{j}\\) mesure la variation de \\(\\mathbf{y}\\) suite à une variation d’une unité de \\(x\\), les autres facteurs étant fixés. Comme il y a \\(k\\) variables indépendantes et une ordonnée à l’origine, l’équation de RLM contient \\(p = k + 1\\) paramètres de population (inconnus).\n\\(\\varepsilon\\) est le terme d’erreur ou la perturbation. Il contient les autres facteurs, différents des \\(x\\) qui affectent la variable dépendante \\(y\\). Quel que soit le nombre de variables explicatives dans notre modèle, il y aura toujours des facteurs que nous ne pourrons pas inclure ; ils seront tout alors compris dans \\(\\varepsilon\\).\n\nEstimation des paramètres du modèle de régression linéaire multiple par les MCO\nLa méthode des MCO peut être utilisée pour estimer les coefficients du l’équation du modèle de RLM. Supposons que nous avons, \\(n > k\\) observations. Soit \\(y_{i}\\) la \\(i\\)ème observation de la variable dépendante (de réponse), \\(x_{ij}\\) est la \\(i\\)ème observation pour la variable dépendante \\(j\\). Les données se présenteront comme dans le tableau qui suit :\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariables dépendantes\n\n\n\n\n\n\n\nObersation, \\(i\\)\nResponse, \\(y\\)\n\\[x_{1}\\]\n\\[x_{2}\\]\n\\[\\ldots\\]\n\\[x_{k}\\]\n\n\n1\n\\[y_{1}\\]\n\\[x_{11}\\]\n\\[x_{12}\\]\n\\[\\ldots\\]\n\\[x_{1k}\\]\n\n\n2\n\\[y_{2}\\]\n\\[x_{21}\\]\n\\[x_{22}\\]\n\\[\\ldots\\]\n\\[x_{2k}\\]\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\\[\\vdots\\]\n\n\n\\[n\\]\n\\[y_{n}\\]\n\\[x_{n1}\\]\n\\[x_{n2}\\]\n\\[\\ldots\\]\n\\[x_{nk}\\]\n\n\n\nLe modèle de régression linéaire correspondant, se présente alors comme suit :\n\\[y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\ldots + \\beta_{k}x_{ik} + \\varepsilon_{i}\\]\n\\[y_{i} = \\beta_{0} + \\sum_{j = 1}^{k}{\\beta_{j}x_{ij}} + \\varepsilon_{i},\\ \\ \\ \\ i = 1,2,\\ldots,\\ n\\ \\ \\ et\\ j = 1,2,\\ldots,\\ k\\]\nSous forme matricielle le modèle s’écrit :\n\\[Y = X\\beta + \\varepsilon\\]\n\\(Y = \\begin{bmatrix} y_{1} \\\\ \\begin{matrix} y_{2} \\\\  \\vdots \\\\ \\end{matrix} \\\\ y_{n} \\\\ \\end{bmatrix}\\), \\(X = \\left\\lbrack \\begin{matrix} \\begin{matrix} 1 \\\\ \\begin{matrix} 1 \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} & \\begin{matrix} x_{11} \\\\ \\begin{matrix} x_{21} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} \\\\ 1 & x_{n1} \\\\ \\end{matrix}\\ \\ \\ \\begin{matrix} \\begin{matrix} \\ldots \\\\ \\begin{matrix} \\ldots \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} & \\begin{matrix} x_{1k} \\\\ \\begin{matrix} x_{2k} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} \\\\ \\ldots & x_{nk} \\\\ \\end{matrix} \\right\\rbrack\\) \\(\\beta = \\begin{bmatrix} \\beta_{0} \\\\ \\begin{matrix} \\beta_{2} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\beta_{k} \\\\ \\end{bmatrix}\\) et \\(\\varepsilon = \\begin{bmatrix} \\varepsilon_{1} \\\\ \\begin{matrix} \\varepsilon_{2} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\varepsilon_{n} \\\\ \\end{bmatrix}\\)\n\n\\(Y\\) désigne le vecteur à expliquer de taille \\(n \\times 1\\)\n\\(X\\) la matrice explicative de taille \\(n \\times p\\)\n\\(\\beta\\) le vecteur des coefficients du modèle. Il est de taille \\(p \\times 1\\)\n\\(\\varepsilon\\) le vecteur d’erreurs de taille \\(n \\times 1\\)\n\nLa fonction des moindres carrés associé au modèle de RLM est donnée par :\n\\[S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right) = \\sum_{i = 1}^{n}e_{i}^{2} = \\ \\sum_{i = 1}^{n}\\left( y_{i} - \\beta_{0} - \\sum_{j = 1}^{k}{\\beta_{j}x_{ij}} \\right)^{2}\\]\nPour trouver les estimateurs des MCO, la fonction \\(S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right)\\) doit être minimisée en fonction de \\(\\beta_{0},\\beta_{1},\\ \\beta_{2},\\ldots,\\beta_{k}\\). Les estimateurs des MCO \\({\\widehat{\\beta}}_{0},{\\widehat{\\beta}}_{1},\\ {\\widehat{\\beta}}_{2},\\ldots,{\\widehat{\\beta}}_{k}\\) satisfont les conditions suivantes :\n\\[\\left\\{ \\begin{matrix}\n\\&\\frac{\\partial S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right)}{\\partial\\beta_{0}} = - 2\\sum_{i = 1}^{n}{\\left( y_{i} - \\widehat{\\beta_{0}} - \\sum_{j = 1}^{k}{\\widehat{\\beta_{j}}x_{ij}} \\right) = 0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1) \\\\\n\\&\\frac{\\partial S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right)}{\\partial\\beta_{j}} = - 2\\sum_{i = 1}^{n}{\\left( y_{i} - \\widehat{\\beta_{0}} - \\sum_{j = 1}^{k}{\\widehat{\\beta_{j}}x_{ij}} \\right)x_{ij}} = 0\\ \\ \\ \\ \\ \\ \\ j = 1,2,\\ldots,\\ k\\ \\ \\ \\ (2) \\\\\n\\end{matrix} \\right.\\ \\]\nEn simplifiant, nous obtenons les équations normales des MCO qui suivent :\n\\[\\sum_{i = 1}^{n}y_{i} = n\\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}x_{i1} + \\widehat{\\beta_{2}}\\sum_{i = 1}^{n}x_{i2} + \\ldots + \\widehat{\\beta_{k}}\\sum_{i = 1}^{n}x_{ik}\\]\n\\[\\sum_{i = 1}^{n}{x_{i1}y}_{i} = \\widehat{\\beta_{0}}\\sum_{i = 1}^{n}x_{i1} + \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}{x_{i1}}^{2} + \\widehat{\\beta_{2}}\\sum_{i = 1}^{n}{x_{i1}x_{i2}} + \\ldots + \\widehat{\\beta_{k}}\\sum_{i = 1}^{n}{x_{i1}x_{ik}}\\]\n\\[\\vdots .\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\ \\ \\ \\ \\ \\ \\ \\  \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\vdots\\]\n\\[\\sum_{i = 1}^{n}{x_{ik}y}_{i} = \\widehat{\\beta_{0}}\\sum_{i = 1}^{n}x_{ik} + \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}{x_{ik}x_{i1}} + \\widehat{\\beta_{2}}\\sum_{i = 1}^{n}{x_{ik}x_{i2}} + \\ldots + \\widehat{\\beta_{k}}\\sum_{i = 1}^{n}{x_{ik}}^{2}\\]\n\\(\\begin{bmatrix} \\sum_{i = 1}^{n}y_{i} \\\\ \\begin{matrix} \\sum_{i = 1}^{n}{x_{i1}y}_{i} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\sum_{i = 1}^{n}{x_{ik}y}_{i} \\\\ \\end{bmatrix} = \\left\\lbrack \\begin{matrix} \\begin{matrix} n \\\\ \\begin{matrix} \\sum_{i = 1}^{n}x_{i1} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} & \\begin{matrix} \\sum_{i = 1}^{n}x_{i1} \\\\ \\begin{matrix} \\sum_{i = 1}^{n}{x_{i1}}^{2} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} \\\\ \\sum_{i = 1}^{n}x_{ik} & \\sum_{i = 1}^{n}{x_{ik}x_{i1}} \\\\ \\end{matrix}\\ \\ \\ \\begin{matrix} \\begin{matrix} \\ldots \\\\ \\begin{matrix} \\ldots \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} & \\begin{matrix} \\sum_{i = 1}^{n}x_{ik} \\\\ \\begin{matrix} \\sum_{i = 1}^{n}{x_{i1}x_{ik}} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\end{matrix} \\\\ \\ldots & \\sum_{i = 1}^{n}{x_{ik}}^{2} \\\\ \\end{matrix} \\right\\rbrack\\begin{bmatrix} \\widehat{\\beta_{0}} \\\\ \\begin{matrix} \\widehat{\\beta_{1}} \\\\  \\vdots \\\\ \\end{matrix} \\\\ \\widehat{\\beta_{k}} \\\\ \\end{bmatrix}\\)\nPour faciliter la résolution du système d’équation nous allons utiliser la forme matricielle du modèle.\n\\[S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right) = \\sum_{i = 1}^{n}e_{i}^{2} = \\varepsilon^{'}\\varepsilon = (Y - X\\beta)^{'}(Y - X\\beta)\\]\n\\[S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right) = Y^{'}Y - \\beta^{'}X^{'}Y - Y^{'}X\\beta + \\beta^{'}X^{'}X\\beta\\]\n\\[S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right) = Y^{'}Y - 2\\beta^{'}X^{'}Y + \\beta^{'}X^{'}X\\beta\\]\nPuisque \\(\\beta^{'}X^{'}Y\\) est une matrice (ou un scalaire) \\(1 \\times 1\\) et sa transposée \\(Y^{'}X\\beta\\) est le même scalaire. La minimisation de \\(S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right)\\) satisfait :\n\\[\\frac{\\partial S\\left( \\beta_{0},\\beta_{1},\\cdots,\\beta_{k} \\right)}{\\partial\\beta} = - 2X^{'}Y + X^{'}X\\widehat{\\beta}\\]\nOn a :\n\\[X^{'}X\\widehat{\\beta} = X^{'}Y\\]\nPour résoudre l’équation, on multiplie les deux côtés de l’équation par \\({(X^{'}X)}^{- 1}\\) en supposant que la matrice \\({(X^{'}X)}^{- 1}\\) existe. La matrice \\({(X^{'}X)}^{- 1}\\) va toujours exister si les variables explicatives sont linéairement indépendantes, c’est-à-dire qu’aucune colonne de la matrice \\(X\\) n’est une combinaison linéaire des autres colonnes.\n\\[\\widehat{\\beta} = {(X^{'}X)}^{- 1}X^{'}Y\\]\n\n\nInterprétation de la régression multiple en effet partiel\nL’équation estimée par les MCO de la RLM est comme suit :\n\\[\\widehat{y} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{1} + \\widehat{\\beta_{2}}x_{2} + \\ldots + \\widehat{\\beta_{k}}x_{k}\\]\n\\(\\widehat{\\beta_{0}}\\) représente l’ordonnée à l’origine lorsque les \\(x_{j}\\) sont égales à zéro. Dans certains cas, l’estimation de l’ordonnée à l’origine donne des informations intéressantes ; dans d’autres, elle n’a pas de sens. Notons, cependant que cette ordonnée à l’origine est indispensable si nous désirons obtenir une estimation \\(y\\) à partir de la régression des MCO lorsque les \\(x_{j}\\) sont égales à zéro. Sans cette ordonnée à l’origine la valeur du \\(y\\) sera toujours nulle lorsque les \\(x_{j}\\) sont égales à zéro.\nLes estimations des \\(\\widehat{\\beta_{j}}\\) s’interprètent comme des effets marginaux ou des effets ceteris paribus. De l’équation estimée, nous déduisons que :\n\\[\\Delta\\widehat{y} = \\widehat{\\beta_{1}}{\\Delta x}_{1} + \\widehat{\\beta_{2}}{\\Delta x}_{2} + \\ldots + \\widehat{\\beta_{k}}{\\Delta x}_{k}\\]\nToutes choses étant égales par ailleurs, c’est-à-dire si toutes les autres variables sont maintenues constantes ou fixées, le coefficient de \\(x_{1}\\) mesure le changement de \\(\\widehat{y}\\) dû à une variation d’une unité de \\(x_{1}\\). Soit :\n\\[\\Delta\\widehat{y} = \\widehat{\\beta_{1}}{\\Delta x}_{1}\\]\nEn gardant \\(x_{2},x_{3},\\ldots,x_{k}\\) constants. De cette manière, nous neutralisons l’effet des variables \\(x_{2},x_{3},\\ldots,x_{k}\\) quand nous estimons l’effet de \\(x_{1}\\) sur \\(y\\). Les autres coefficients ont une interprétation similaire.\n\n\n\nCas particulier : modèle sans terme constant\nLa théorie économique postule parfois des relations dans lesquelles \\(\\beta_{0} = 0\\) : c’est le cas par exemple pour une fonction de production de produit industriel où le facteur de production (unique) nul entraîne une production nulle. L’estimation de \\(\\beta_{1}\\) est alors donnée par la formule suivante :\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{N}{x_{i}y_{i}}}{\\sum_{i = 1}^{N}x_{i}^{2}}\\]\nNous remarquons qu’il s’agit de l’application de la formule générale dans laquelle \\(\\overline{x}\\) et \\(\\overline{y}\\) sont nulles. Dans le cas de variables centrées, c’est donc cette dernière formule qu’il convient d’employer car le terme constant est nul.\nNotons que, les données sont centrées lorsque les observations sont centrées sur leur moyenne : \\(\\left( x_{i} - \\overline{x} \\right)\\), la somme des données centrées est donc par construction nulle.\n\n\nPropriétés des estimateurs des MCO\nPropriétés 1 : Les estimateurs MCO sont sans biais \nPar définition le biais d’un paramètre \\(\\theta\\) dont l’estimateur est \\(\\widehat{\\theta}\\) est donné par :\n\\[\\mathbf{Biais = E}\\left( \\widehat{\\mathbf{\\theta}} \\right)\\mathbf{- \\theta}\\]\nAinsi, l’estimateur des MCO est sans biais si : \\(\\mathbf{E}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{i}}} \\right)\\mathbf{=}\\mathbf{\\beta}_{\\mathbf{i}}\\)\nNB : Démontrer les équivalences suivantes (Annexe B de Wooldridge)\n\\(\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right) = \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)y_{i}\\)\n\\[\\sum_{i = 1}^{n}\\left( y_{i} - \\overline{y} \\right)^{2} = \\sum_{i = 1}^{N}{y_{i}}^{2} - n{\\overline{y}}^{2}\\]\n\\[\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)x_{i} = \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} = SCE\\]\n\\[\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right) = 0\\]\n1er cas : L’intercepte\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)(y_{i} - \\overline{y})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)y_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)(\\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)(\\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[\\widehat{\\beta_{1}} = \\frac{\\beta_{0}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right) + \\beta_{1}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)x_{i} + \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\nSachant que \\(\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right) = 0\\) et \\(\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)x_{i} = \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\)\n\\[\\widehat{\\beta_{1}} = \\beta_{1} + \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[E(\\widehat{\\beta_{1})} = E(\\beta_{1}) + \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)E(\\varepsilon_{i})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[\\mathbf{E}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{1}}} \\right)\\mathbf{=}\\mathbf{\\beta}_{\\mathbf{1}}\\]\n2e cas : La constante\n\\[\\widehat{\\beta_{0}} = \\overline{y} - \\widehat{\\beta_{1}}\\overline{x}\\]\n\\[\\widehat{\\beta_{0}} = \\beta_{0} + \\beta_{1}\\overline{x} + \\overline{\\varepsilon} - \\widehat{\\beta_{1}}\\overline{x}\\]\n\\[\\widehat{\\beta_{0}} = \\beta_{0} - (\\widehat{\\beta_{1}} - \\beta_{1})\\overline{x} + \\overline{\\varepsilon}\\]\nOr \\(E\\left( \\widehat{\\beta_{1}} - \\beta_{1} \\right) = 0\\), donc :\n\\[E(\\widehat{\\beta_{0})} = {E(\\beta}_{0}) + E(\\overline{\\varepsilon})\\]\n\\[\\mathbf{E(}\\widehat{\\mathbf{\\beta}_{\\mathbf{0}}}\\mathbf{) =}\\mathbf{\\beta}_{\\mathbf{0}}\\]\nEn conclusion, les estimateurs des MCO sont sans biais si et seulement si les deux hypothèses suivantes sont respectées :\n\nHypothèse 2 : L’exogène \\(\\mathbf{x}\\) n’est pas stochastique (X est non aléatoire) ;\nHypothèse 3 : \\(E\\left( \\varepsilon_{i} \\right) = 0\\), l’espérance de l’erreur est nulle.\n\nPropriété2 : Les estimateurs MCO sont convergents \nUn estimateur \\(\\widehat{\\theta}\\) est convergent si : \\(\\mathbf{Lim\\ V}\\left( \\widehat{\\theta} \\right)\\mathbf{\\rightarrow 0\\ lorsque\\ n \\rightarrow \\infty}\\)\n1er cas : L’intercepte\nCalculons la Variance de \\(\\widehat{\\beta_{1}}\\ \\):\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)(y_{i} - \\overline{y})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\nOr \\(\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right) = \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)y_{i}\\)\nOn a donc :\n\\[\\widehat{\\beta_{1}} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)y_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[Var\\left( \\widehat{\\beta_{1}} \\right) = Var\\left( \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)y_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}} \\right) = \\left( \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}} \\right)^{2}Var\\left( y_{i} \\right)\\]\n\\[\\mathbf{V}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{1}}} \\right)\\mathbf{=}\\frac{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}\\mathbf{=}\\frac{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}{\\mathbf{SCE}}\\]\nUne autre méthode de calculer la variance de \\(\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\)\n\\[\\widehat{\\beta_{1}} = \\beta_{1} + \\ \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[V\\left( \\widehat{\\beta_{1}} \\right) = Var\\left( \\beta_{1} + \\ \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}} \\right)\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = Var\\left( \\beta_{1} \\right) + Var\\left( \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}} \\right)\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = 0 + \\frac{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}Var(\\varepsilon_{i})}{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{4}} = \\frac{\\frac{1}{n}Var(x)Var(\\varepsilon_{i})}{{(Var(x))}^{2}}\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\frac{Var(\\varepsilon_{i})}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\nOr d’après les hypothèses MCO : \\(Var(\\varepsilon_{i}) = \\ \\sigma_{\\varepsilon}^{2}\\), donc :\n\\[\\mathbf{V}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{1}}} \\right)\\mathbf{=}\\frac{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}\\mathbf{=}\\frac{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}{\\mathbf{SCE}}\\]\nConvergence de \\(\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\)\n\\[lorsque\\ n \\rightarrow \\infty\\ alors\\ \\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\ tend\\ également\\ vers\\ \\infty,\\ d^{'}où\\ V\\left( \\widehat{\\beta_{1}} \\right) \\rightarrow 0\\ car\\ \\sigma^{2} = cst\\]\nIl s’en suit donc que \\(\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\) est estimateur convergent.\n2e cas : La constante\nCalculons la Variance de \\(\\widehat{\\beta_{0}}\\ \\):\n\\[Var(\\widehat{\\beta_{0})} = Var(\\overline{y} - \\widehat{\\beta_{1}}\\overline{x})\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = Var\\left( \\overline{y} \\right) + Var(\\widehat{\\beta_{1})}{\\overline{x}}^{2} - 2\\overline{x}cov\\left( \\overline{y},\\widehat{\\beta_{1}} \\right)\\]\nOn montre que \\(cov\\left( \\overline{y},\\widehat{\\beta_{1}} \\right) = 0\\). On a donc :\n\\[\\ Var(\\widehat{\\beta_{0})} = Var(\\frac{1}{n}\\sum_{i = 1}^{n}y_{i}) - \\frac{\\sigma_{\\varepsilon}^{2}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}{\\overline{x}}^{2}\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\frac{1}{n^{2}}\\sum_{i = 1}^{n}{y_{i}}^{2} - \\frac{\\sigma_{\\varepsilon}^{2}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}{\\overline{x}}^{2}\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\frac{1}{n}Var(y_{i}) - \\frac{\\sigma_{\\varepsilon}^{2}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}{\\overline{x}}^{2}\\]\n\\[\\mathbf{V}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{0}}} \\right)\\mathbf{=}\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}\\left( \\frac{\\mathbf{1}}{\\mathbf{n}}\\mathbf{+}\\frac{{\\overline{\\mathbf{x}}}^{\\mathbf{2}}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}} \\right)\\mathbf{=}\\mathbf{\\sigma}^{\\mathbf{2}}\\left( \\frac{\\mathbf{1}}{\\mathbf{n}}\\mathbf{+}\\frac{{\\overline{\\mathbf{x}}}^{\\mathbf{2}}}{\\mathbf{SCE}} \\right)\\]\n\\[\\mathbf{lorsque\\ n \\rightarrow \\infty\\ alors\\ Lim\\ }\\mathbf{V}\\left( \\widehat{\\mathbf{\\beta}_{\\mathbf{0}}} \\right)\\mathbf{\\rightarrow 0}\\]\nOn démontre ainsi que \\(\\widehat{\\mathbf{\\beta}_{\\mathbf{0}}}\\) est aussi convergent comme \\(\\widehat{\\mathbf{\\beta}_{\\mathbf{1}}}\\).\nThéorème de Gauss-Markov\nSelon le théorème de Gauss-Markov, les estimateurs des MCO de la régression sont sans biais et convergents. On peut même aller plus loin et prouver que parmi les estimateurs linéaires sans biais de la régression, les estimateurs MCO sont à variance minimale c.-à-d. il n’existe pas d’autres estimateurs linéaires sans biais présentant une plus petite variance. Les estimateurs des MCO sont BLUE (Best Linear Unbiased Estimator). On dit qu’ils sont efficaces.\n\n\nAutres propriétés algébriques des MCO\nPropriété 1 : La moyenne des résidus est nulle :\n\\[\\frac{1}{N}\\sum_{i = 1}^{N}{e_{i} = 0}\\]\nEn effet\n\\[\\frac{1}{N}\\sum_{i = 1}^{N}e_{i} = \\frac{1}{N}\\sum_{i = 1}^{N}\\left\\lbrack y_{i} - \\left( \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} \\right) \\right\\rbrack = \\overline{y} - \\widehat{\\beta_{0}} - \\widehat{\\beta_{1}}\\overline{x} = \\overline{y} - \\left( \\overline{y} - \\widehat{\\beta_{1}}\\overline{x} \\right) - \\widehat{\\beta_{1}}\\overline{x} = 0\\]\nPropriété 2 : La covariance entre les résidus et les valeurs de la variable explicative est nulle :\n\\[\\sum_{i = 1}^{N}{x_{i}e_{i} = 0}\\]\nPropriété 3 : La régression passe par le point moyen de l’échantillon :\n\\[\\overline{y} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}\\overline{x}\\]\nPropriété 4 : La somme des valeurs observées de \\(y_{i}\\) est égale à la somme des valeurs ajustées \\(\\widehat{y_{i}}\\).\n\\[\\sum_{i = 1}^{N}y_{i} = \\sum_{i = 1}^{N}\\widehat{y_{i}}\\]\nPropriété 5 : La covariance entre les résidus et les valeurs prédites sont nulle :\n\\[\\sum_{i = 1}^{N}{\\widehat{y_{i}}e_{i}} = \\sum_{i = 1}^{N}\\left( \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} \\right)e_{i} = \\widehat{\\beta_{0}}\\sum_{i = 1}^{N}e_{i} + \\widehat{\\beta_{1}}\\sum_{i = 1}^{N}{x_{i}e_{i}} = 0\\]\nPropriété 6 : L’estimateur de la variance des erreurs est sans biais\nNous allons commencer par montrer la différence entre les erreurs (ou perturbations) et les résidus.\nA partir des MCO, la valeur prédite de \\(\\widehat{y_{i}}\\) conditionnellement à \\(x_{i}\\) est :\n\\[\\widehat{y_{i}} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i}\\]\nLe modèle de régression simple peut s’écrire sous deux formes selon qu’il s’agit\ndu modèle théorique spécifié par l’économiste ou du modèle estimé à partir d’un\néchantillon.\nModèle théorique spécifié par l’économiste avec \\(\\varepsilon_{i}\\) l’erreur inconnue :\n\\[y = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i}\\]\nModèle estimé à partir d’un échantillon d’observations :\n\\[y_{i} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} + e_{i}\\]\n\\[\\mathbf{y}_{\\mathbf{i}}\\mathbf{=}\\widehat{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{+}\\mathbf{e}_{\\mathbf{i}}\\]\nLa droite de régression des MCO est :\n\\[\\widehat{y} = \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i}\\]\nLe résidu des MCO est :\n\\[e_{i} = y_{i} - \\widehat{y_{i}} = \\left( \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i} \\right) - \\left( \\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i} \\right)\\]\n\\[\\mathbf{e}_{\\mathbf{i}}\\mathbf{=}\\mathbf{\\varepsilon}_{\\mathbf{i}}\\mathbf{+}\\left( \\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{-}\\widehat{\\mathbf{\\beta}_{\\mathbf{0}}} \\right)\\mathbf{+ (}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{-}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{)}\\mathbf{x}_{\\mathbf{i}}\\]\nLe résidu observé \\(e_{i}\\) est donc la différence entre les valeurs observées de la variable à expliquer et les valeurs ajustées à l’aide des estimations des coefficients du modèle. Les erreurs ne peuvent jamais être observées alors que les résidus sont calculés à partir d’une base de données.\nOn constate que le résidu \\(e_{i}\\) n’est pas égal à l’erreur \\(\\varepsilon_{i}\\). C’est la différence attendue entre ces deux termes qui est égale à zéro comme pour \\(\\beta_{0}\\) et \\(\\widehat{\\beta_{0}}\\), d’une part et \\(\\beta_{1}\\) et \\(\\widehat{\\beta_{1}}\\), d’autre part.\nMaintenant que nous comprenons la différence entre les erreurs et les résidus, nous pouvons estimer la variance de l’erreur \\(\\sigma^{2}\\). Comme \\(\\mathbf{\\sigma}^{\\mathbf{2}}\\mathbf{= E(}{\\varepsilon_{i}}^{\\mathbf{2}}\\mathbf{)}\\), on pourrait penser que \\(\\frac{1}{n}\\sum_{i = 1}^{n}{\\varepsilon_{i}}^{\\mathbf{2}}\\) est un estimateur sans biais de \\(\\sigma^{2}\\). Ce n’est malheureusement pas le cas pour la simple raison qu’il est impossible d’observer les erreurs \\(\\varepsilon_{i}\\). La bonne nouvelle est que nous pouvons remplacer les erreurs par les résidus. Nous obtenons alors :\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}{\\mathbf{e}_{\\mathbf{i}}}^{\\mathbf{2}} = \\frac{SCR}{n}\\]\nIl s’agit d’un « vrai estimateur » car il offre une règle de calcul qui s’applique à n’importe quel échantillon de données. L’inconvénient de cet estimateur est qu’il est biaisé, bien que ce biais soit négligeable lorsque \\(n\\) est grand. L’estimateur \\(\\frac{SCR}{n}\\) est biaisé pour la principale raison qu’il ne tient pas compte de deux contraintes que les résidus des MCO doivent respecter, à savoir :\n\\(\\sum_{i = 1}^{N}{e_{i} = 0}\\) et \\(\\sum_{i = 1}^{N}{x_{i}e_{i} = 0}\\)\nEn tenant compte de ces deux contraintes, nous perdons deux degrés de liberté. Si nous connaissons la valeur des \\(n - 2\\) résidus dans notre échantillon, nous sommes contraints de choisir les deux derniers résidus de sorte que ces conditions soient respectées. C’est la raison pour laquelle, il n’y a que \\(n - 2\\) degrés de liberté dans les résidus. Ainsi, l’estimateur sans biais de \\(\\sigma^{2}\\)que nous allons utiliser doit tenir compte de l’ajustement relatif aux degrés de liberté.\n\\[\\widehat{\\sigma^{2}} = \\frac{1}{n - 2}\\sum_{i = 1}^{n}{\\mathbf{e}_{\\mathbf{i}}}^{\\mathbf{2}} = \\frac{SCR}{n - 2}\\]\nOn a :\n\\[SCR = \\sum_{i = 1}^{N}{e_{i}}^{2} = \\sum_{i = 1}^{N}{(y_{i} - \\widehat{y_{i}})}^{2}\\]\n\\[SCR = \\sum_{i = 1}^{N}{e_{i}}^{2} = \\sum_{i = 1}^{N}{(y_{i} - (\\widehat{\\beta_{0}} + \\widehat{\\beta_{1}}x_{i}))}^{2} = \\sum_{i = 1}^{N}{y_{i}}^{2} - n{\\overline{y}}^{2} - \\widehat{\\beta_{1}}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\]\nOr,\n\\[\\sum_{i = 1}^{N}{y_{i}}^{2} - n{\\overline{y}}^{2} = \\sum_{i = 1}^{n}\\left( y_{i} - \\overline{y} \\right)^{2} = SCT\\]\n\\[\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} = SCE\\]\n\\[SCR = SCT - \\widehat{\\beta_{1}}SCE\\]\n\\[\\widehat{\\mathbf{\\sigma}^{\\mathbf{2}}}\\mathbf{=}\\frac{\\mathbf{SCT}\\mathbf{-}\\widehat{\\beta_{1}}\\mathbf{SCE}}{\\mathbf{n - 2}}\\]\n\n\n\nLa méthode du maximum de vraisemblance\nLa méthode du maximum de vraisemblance (MV) (en anglais « Maximum Likelihood method » ou ML method) permet aussi d’estimer les paramètres d’un modèle de régression, sous l’hypothèse que la vraie (loi) distribution desdits paramètres est connue.\nSi le principe pour les MCO est de trouver le paramètre qui minimise la somme des carrés des erreurs, la méthode du maximum de vraisemblance cherche par contre à trouver le paramètre à même (avec une forte probabilité) de reproduire les vraies valeurs de l’échantillon (celles réellement observées). Autrement dit, l’estimation par la méthode du maximum de vraisemblance cherche à trouver les valeurs des paramètres qui rendent l’observation de l’échantillon le plus vraisemblable.\nL’on notera aussi que, sous l’hypothèse que les erreurs sont normalement distribuées \n\\(\\mathbf{\\varepsilon}\\mathcal{↝ N}\\mathbf{(0,}\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}\\mathbf{\\ }\\mathbf{)}\\), les estimateurs des MCO et ceux du maximum de vraisemblance sont identiques.\nSupposons que, dans le modèle de régression linéaire simple comme suit :\n\\[Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}\\]\n\\(Y_{i}\\) est linéairement indépendants du terme d’erreur \\(\\varepsilon_{i}\\) et suit une distribution normale, avec une moyenne : \\(E\\left( Y_{i} \\right) = \\beta_{0} + \\beta_{1}X_{i}\\) et une variance = \\(V\\left( Y_{i} \\right) = \\sigma^{2}\\)\nOn sait que : \\(\\varepsilon_{i}\\mathcal{↝ N}(0,\\sigma_{\\varepsilon}^{2}\\mathbf{)}\\), alors \\(E\\left( \\varepsilon_{i} \\right) = 0\\). On en déduit que :\n\\[\\mathbf{E}\\left( \\mathbf{Y}_{\\mathbf{i}} \\right)\\mathbf{= E}\\left( \\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{+}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{X}_{\\mathbf{i}}\\mathbf{+}\\mathbf{\\varepsilon}_{\\mathbf{i}} \\right)\\mathbf{=}\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{+}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{X}_{\\mathbf{i}}\\]\nPuisque :\n\\[\\mathbf{Y}_{\\mathbf{i}}\\mathbf{- E}\\left( \\mathbf{Y}_{\\mathbf{i}} \\right)\\mathbf{=}\\left( \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i} \\right)\\mathbf{-}\\left( \\beta_{0} + \\beta_{1}X_{i} \\right)\\mathbf{=}\\varepsilon_{i}\\]\nAlors :\n\\[\\mathbf{Var}\\left( \\mathbf{Y}_{\\mathbf{i}} \\right)\\mathbf{=}{\\mathbf{E}\\left\\lbrack \\mathbf{Y}_{\\mathbf{i}}\\mathbf{- E}\\left( \\mathbf{Y}_{\\mathbf{i}} \\right) \\right\\rbrack}^{\\mathbf{2}}\\mathbf{=}{\\mathbf{E}\\left\\lbrack \\varepsilon_{i} \\right\\rbrack}^{\\mathbf{2}}\\mathbf{=}\\sigma_{\\varepsilon}^{2}\\]\nD’où, la distribution de \\(\\mathbf{Y}_{\\mathbf{i}}\\mathbf{\\ }\\)est comme suit :\n\\[Y_{i}\\mathcal{↝ N}\\left\\lbrack \\left( \\beta_{0} + \\beta_{1}X_{i} \\right)\\mathbf{;}\\sigma_{\\varepsilon}^{2} \\right\\rbrack\\]\nOn définit une fonction de densité de probabilité jointe qui, compte tenu de l'indépendance des Y, peut s'écrire comme un produit de n fonctions de densité individuelles, soit :\n\\[f\\left( Y_{1},Y_{2},\\ldots Y_{n} \\middle| \\beta_{0} + \\beta_{1}X_{i},\\sigma_{\\varepsilon}^{2} \\right) = f\\left( Y_{1} \\middle| \\beta_{0} + \\beta_{1}X_{i},\\ \\sigma_{\\varepsilon}^{2} \\right) \\times \\left( Y_{2} \\middle| \\beta_{0} + \\beta_{1}X_{i},\\ \\sigma_{\\varepsilon}^{2} \\right) \\times \\ldots \\times \\left( Y_{n} \\middle| \\beta_{0} + \\beta_{1}X_{i},\\ \\sigma_{\\varepsilon}^{2} \\right)\\]\n\\[= \\prod_{i = 1}^{n}{f\\left( Y_{i} \\right)\\ }\\]\nOù la \\(f\\left( Y_{i} \\right)\\) est la fonction de densité de la loi normale de moyenne \\(\\beta_{0} + \\beta_{1}X_{i}\\) et de variance \\(\\sigma_{\\varepsilon}^{2}\\).\n\\[f\\left( Y_{i} \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}}}\\exp\\left\\{ - \\frac{1}{2}\\frac{\\left( Y_{i} - \\beta_{0} - \\beta_{1}X_{i} \\right)²}{\\sigma_{\\varepsilon}^{2}} \\right\\}\\]\nLa fonction de densité de probabilité jointe devient alors :\n\\[\\begin{matrix}\nf\\left( Y_{1},Y_{2},\\ldots Y_{n} \\middle| \\beta_{0} + \\beta_{1}X_{i},\\sigma_{\\varepsilon}^{2} \\right) = \\prod_{i = 1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}}}\\exp\\left\\{ - \\frac{1}{2}\\frac{\\left( Y_{i} - \\beta_{0} - \\beta_{1}X_{i} \\right)²}{\\sigma^{2}} \\right\\}} \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\frac{1}{{\\sqrt{2\\pi\\sigma_{\\varepsilon}}}^{n}}\\exp\\left\\{ - \\frac{1}{2}\\sum_{}^{}\\frac{\\left( Y_{i} - \\beta_{0} - \\beta_{1}X_{i} \\right)²}{\\sigma_{\\varepsilon}^{2}} \\right\\} \\\\\n\\end{matrix}\\]\nLa fonction précédente est appelée fonction de vraisemblance, notée \\(L\\left( \\beta_{0},\\beta_{1},\\sigma_{\\varepsilon}^{2} \\right)\\) que l’on écrit :\n\\[L\\left( \\beta_{0},\\beta_{1},\\sigma_{\\varepsilon}^{2} \\right) = \\frac{1}{{\\sqrt{2\\pi\\sigma_{\\varepsilon}}}^{n}}\\exp\\left\\{ - \\frac{1}{2}\\sum_{}^{}\\frac{\\left( Y_{i} - \\beta_{0} - \\beta_{1}X_{i} \\right)²}{\\sigma_{\\varepsilon}^{2}} \\right\\}\\]\nLorsqu’on effectue une transformation logarithmique de la fonction de vraisemblance ci-dessus, l’on obtient la fonction log-vraisemblance qui servira de base à l’estimation des paramètres \\(\\widetilde{\\mathbf{\\beta}_{\\mathbf{0}}}\\ \\)et \\(\\widetilde{\\mathbf{\\beta}_{\\mathbf{1}}}\\) du maximum de vraisemblance. La fonction log-vraisemblance s’écrit :\n\\[\\ln{L\\left( \\beta_{0},\\beta_{1},\\sigma_{\\varepsilon}^{2} \\right)} = - n\\ln\\sigma_{\\varepsilon} - \\frac{n}{2}\\ln{(2\\pi}) - \\frac{1}{2}\\sum_{}^{}\\frac{\\left( Y_{i} - \\beta_{0} - \\beta_{1}X_{i} \\right)²}{\\sigma_{\\varepsilon}^{2}}\\]\nConsidérant que : \\(\\ln\\sigma_{\\varepsilon}^{2} = 2\\ln\\sigma_{\\varepsilon}\\) alors, \\(\\frac{1}{2}\\ln\\sigma^{2} = \\ln\\sigma_{\\varepsilon}\\), on peut alors fonction log-vraisemblance comme suit :\n\\[\\mathbf{\\ln}{\\mathbf{L}\\left( \\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{,}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{,}\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}} \\right)}\\mathbf{= -}\\frac{\\mathbf{n}}{\\mathbf{2}}\\mathbf{\\ln}\\mathbf{\\sigma}^{\\mathbf{2}}\\mathbf{-}\\frac{\\mathbf{n}}{\\mathbf{2}}\\mathbf{\\ln}{\\mathbf{(2}\\mathbf{\\pi}}\\mathbf{) -}\\frac{\\mathbf{1}}{\\mathbf{2}}\\sum_{}^{}\\frac{\\left( \\mathbf{Y}_{\\mathbf{i}}\\mathbf{-}\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{-}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{X}_{\\mathbf{i}} \\right)\\mathbf{²}}{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}\\]\nPour estimer les paramètres \\(\\widetilde{\\beta_{0}},\\widetilde{\\beta_{1}}\\ \\)et \\({\\widetilde{\\sigma}}_{\\varepsilon}^{2}\\) par le maximum de vraisemblance, la démarche va consister à maximiser la fonction log-vraisemblance ci-dessus. Ce qui revient à annuler ses dérivées premières par rapport aux arguments \\(\\beta_{0},\\beta_{1}\\) et \\(\\sigma_{\\varepsilon}^{2}\\). On obtient :\n\\[\\left\\{ \\begin{matrix}\n\\frac{\\partial\\ln{L(.)}}{\\partial\\beta_{0}} = - \\frac{1}{{\\widetilde{\\sigma}}_{\\varepsilon}^{2}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right) = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\\ \\ \\ } \\\\\n\\frac{\\partial\\ln{L(.)}}{\\partial\\beta_{1}} = - \\frac{1}{{\\widetilde{\\sigma}}_{\\varepsilon}^{2}}\\sum_{}^{}\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)X_{i}\\  = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)\\  \\\\\n\\frac{\\partial\\ln{L(.)}}{\\partial\\sigma_{\\varepsilon}^{2}} = - \\frac{n}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{2}} + \\frac{1}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{4}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)^{2} = 0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (3) \\\\\n\\end{matrix} \\right.\\\n\\]\n\\[\\left\\{ \\begin{matrix}\n\\frac{1}{{\\widetilde{\\sigma}}_{\\varepsilon}^{2}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right) = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\\ \\ \\ } \\\\\n\\frac{1}{{\\widetilde{\\sigma}}_{\\varepsilon}^{2}}\\sum_{}^{}\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)X_{i}\\  = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)\\  \\\\\n- \\frac{n}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{2}} + \\frac{1}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{4}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)^{2} = 0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (3) \\\\\n\\end{matrix} \\right.\\ \\]\nAlors simplification, les équations 1 et 2 donnent :\n\\[\\left\\{ \\begin{matrix}\n\\sum_{}^{}Y_{i} = n\\widetilde{\\beta_{0}} + \\widetilde{\\beta_{1}}\\sum_{}^{}X_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\\\\n\\sum_{}^{}{Y_{i}X_{i}} = \\widetilde{\\beta_{0}}\\sum_{}^{}X_{i} + \\widetilde{\\beta_{1}}\\sum_{}^{}X_{i}^{2} \\\\\n\\end{matrix} \\right.\\ \\]\nOn a donc :\n\\[\\widetilde{\\mathbf{\\beta}_{\\mathbf{0}}}\\mathbf{=}\\overline{\\mathbf{y}}\\mathbf{-}\\widetilde{\\mathbf{\\beta}_{\\mathbf{1}}}\\overline{\\mathbf{x}}\\]\net\n\\[\\widetilde{\\mathbf{\\beta}_{\\mathbf{1}}}\\mathbf{=}\\frac{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)\\mathbf{(}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{y}}\\mathbf{)}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}\\]\nOn en déduit que les estimateurs \\(\\widetilde{\\mathbf{\\beta}_{\\mathbf{0}}}\\ \\)et \\(\\widetilde{\\mathbf{\\beta}_{\\mathbf{1}}}\\) du maximum de vraisemblance sont identiques à ceux obtenus par la méthode des MCO.\nAprès simplification de l'équation 3, on obtient l'estimateur MV de \\({\\widetilde{\\sigma}}_{\\varepsilon}^{2}\\ \\):\n\\[\\frac{n}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{2}} + \\frac{1}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{4}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)^{2} = 0}\\]\n\\[- n + \\frac{1}{2{\\widetilde{\\sigma}}_{\\varepsilon}^{2}}\\sum_{}^{}{\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)^{2} = 0}\\]\n\\[{\\widetilde{\\sigma}}_{\\varepsilon}^{2} = \\frac{1}{n}\\sum_{}^{}\\left( Y_{i} - \\widetilde{\\beta_{0}} - \\widetilde{\\beta_{1}}X_{i} \\right)^{2}\\]\n\\[{\\widetilde{\\sigma}}_{\\varepsilon}^{2} = \\frac{1}{n}\\sum_{}^{}\\left( Y_{i} - \\widetilde{Y_{i}} \\right)^{2}\\]\n\\[{\\widetilde{\\mathbf{\\sigma}}}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}\\mathbf{=}\\frac{\\mathbf{1}}{\\mathbf{n}}\\sum_{}^{}{\\mathbf{e}_{\\mathbf{i}}^{\\mathbf{2}}\\mathbf{\\ }}\\]\nOr l’estimateur MCO de la variance des erreurs est donné par :\n\\[{\\widetilde{\\sigma}}_{\\varepsilon}^{2} = \\frac{\\mathbf{1}}{\\mathbf{n - 2}}\\sum_{}^{}{\\mathbf{e}_{\\mathbf{i}}^{\\mathbf{2}}\\mathbf{\\ }}\\]\nOn remarque l’estimateur du maximum de vraisemblance \\({\\widetilde{\\mathbf{\\sigma}}}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}\\) est différent de l’estimateur MCO \\({\\widehat{\\sigma}}_{\\varepsilon}^{2}\\) qui était sans biais. On en déduit donc que l'estimateur MV est biaisé, mais il reste convergent. Autrement dit, que lorsque la taille n de l'échantillon augmente, les deux estimateurs tendent, à s'égaliser. Par conséquent, asymptotiquement (lorsque n croît indéfiniment), l'estimateur MV de \\({\\widetilde{\\sigma}}_{\\varepsilon}^{2}\\) est également non biaisé.\nRemarque :\nLa méthode du Maximum de Vraisemblance est une alternative à la méthode des MCO. Elle est généralement appelée la méthode des grands échantillons. Elle est d'une application plus large car elle s'applique aussi aux modèles de régression non linéaires dans les paramètres. Cependant, son utilisation nécessite une bonne spécification de la forme fonctionnelle de la distribution des observations."
  },
  {
    "objectID": "MOD1_Yquanti_cours.html#chapitre-4-inférence-statistique",
    "href": "MOD1_Yquanti_cours.html#chapitre-4-inférence-statistique",
    "title": "Cours d’économétrie",
    "section": "Chapitre 4 : Inférence Statistique",
    "text": "Chapitre 4 : Inférence Statistique\n\nTests d’hypothèses sur les paramètres de la population : test de Student\nNous nous intéressons maintenant au problème de tests d’hypothèses sur les paramètres de la population du modèle de régression linéaire. Pour ce faire, nous devons connaitre la loi de distribution de \\({\\widehat{\\mathbf{\\beta}}}_{j}\\). Selon les hypothèses du modèle de régression linéaire, nous savons que le terme d’erreur \\(\\varepsilon_{i}\\) dans la population est indépendante des variables explicatives \\(x_{j}\\ (j = 1,\\ldots,k)\\) et suit une distribution normale de moyenne nulle et de variance \\(\\sigma^{2}\\), soit :\n\\[\\varepsilon_{i}\\ \\mathcal{↝ N(}0,\\sigma^{2})\\]\nOn en déduit que :\n\\[y_{i}׀x_{i}\\mathcal{↝ N(}\\beta_{0} + \\beta_{1}x_{i},\\sigma^{2})\\]\nChaque \\(\\widehat{\\beta_{j}}\\) peut s’écrire comme une combinaison linéaire des erreurs \\(\\varepsilon_{i}\\) et des variables indépendantes :\n\\[\\widehat{\\beta_{j}} = \\beta_{j} + \\ \\frac{\\sum_{i = 1}^{n}\\left( x_{ij} - \\overline{x} \\right)\\varepsilon_{i}}{\\sum_{i = 1}^{n}\\left( x_{ij} - \\overline{x} \\right)^{2}}\\]\nPuisque les \\(x_{i}\\)sont supposées non aléatoires, et étant donné que les erreurs sont indépendantes et identiquement distribuées selon une loi normale \\(\\mathcal{N(}0,\\sigma^{2})\\), il en ressort que conditionnellement aux valeurs prises dans l’échantillon par les variables indépendantes du modèle, on a :\n\\[\\widehat{\\beta_{j}}\\mathcal{↝ N}\\left( \\beta_{j},Var\\left( \\widehat{\\beta_{j}} \\right) \\right)\\ \\]\navec \\(Var\\left( \\widehat{\\beta_{j}} \\right) = \\frac{\\mathbf{\\sigma}_{\\mathbf{\\varepsilon}}^{\\mathbf{2}}}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\) dès lors, sous l’hypothèse nulle, on a :\n\\[\\frac{\\widehat{\\beta_{j}} - \\beta_{j}}{\\sqrt{Var\\left( \\widehat{\\beta_{j}} \\right)}} = \\frac{\\widehat{\\beta_{j}} - \\beta_{j}}{\\sigma\\left( \\widehat{\\beta_{j}} \\right)}\\ \\mathcal{↝ N(}0,1)\\]\nOr \\(\\sigma_{\\varepsilon}^{2}\\) est inconnu, on va donc le remplacer par son estimateur \\(\\widehat{\\sigma^{2}}\\). On a donc :\n\\[\\mathbf{t}_{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}}\\mathbf{=}\\frac{\\widehat{\\beta_{j}} - \\beta_{j}}{\\widehat{\\sigma}\\left( \\widehat{\\beta_{j}} \\right)} ↝ \\mathbf{t}_{\\mathbf{(}\\mathbf{n}\\mathbf{-}\\mathbf{p}\\mathbf{)}}\\]\nOù \\(p\\) est le nombre de paramètres du modèle.\nSupposons que nous voulons tester l’hypothèse selon laquelle l’intercepte est différente de zéro. Ce test est également appelé test de significativité du paramètre \\(\\beta_{j}\\). Il permet de vérifier l’influence réelle de l’exogène \\(x_{i}\\) sur l’endogène \\(y_{i}\\). Les hypothèses correspondantes à ce test sont données par :\n\\[\\left\\{ \\begin{matrix}\n\\mathbf{H}_{\\mathbf{0}}\\mathbf{:\\ }\\mathbf{\\beta}_{\\mathbf{j}}\\mathbf{=}\\mathbf{0} \\\\\n\\mathbf{H}_{\\mathbf{1}}\\mathbf{:\\ }\\mathbf{\\beta}_{\\mathbf{j}}\\mathbf{\\neq}\\mathbf{0} \\\\\n\\end{matrix} \\right.\\ \\]\nSous \\(Ho\\) la statistique ou ratio de Student est donné par :\n\\[\\mathbf{t}_{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{i}}}\\mathbf{=}\\frac{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}}{{\\widehat{\\mathbf{\\sigma}}}_{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{i}}}}\\mathbf{\\rightarrow}\\mathbf{t}_{\\mathbf{((}\\mathbf{n}\\mathbf{- 2)}}\\]\nSoit \\(\\alpha\\ \\)le seuil de significativité ou risque d’erreur. La règle de décision est comme suit :\n\nSi \\(t_{cal} > t_{lue}(n - p)\\) alors on rejette \\(H_{0}\\)\nSi \\(t_{cal} < t_{lue}(n - p)\\), on ne rejette pas \\(H_{0}\\)\n\nBien que \\(H_{0}:\\ \\beta_{j} = 0\\) soit l’hypothèse la plus courante, il arrive que nous soyons amenés à tester d’autres hypothèses relatives à \\(\\beta_{j}\\) et notamment la possibilité que le paramètre prenne d’autres valeurs constantes. Dans ce cas, les hypothèses posées sont comme suit :\n\\[\\left\\{ \\begin{matrix}\nH_{0}:\\ \\beta_{j} = a_{j} \\\\\nH_{1}:\\ \\beta_{j} \\neq a_{j} \\\\\n\\end{matrix} \\right.\\ \\]\nAvec \\(a_{j}\\) la valeur hypothétique de \\(\\beta_{j}\\) que nous stipulons. Dans ce cas, la statistique de Student, s’écrit :\n\\[t_{{\\widehat{\\mathbf{\\beta}}}_{i}} = \\frac{{\\widehat{\\mathbf{\\beta}}}_{j} - a_{j}}{{\\widehat{\\sigma}}_{{\\widehat{\\mathbf{\\beta}}}_{i}}} \\rightarrow t_{((n - 2)}\\]\nComme précédemment, la statistique de \\(t_{{\\widehat{\\mathbf{\\beta}}}_{i}}\\) mesure simplement le nombre d’écart-type qui sépare \\({\\widehat{\\mathbf{\\beta}}}_{j}\\) de sa valeur hypothétique \\(\\beta_{j}\\). La formulation générale de la statistique \\(t_{{\\widehat{\\mathbf{\\beta}}}_{i}}\\) est donné par :\n\\[t_{{\\widehat{\\mathbf{\\beta}}}_{i}} = \\frac{(estimateur - valeur\\ hypothétique)}{écart - type\\ estimé}\\]\n\n\nIntervalle de confiance des estimateurs des MCO\nSous les hypothèses du modèle de régression linéaire, nous pouvons également facilement construire un intervalle de confiance (IC) pour un paramètre de la population \\(\\beta_{j}\\). Cet intervalle de confiance fournit l’ensemble des valeurs possibles du paramètre de la population et pas simplement une estimation ponctuelle de cette valeur.\nAinsi, sous l’hypothèse alternative \\(\\mathbf{H}_{\\mathbf{1}}:\\ \\mathbf{\\beta}_{\\mathbf{j}} \\neq \\mathbf{0}\\), l’intervalle de confiance (\\(IC\\)) au niveau (\\(\\mathbf{1 - \\alpha})\\) c’est-à-dire avec 95% de chance de contenir le paramètre inconnu \\(\\mathbf{\\beta}_{\\mathbf{j}}\\) est donné par :\n\\[\\mathbf{IC = \\lbrack}{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}\\mathbf{-}\\mathbf{t}_{\\left( \\frac{\\mathbf{\\alpha}}{\\mathbf{2}}\\mathbf{;n - 2} \\right)}\\mathbf{\\times}{\\widehat{\\mathbf{\\sigma}}}_{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}}\\mathbf{\\ ;\\ }{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}\\mathbf{-}\\mathbf{t}_{\\left( \\frac{\\mathbf{\\alpha}}{\\mathbf{2}}\\mathbf{;n - 2} \\right)}\\mathbf{\\times}{\\widehat{\\mathbf{\\sigma}}}_{{\\widehat{\\mathbf{\\beta}}}_{\\mathbf{j}}}\\mathbf{\\rbrack}\\]\nAvec \\({\\widehat{\\sigma}}_{{\\widehat{\\beta}}_{i}}\\), l’estimateur de l’écart type de \\({\\widehat{\\beta}}_{j}\\).\n\nDécomposition de la variance et coefficient de détermination\n\nDécomposition de la variance - Équation d’analyse de variance\n\n\nL’analyse de la variance encore appelé ANOVA (Analysis of Variance) consiste à expliquer la variance totale sur l’ensemble des échantillons en fonction de la variance due à l’interaction entre les variables du modèle (la variance expliquée par le modèle) et de la variance résiduelle aléatoire (la variance non expliquée par le modèle). Elle est fondée sur l’orthogonalité entre le vecteur des résidus estimés et de la variable prédite.\n\\[y_{i} = \\widehat{y_{i}} + e_{i}\\]\n\\[\\sum_{i = 1}^{n}y_{i} = \\sum_{i = 1}^{n}\\widehat{y_{i}} + \\sum_{i = 1}^{n}e_{i}\\]\nOn a donc (en divisant par \\(\\frac{1}{n}\\))\n\\[\\overline{y_{i}} = \\overline{\\widehat{y_{i}}}\\]\nCar : \\(\\sum_{i = 1}^{n}{e_{i} = 0}\\)\n\\[y_{i} - \\overline{y_{i}} = \\widehat{y_{i}} - \\overline{\\widehat{y_{i}}} + e_{i}\\]\n\\[{(y}_{i} - \\overline{y_{i}}) = (\\widehat{y_{i}} - \\overline{y_{i}}) + (y_{i} - \\widehat{y_{i}})\\]\n\\[\\sum_{i = 1}^{n}{(y_{i} - \\overline{y_{i}})}^{2} = \\sum_{i = 1}^{n}{(\\widehat{y_{i}} - \\overline{y_{i}})}^{2} + \\sum_{i = 1}^{n}{(y_{i} - \\widehat{y_{i}})}^{2} + 2\\sum_{i = 1}^{n}{(\\widehat{y_{i}} - \\overline{y_{i}})(y_{i} - \\widehat{y_{i}})}\\]\nOr,\n\\[2\\sum_{i = 1}^{n}{(\\widehat{y_{i}} - \\overline{y_{i}})(y_{i} - \\widehat{y_{i}})} = 2\\sum_{i = 1}^{n}{\\widehat{y_{i}}(y_{i} - \\widehat{y_{i}})} - 2\\overline{y_{i}}\\sum_{i = 1}^{n}{(y_{i} - \\widehat{y_{i}})}\\]\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = 2\\sum_{i = 1}^{n}{\\widehat{y_{i}}e_{i}} - 2\\overline{y_{i}}\\sum_{i = 1}^{n}e_{i} = 0\\ (voir\\ les\\ hypothèses\\ des\\ MCO)\\]\nOn a donc,\n\\[\\sum_{i = 1}^{n}{(y_{i} - \\overline{y_{i}})}^{2} = \\sum_{i = 1}^{n}{(\\widehat{y_{i}} - \\overline{y_{i}})}^{2} + \\sum_{i = 1}^{n}{(y_{i} - \\widehat{y_{i}})}^{2}\\]\n\\[\\mathbf{SCT}\\mathbf{=}\\mathbf{SCE}\\mathbf{+}\\mathbf{SCR}\\]\nCette équation est l’équation fondamentale de l’analyse de la variance pour les modèles de régression. La variance totale est égale à la somme de la variance expliquée et de la variance des résidus ou encore la somme des carrés totaux (SCT) est égale à la somme des carrés expliqués (SCE) et de la somme des carrés des résidus (SCR). SCT indique la variabilité expliquée par le modèle, c’est-à-dire la variation de \\(y\\) expliquée par \\(x\\). SCR indique la variabilité non-expliquée par le modèle, c’est-à-dire l’écart entre les valeurs observées de \\(y\\) et celle prédites par le modèle.\nDeux situations extrêmes peuvent survenir :\n\nDans le meilleur des cas, \\(SCR = 0\\ \\): les variations de \\(y\\) sont complétement expliquées par celles de \\(x\\). On a un modèle parfait. La droite de régression passe exactement par tous les points du nuage (\\(\\widehat{y_{i}} = y_{i}\\)).\nDans le pire des cas, \\(SCE = 0\\ \\): \\(x\\) n’apporte aucune information sur \\(y\\). Ainsi, \\(\\widehat{y_{i}} = \\overline{y_{i}}\\), la meilleure prédiction de \\(y\\) est sa propre moyenne.\n\nLe coefficient de détermination\n\n\nIl est possible de déduire un indicateur synthétique à partir de l’équation d’analyse de variance. C’est le coefficient de détermination \\(R^{2}\\) qui permet de mesurer la qualité de l’ajustement. Il est donné par :\n\\[\\mathbf{R}^{\\mathbf{2}}\\mathbf{=}\\frac{\\mathbf{SCE}}{\\mathbf{SCT}}\\mathbf{= 1}\\mathbf{-}\\frac{\\mathbf{SCR}}{\\mathbf{SCT}}\\]\n\\[\\mathbf{R}^{\\mathbf{2}}\\mathbf{=}\\frac{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\widehat{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{-}\\overline{y_{i}}\\mathbf{)}}^{\\mathbf{2}}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{)}}^{\\mathbf{2}}}\\mathbf{= 1}\\mathbf{-}\\frac{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\mathbf{e}_{\\mathbf{i}}\\mathbf{)}}^{\\mathbf{2}}}{\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{)}}^{\\mathbf{2}}}\\]\nEn effet, plus la variance expliquée est proche de la variance totale, meilleur est l’ajustement du nuage de points par la droite des moindres carrés. \\(R^{2}\\) indique la proportion de variance de \\(y\\) expliquée par le modèle. Autrement dit, \\(100*R^{2}\\) est le pourcentage de la variance de \\(y\\) expliquée par \\(x\\).\nPropriété :\n\n\\(R^{2}\\) est une quantité positive,\n\\(0 \\leq r^{2} \\leq 1\\)\nPlus \\(R^{2}\\) est proche de 1, meilleur sera le modèle. La connaissance des valeurs de \\(x\\) permet de deviner avec précision celle de \\(y\\).\nLorsque \\(R^{2}\\) est proche de 0, cela veut dire \\(x\\) n’apporte pas d’informations utiles sur \\(y\\).\n\nCependant, il est important de noter qu’on le peut juger de la qualité d’une régression uniquement que la base de \\(R^{2}\\).\n\nÉvaluation globale de la régression\nNous avions mis en avant la décomposition de la variance et le coefficient de détermination \\(R^{2}\\) pour évaluer la qualité de l’ajustement. Le \\(R^{2}\\) indique dans quelle proportion la variabilité de \\(Y\\) pouvait être expliquée par \\(X\\). En revanche, il ne répond pas à la question : est-ce que la régression est globalement significative ? En d’autres termes, est-ce que les \\(X\\) (il n’y en a qu’un seul pour l’instant dans la régression simple) emmènent significativement de l’information sur \\(Y\\) représentative d’une relation linéaire réelle dans la population, et qui va au-delà̀ des simples fluctuations d’échantillonnage ?\nPour répondre à cette question, nous allons étendre l’étude de la décomposition de la variance par l’analyse du tableau de la variance.\n\n\nTableau d’analyse de la variance\nLe tableau ci-dessous présente l’analyse de la variance pour un modèle de régression simple.\n\n\n\n\n\n\n\n\n\nSource de variation\nSomme des carrés\nDegré de liberté\nCarrés moyens\n\n\n\n\n\\[x\\]\n\\[\\mathbf{SCE =}\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\widehat{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{-}\\overline{\\widehat{\\mathbf{y}_{\\mathbf{i}}}}\\mathbf{)}}^{\\mathbf{2}}\\]\n\\[1\\]\n\\[\\frac{\\mathbf{SCE}}{\\mathbf{1}}\\]\n\n\nRésidus\n\\[\\mathbf{SCR =}\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\mathbf{e}_{\\mathbf{i}}\\mathbf{)}}^{\\mathbf{2}}\\]\n\\[n - 2\\]\n\\[\\frac{\\mathbf{SCR}}{\\mathbf{(n}\\mathbf{-}\\mathbf{2)}}\\]\n\n\nTotal\n\\[\\mathbf{SCT =}\\sum_{\\mathbf{i = 1}}^{\\mathbf{n}}{\\mathbf{(}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}\\overline{\\mathbf{y}_{\\mathbf{i}}}\\mathbf{)}}^{\\mathbf{2}}\\]\n\\[n - 1\\]\n\n\n\n\nÀ partir du tableau de l’ANOVA, nous effectuons le test de la linéarité de la régression en calculant la statistique de Fisher \\(F\\) qui suit une loi de Fisher \\(F(1,\\ n - 2)\\) degré de liberté. Il revient à tester si la variable explicative \\(x\\) contribue pas à l’explication du modèle. Le test d’hypothèse est le suivant :\n\\[H_{0}:\\beta_{1} = 0\\ \\  \\leftrightarrow \\ H_{0}:SCE = 0\\]\nLa statistique de Ficher est donnée par :\n\\[\\mathbf{F}^{\\mathbf{*}}\\mathbf{=}\\frac{\\frac{\\mathbf{SCE}}{\\mathbf{1}}}{\\frac{\\mathbf{SCR}}{\\mathbf{(n}\\mathbf{-}\\mathbf{2)}}}\\mathbf{=}\\left( \\mathbf{n}\\mathbf{-}\\mathbf{2} \\right)\\frac{\\mathbf{SCE}}{\\mathbf{SCR}}\\mathbf{= (n - 2)}\\frac{\\mathbf{R}^{\\mathbf{2}}}{\\mathbf{(1 -}\\mathbf{R}^{\\mathbf{2}}\\mathbf{)}}\\]\nLa statistique de \\(F^{*}\\) est le rapport de la somme des carrés expliqués par \\(x\\) et de la somme des carrés des résidus, chacune de ces sommes étant divisée par son degré de liberté respectif. Ainsi, si la variance expliquée est significativement supérieure à la variance résiduelle, la variable \\(x\\) est considérée comme étant une variable réellement explicative.\nLa statistique \\(F^{*}\\) suit une loi de Ficher à \\(1\\ et\\ n - 2\\) degré de liberté.\n\nSi \\(\\mathbf{F}^{\\mathbf{*}}\\mathbf{\\geq}\\mathbf{F}_{\\mathbf{(1,\\ n}\\mathbf{-}\\mathbf{2)}}^{\\mathbf{\\alpha}}\\) alors nous décidons de rejeter l’hypothèse nulle \\(H_{0}\\) et par conséquent d’accepter l’hypothèse alternative \\(H_{1}\\) au risque \\(\\alpha\\), c’est-à-dire qu’il existe une liaison linéaire significative entre \\(x\\) et \\(y\\).\nSi \\(\\mathbf{F}^{\\mathbf{*}}\\mathbf{\\leq}\\mathbf{F}_{\\mathbf{(1,\\ n}\\mathbf{-}\\mathbf{2)}}^{\\mathbf{\\alpha}}\\) alors nous décidons de ne pas rejeter l’hypothèse nulle \\(H_{0}\\) et par conséquent de l’accepter, c’est-à-dire qu’il n’existe pas une liaison linéaire significative entre \\(x\\) et \\(y\\).\n\nEn effet, si l’hypothèse nulle \\(H_{0}\\) est vérifiée alors cela implique que \\(\\rho(x,y) = 0\\) c’est-à-dire \\(Cov(x,y) = 0\\). Donc il n’existe aucune liaison linéaire entre \\(x\\) et \\(y\\).\nOn peut encore écrire \\(F^{*}\\) comme suit :\n\\[F^{*} = \\left( t_{\\widehat{\\beta_{1}}}^{*} \\right)^{2} = \\left( \\frac{{\\widehat{\\beta}}_{1}}{{\\widehat{\\sigma}}_{{\\widehat{\\beta}}_{1}}} \\right)^{2} = \\frac{\\widehat{\\beta_{1}}}{\\frac{{\\widehat{\\sigma}}_{\\varepsilon}^{2}}{\\sum_{i}^{}\\left( x_{i} - \\overline{x} \\right)^{2}}} = \\frac{{\\widehat{\\beta}}_{1}^{2}\\sum_{i}^{}\\left( x_{i} - \\overline{x} \\right)^{2}}{\\frac{\\sum_{i}^{}e_{i}^{2}}{(n - 2)}}\\]\nLa statistique \\(F^{*}\\) permet de tester la significativité globale de la régression ou encore d’effectuer une évaluation globale de la régression, sous l’hypothèse nulle d’absence de liaison linéaire entre la variable endogène et les variables exogènes. Cette statistique indique si la variance expliquée est significativement supérieure à la variance résiduelle. Dans ce cas, on peut considérer que l’explication donnée par la régression traduit une relation qui existe réellement dans la population.\n\n\n\nLa prévision\nAprès l’estimation des paramètres, la question que l’on se pose est : quelle utilisation faire de cette régression passée ? L’une des utilisations est la « prédiction» de la valeur future de la variable expliquée étant donné un certain niveau de la variable explicative.\nLa prévision suppose donc que l’on connait la valeur de la variable explicative à un instant (\\(\\mathbf{t}\\mathbf{+ h}\\)) c’est-à-dire \\(\\mathbf{X}_{\\mathbf{t + h}}\\) est connu et l’on cherche à prévoir \\(\\mathbf{Y}_{\\mathbf{t + h}}\\mathbf{.}\\ t\\) est appelé l’origine de la prévision et \\(h\\), l’horizon de la prévision.\nNous savons que la valeur prédite de \\(Y_{t}\\) est donné par :\n\\[{\\widehat{Y}}_{t} = {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}X_{t}\\]\nLa valeur prédite \\(Y\\) à l’instant \\(\\mathbf{t}\\mathbf{+ h}\\) sera donc donnée par :\n\\[{\\widehat{Y}}_{t + h} = {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}X_{t + h}\\]\nPuisque \\({\\widehat{Y}}_{t + h}\\ \\)est un estimateur, il est probablement différent de sa valeur réelle \\(Y_{t + h}\\). L’erreur de prédiction est définie par : \\({\\mathbf{Y}_{\\mathbf{t}}\\mathbf{-}\\widehat{\\mathbf{Y}}}_{t}\\). On peut montrer que sous les hypothèses du modèle (incluant l’hypothèse de normalité), on a :\n\\[{\\mathbf{Y}_{\\mathbf{t}}\\mathbf{-}\\widehat{\\mathbf{Y}}}_{t} ↝ N(0,\\sigma_{\\varepsilon_{t + h}}^{2}(1 + \\frac{1}{n} + \\frac{\\left( X_{t + h} - \\overline{X} \\right)}{\\sum_{t = 1}^{T}\\left( X_{t} - \\overline{X} \\right)^{2}})\\]\nL’intervalle de prédiction est un intervalle dans lequel une future observation \\(\\mathbf{Y}_{\\mathbf{t}}\\) va tomber avec une certaine probabilité (différent d’un intervalle de confiance).\nOn montre que \\({\\widehat{Y}}_{t + h\\ }\\) suit une distribution normale de moyenne (\\(\\beta_{0} + \\beta_{1}X_{t + h}\\)) et de variance :\n\\[V\\left( {\\widehat{Y}}_{t + h} \\right) = \\sigma_{\\varepsilon_{t + h}}^{2}(1 + \\frac{1}{n} + \\frac{\\left( X_{t + h} - \\overline{X} \\right)}{\\sum_{t = 1}^{T}\\left( X_{t} - \\overline{X} \\right)^{2}}\\]\nEn remplaçant l’inconnue \\(\\sigma_{\\varepsilon_{t + h}}^{2}\\ \\)par son estimateur sans biais \\({\\widehat{\\sigma}}_{\\varepsilon_{t + h}}^{2}\\), on peut construire un intervalle de confiance pour le vrai \\(Y_{t + h}\\) et tester les hypothèses le concernant.\nPour ce faire, on définit :\n\\[\\frac{Y_{t + h} - {\\widehat{Y}}_{t + h}}{{\\widehat{\\sigma}}_{\\varepsilon_{t + h}}^{2}} \\rightarrow t(n - 2)\\]\nSoit \\({IC}_{p}\\ \\)l’intervalle de prédiction de\\(\\ {\\widehat{\\mathbf{Y}}}_{t + h}\\) au niveau de confiance (\\(1 - \\alpha)\\). Il est donné par :\n\\[{IC}_{p} = \\left\\lbrack {\\widehat{\\mathbf{Y}}}_{t + h}\\  \\pm \\mathbf{t}_{\\left( \\frac{\\mathbf{\\alpha}}{\\mathbf{2}}\\mathbf{;n - 2} \\right)} \\times \\left\\lbrack \\sigma_{\\varepsilon_{t + h}}^{2}(1 + \\frac{1}{n} + \\frac{\\left( X_{t + h} - \\overline{X} \\right)}{\\sum_{t = 1}^{T}\\left( X_{t} - \\overline{X} \\right)^{2}} \\right\\rbrack^{\\frac{1}{2}} \\right\\rbrack\\]\n\n\nExercices\nExercice 1\nA partir des données sur le niveau d’éducation (mesuré par le nombre d’années d’études) et les salaires horaires moyens perçus par les individus pour chaque niveau d’éducation, on obtient la régression suivante :\n\\[\\begin{matrix}\n{Salaire\\ moyen}_{i} = 0,7437 + 0,641{Education}_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\\\\nEcart - type = (0,8355)\\ \\ \\ (\\ \\ )\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ t = (\\ )\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (9,6536)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ r^{2} = 0,8944\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ n = 13 \\\\\n\\end{matrix}\\]\n1- Complétez les chiffres manquants.\n2-Comment interpréter le coefficient 0,6416 ?\n3-Rejetteriez-vous l’hypothèse selon laquelle l’éducation n’a pas d’influence sur les salaires ?\nQuel test utiliseriez-vous ? Pourquoi ?\n4 - Construisez la table ANOVA pour cet exemple et testez l’hypothèse d’un coefficient de pente nul. Quel test utiliseriez-vous et Pourquoi ?\n5 - Supposez que dans la régression donnée ci-dessus le \\(r^{2}\\) ne vous était pas fourni. Auriez-vous pu l’obtenir des autres informations contenues dans la régression ?\nExercice 2\nSoit les résultats d’une estimation économétrique :\n\\[\\begin{matrix}\n{\\widehat{Y}}_{i} = - 32,95 + 1,25X_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\\\\nn = 20\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ R^{2} = 0,23\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {\\widehat{ó}}_{{\\widehat{u}}_{i}}^{2} = 10,66 \\\\\n\\end{matrix}\\]\n1 - A partir des informations connues, on demande de retrouver les statistiques suivantes : la somme des carrés des résidus (SCR), la somme des carrés totaux (SCT), la somme des carrés expliqués (SCE), la statistique F de Fisher et l’écart-type de la pente.\n2- La pente est-elle significativement supérieur à 1?"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html",
    "href": "MOD1_Yquanti_exo1.html",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "",
    "text": "On se propose dans ce TD de modéliser la relation entre PIB par habiatnt (X) et émission de CO2 des pays africains (Y) en 2018 à l’aide d’une relation linéaire de type Y = f(X). On commencera par utiliser un modèle de régression linéaire simple en soulignant les multiples violation des hypothèses qu’il entraîne. Puis on proposera deux solutions alternatives, l’une en retirant les valeurs exceptionnelles, l’autre en transformant les variables X et Y de façon logarithmique."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#importation-des-données",
    "href": "MOD1_Yquanti_exo1.html#importation-des-données",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "1.1 Importation des données",
    "text": "1.1 Importation des données\n\ndon<-read.csv2(\"DEV-AFRIC-2018/data/afrika_don.csv\")"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#sélection-des-variables",
    "href": "MOD1_Yquanti_exo1.html#sélection-des-variables",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "1.2 Sélection des variables",
    "text": "1.2 Sélection des variables\nOn décide de renommer les deux variables choisies X et Y\n\nX : PIB en $/habitant\nY : CO2 en tonnes/habitant\n\n\ndon$X<-don$PIB\ndon$Y<-don$CO2HAB"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#extraction-du-tableau-à-analyser",
    "href": "MOD1_Yquanti_exo1.html#extraction-du-tableau-à-analyser",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "1.4 Extraction du tableau à analyser",
    "text": "1.4 Extraction du tableau à analyser\nOn ne garde que les colonnes iso3, name, reg, X et Y. Et on élimine les lignes comportant des valeurs manquantes à l’aide de la fonction complete.case()\n\ntab<-don[,c(\"iso3\",\"name\",\"X\",\"Y\")]\ntab<-tab[complete.cases(tab), ]"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#astuce-stockage-des-textes-dhabillage",
    "href": "MOD1_Yquanti_exo1.html#astuce-stockage-des-textes-dhabillage",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "1.5 Astuce : stockage des textes d’habillage",
    "text": "1.5 Astuce : stockage des textes d’habillage\nOn prépare un ensemble de textes que l’on pourra utiliser pour l’habillage de nos graphiques. Cela évitera de devoir ensuite les retaper à chaque fois.\n\nnomX <- \"PIB ($/hab)\"\nnomY <- \"Pollution (t. de CO2/hab).\" \ntitre <- \"Les pays Africains en 2018\"\nnote <- \"Source : Rapport sur le développement humain 2020\""
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#la-distribution-de-x",
    "href": "MOD1_Yquanti_exo1.html#la-distribution-de-x",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "2.1 La distribution de X",
    "text": "2.1 La distribution de X\n\nCalculer les paramètres principaux et commentez les\n\nsummary(tab$X)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  756.6  2014.9  3289.4  5168.8  6437.0 19458.9 \n\n\n\nCommentaire : Le PIB par habitant des pays africians varie entre 756 et 19459. Il est en moyenne de 5169. La moitié des pays ont un taux compris entre Q1 (2015) et Q3 (6437)\n\n\n\nFaire un histogramme\n\nHistogramme rapide\n\n\nhist(tab$X)\n\n\n\n\n\nHistogramme amélioré\n\n\nhist(tab$X, \n     xlab=nomX,\n     breaks=quantile(tab$X, c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)),\n     main = titre,\n     sub = note,\n     col = \"lightyellow\")\nlines(density(tab$X),col=\"red\")\n\n\n\n\n\nCommentaire :  La distribution semble unimodale mais fortement asymétrique à gauche.\n\n\n\nTester la normalité\n\n# Graphique \nqqnorm(tab$X)\nqqline(tab$X, col = \"red\")\n\n\n\n# test\nshapiro.test(tab$X)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tab$X\nW = 0.796, p-value = 1.584e-06\n\n\n\nCommentaire : Le graphique montre que la distribution ne suit pas une loi gaussienne, ce qui est confirmé par le test de Shapiro-Wilks (p < 0.001)\n\n\n\nExaminer la présence de valeurs exceptionnelles\n\nboxplot(tab$X, \n        horizontal = T,\n        xlab = nomX,\n        main = titre,\n        sub = note)\n\n\n\n\n\nCommentaire : La boxplot montre la présence d’au moins quatre valeurs excptionnelles situées à plus de 1.5*(Q3-Q1) de la médiane."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#la-distribution-de-y",
    "href": "MOD1_Yquanti_exo1.html#la-distribution-de-y",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "2.2 La distribution de Y",
    "text": "2.2 La distribution de Y\n\nCalculer les paramètres principaux\n\nsummary(tab$Y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.02423 0.18254 0.39648 1.14048 1.10447 8.09036 \n\n\n\nCommentaire : En 2018 les émissions de CO2 des pays d’u Monde’Afrique varient entre 0.02 t/hab. et 8.1 t./hab. La moyenne est de 1.14 t.hab. La moitié des pays se situent entre 0.18 t./hab (Q1) et 1.10 t./hab (Q3). L’écart entre la moyenne et la médiane suggère une distribution dissymétrique à gauche. Ce que l’on va vérifier avec l’histogramme.\n\n\n\nFaire un histogramme\n\nHistogramme rapide\n\n\nhist(tab$Y)\n\n\n\n\n\nHistogramme amélioré\n\n\nhist(tab$Y, \n     xlab=nomY,\n     breaks=quantile(tab$Y, c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)),\n     main = titre,\n     sub = note,\n     col=\"lightyellow\")\nlines(density(tab$Y),col=\"red\")\n\n\n\n\n\nCommentaire : La distribution de Y est unimodale mais très fortement dissymétrique à gauche.\n\n\n\nTester la normalité\n\n# Graphique \nqqnorm(tab$Y)\nqqline(tab$Y, col = \"red\")\n\n\n\n# test\nshapiro.test(tab$Y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tab$Y\nW = 0.6079, p-value = 7.414e-10\n\n\n\nCommentaire : Le graphique montre que la distribution ne suit pas une loi gaussienne, ce qui est confirmé par le test de Shapiro-Wilks (p < 0.001)\n\n\n\nExaminer la présence de valeurs exceptionnelles\n\nboxplot(tab$Y, \n        horizontal = T,\n        xlab = nomY,\n        main = titre,\n        sub = note)\n\n\n\n\n\nCommentaire : La boxplot montre la présence d’au moins cinq valeurs exceptionnelles situées à plus de 1.5*(Q3-Q1) de la médiane."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#visualiser-la-relation-entre-x-et-y",
    "href": "MOD1_Yquanti_exo1.html#visualiser-la-relation-entre-x-et-y",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "3.1 Visualiser la relation entre X et Y",
    "text": "3.1 Visualiser la relation entre X et Y\n\nGraphique rapide\n\n\nplot(tab$X,tab$Y)\n\n\n\n\n\nGraphique amélioré\n\n\nplot(tab$X,tab$Y,\n     cex = 0.6,\n     pch = 19,\n     col = \"red\",\n     xlab = nomX,\n     ylab = nomY,\n     main = titre,\n     sub = note)\n\ntext(tab$X, tab$Y, tab$iso3,\n     cex = 0.6,\n     col = \"blue\",\n     pos = 1)\n\n\n\n\n\nCommentaire : : La relation est clairement positive ce qui signifie que plus le PIB/habitant augmente, plus les émissions de CO2 par habitant augmente. Plus un pays est riche, plus il pollue ! Il n’est toutefois pas évident que la relation soit linéaire car deux pays (Afrique du Sud et Libye) s’écartent clairement de la tendance générale et suggèrent une relation de type puissance ou exponentielle."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#tester-la-significativité-de-la-relation-entre-x-et-y",
    "href": "MOD1_Yquanti_exo1.html#tester-la-significativité-de-la-relation-entre-x-et-y",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "3.2 Tester la significativité de la relation entre X et Y",
    "text": "3.2 Tester la significativité de la relation entre X et Y\n\nCoefficient de Pearson\n\ncor.test(tab$X,tab$Y)\n\n\n    Pearson's product-moment correlation\n\ndata:  tab$X and tab$Y\nt = 8.9738, df = 44, p-value = 1.688e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6703491 0.8873157\nsample estimates:\n      cor \n0.8041573 \n\ncor(tab$X,tab$Y)**2\n\n[1] 0.6466689\n\n\n\nCommentaire : Selon le test du coefficient de Pearson, la relation est très significative (p < 0.001) et le pouvoir explicatif de X par rapport à Y (r2) sera élevé (65%)\n\n\n\nCoefficien de Spearman\n\ncor.test(tab$X,tab$Y, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  tab$X and tab$Y\nS = 1654, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8979957 \n\n\n\nCommentaire : Le coefficient de corrélation de Spearman (+0.90) est sensiblement plus élevée que celui de Pearson (+0.80). Ceci constitue un signal d’alerte et suggère (i) soit la présence de valeurs exceptionnelles, (ii) soit l’existenced’une relation non linéaire."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#calculer-léquation-de-la-droite-y-axb",
    "href": "MOD1_Yquanti_exo1.html#calculer-léquation-de-la-droite-y-axb",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "4.1 Calculer l’équation de la droite Y = aX+B",
    "text": "4.1 Calculer l’équation de la droite Y = aX+B\n\nmonmodel <- lm(tab$Y~tab$X)\nsummary(monmodel)\n\n\nCall:\nlm(formula = tab$Y ~ tab$X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9949 -0.5007 -0.0551  0.1633  4.7028 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.4317539  0.2375862  -1.817    0.076 .  \ntab$X        0.0003042  0.0000339   8.974 1.69e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 44 degrees of freedom\nMultiple R-squared:  0.6467,    Adjusted R-squared:  0.6386 \nF-statistic: 80.53 on 1 and 44 DF,  p-value: 1.688e-11\n\n\n\nCommentaire : L’équation de la droite est donc **Y = 0.0003*X - 0.432**. Le coefficient de pente de la droite indique que les émissions de CO2 augmentent de 0.0003 tonnes chaque fois que le PIB par habitant augmente de 1 dollar. Ou si l’on préfère, que les émissions de CO2 augmentent de 0.3 tonnes chaque fois que le PIB/hab. augmente de 1000 dollars. La constante (Intercept) indique la valeur qui correspondrait à un pays totalement pauvre et elle serait négative ce qui est évidemment absurde. Le modèle linéaire peut aboutir à des absurdités …"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#visualiser-la-droite",
    "href": "MOD1_Yquanti_exo1.html#visualiser-la-droite",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "4.2 Visualiser la droite",
    "text": "4.2 Visualiser la droite\n\nplot(tab$X,tab$Y,\n     cex = 0.6,\n     pch = 19,\n     col = \"red\",\n     xlab = nomX,\n     ylab = nomY,\n     main = titre,\n     sub = note)\ntext(tab$X, tab$Y, tab$iso3,\n     cex = 0.6,\n     col = \"blue\",\n     pos = 1)\nabline(monmodel, col =\"black\", lwd =2)\n\n\n\n\n\nCommentaire:  La droite s’ajuste plus ou moins au nuage de points mais on remarque que les résidus sont mal répartis autour de celle-ci (autocorrélation) et que les points s’éloignent de plus en plus de la droite au fur et à mesure que X augmente ce qui signifie que la variance n’est pas constante (hétéroscédasticité). Même s’il semble avoir un fort pouvoir explicatif, le modèle semble donc souffrir de défauts importants que l’on discutera dans la partie finale."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#calculer-les-valeurs-estimées-et-les-résidus",
    "href": "MOD1_Yquanti_exo1.html#calculer-les-valeurs-estimées-et-les-résidus",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "4.3 Calculer les valeurs estimées et les résidus",
    "text": "4.3 Calculer les valeurs estimées et les résidus\n\n# Extraction des valeurs estimées et résiduelles\ntab$Yest <- monmodel$fitted.values\ntab$Yres <- monmodel$residuals\n\n# Affichage du tableau trié\ntab[order(tab$Yres),]\n\n   iso3                 name          X          Y        Yest        Yres\n5   BWA             Botswana 17700.3152 2.95736420  4.95231246 -1.99494826\n16  GAB                Gabon 14806.5905 2.52836465  4.07210181 -1.54373716\n41  SWZ            Swaziland  8647.0887 1.05493272  2.19850997 -1.14357725\n21  GNQ           Eq. Guinea 19458.9245 4.34492637  5.48724470 -1.14231833\n32  NAM              Namibia  9784.5769 1.74504687  2.54451013 -0.79946325\n7   CIV        Côte d'Ivoire  5133.5905 0.33469716  1.12977713 -0.79507998\n13  EGY                Egypt 11564.7950 2.42640693  3.08601530 -0.65960837\n17  GHA                Ghana  5303.5336 0.61471889  1.18147028 -0.56675138\n11  DJI             Djibouti  5366.7107 0.67179534  1.20068744 -0.52889210\n30  MRT           Mauritania  5119.7260 0.60443125  1.12555986 -0.52112861\n1   AGO               Angola  6793.7085 1.12098146  1.63475038 -0.51376892\n22  KEN                Kenya  4266.8368 0.35970573  0.86612877 -0.50642305\n34  NGA              Nigeria  5145.2871 0.64987572  1.13333499 -0.48345928\n8   CMR             Cameroon  3628.1177 0.32269646  0.67184374 -0.34914729\n48  ZMB               Zambia  3500.5119 0.30129599  0.63302873 -0.33173275\n36  SDN                Sudan  4059.5331 0.50347675  0.80307131 -0.29959456\n45  TZA             Tanzania  2625.1980 0.22195235  0.36677650 -0.14482415\n35  RWA               Rwanda  2157.3935 0.09123742  0.22448015 -0.13324273\n44  TUN              Tunisia 10759.6827 2.73031139  2.84111696 -0.11080557\n15  ETH             Ethiopia  2161.6109 0.13674141  0.22576299 -0.08902158\n46  UGA               Uganda  2151.6788 0.13506038  0.22274186 -0.08768148\n28  MLI                 Mali  2305.2361 0.18661531  0.26945079 -0.08283548\n18  GIN               Guinea  2531.2341 0.25609996  0.33819464 -0.08209468\n4   BFA         Burkina Faso  2160.5895 0.19739801  0.22545229 -0.02805428\n26  MAR              Morocco  7476.1792 1.84045155  1.84234373 -0.00189218\n20  GNB        Guinea-Bissau  1969.2724 0.18117668  0.16725762  0.01391906\n42  TCD                 Chad  1577.9727 0.06553287  0.04823243  0.01730044\n10  COG                Congo  3356.2418 0.61610961  0.58914478  0.02696483\n19  GMB               Gambia  2175.5690 0.26780913  0.23000875  0.03780039\n39  SLE         Sierra Leone  1690.8316 0.14093841  0.08256175  0.05837667\n3   BEN                Benin  3224.0433 0.62247911  0.54893275  0.07354636\n27  MDG           Madagascar  1629.6623 0.16302068  0.06395532  0.09906535\n9   COD      Dem. Rep. Congo  1091.9213 0.02422917 -0.09961426  0.12384343\n38  SEN              Senegal  3354.8272 0.73865658  0.58871450  0.14994208\n33  NER                Niger  1207.7762 0.10333371 -0.06437363  0.16770734\n31  MWI               Malawi  1051.1249 0.07611109 -0.11202366  0.18813475\n6   CAF Central African Rep.   938.9888 0.06508274 -0.14613312  0.21121586\n2   BDI              Burundi   756.5941 0.04667763 -0.20161380  0.24829143\n23  LBR              Liberia  1462.4118 0.32340916  0.01308122  0.31032794\n29  MOZ           Mozambique  1284.9965 0.28091941 -0.04088480  0.32180421\n49  ZWE             Zimbabwe  2982.9890 0.84928791  0.47560907  0.37367884\n43  TGO                 Togo  1574.2385 0.43325640  0.04709655  0.38615984\n12  DZA              Algeria 11414.5995 3.68769052  3.04032896  0.64736157\n25  LSO              Lesotho  2758.1290 1.26133125  0.40721135  0.85411991\n24  LBY                Libya 15096.0769 8.08792735  4.16015754  3.92776981\n47  ZAF         South Africa 12556.2802 8.09035696  3.38760439  4.70275256\n\n\n\nCommentaire : Le tableau permet de repérer les pays qui s’éloignent le plus de la droite en raison d’une surestimation ou d’une sous-estimation de leurs émissions de CO2 par le PIB. Les résidus négatifs correspondent à despays qui émettent moins de CO2 que ce que laisserait prévoir leur PIB. C’est par exemple le cas du Bostwana dont le PIB élevé (17700$/hab.) laissait prévoir 4.95 t. de CO2 par habitant mais qui en pratique n’en émet que 2.96 soit un résidu de -2 tonnes. Inversement le PIB de l’Afrique du Sud (12256 $/hab) laissait prévoir 3.9 tonnes de CO2 par habitan alors que la valeur observée est de 8.1 tonnes, soit un résidu de +4.7 tonnes de plus que prévu. Dans les deux cas on peut chercher des explications ad hoc (e.g. importance de la production de charbon en Afrique du Sud) mais il faut aussi se demander si ces écarts ne nont pas justes liés à une mauvaise spécification de notre modèle …"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#sauvegarder-les-résultats-du-modèle",
    "href": "MOD1_Yquanti_exo1.html#sauvegarder-les-résultats-du-modèle",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "4.4 Sauvegarder les résultats du modèle",
    "text": "4.4 Sauvegarder les résultats du modèle\n\nwrite.table(x = tab,\n            file = \"result.csv\",\n            row.names = FALSE)"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#autocorrélation-des-résidus",
    "href": "MOD1_Yquanti_exo1.html#autocorrélation-des-résidus",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "5.1 Autocorrélation des résidus",
    "text": "5.1 Autocorrélation des résidus\n\nplot(monmodel,\n     which = 1,\n     labels.id = tab$name,\n     col=\"red\")\n\n\n\ndurbinWatsonTest(monmodel)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.04054838       1.91116   0.712\n Alternative hypothesis: rho != 0\n\n\n\nCommentaire : le graphique permet de voir que les résidus ne sont pas indépendants des valeurs estimées de Y, ce qui signifie que les points se situent en moyenne tantôt au dessus de la droite de régression, tantôt en dessous ce qui fausse leur estimation. Dans un modèle sans autocorrélation, la courbe rouge devrait suivre la ligne pointillé correspondant à une moyenne nulle des résidus, ce qui n’est visiblement pas le cas. On peut s’en assurer à l’aide du test de Durbin Watson qui pose l’hypothèse H0 : Il existe une autocorrélation des résidus. Cette hypothèse ne peut pas être rejetée (p > 0.66) donc il existe bien une autocorrélation des résidus qui va fausser les prévisions du modèle de régression linéaire."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#normalité-des-résidus",
    "href": "MOD1_Yquanti_exo1.html#normalité-des-résidus",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "5.2 Normalité des résidus",
    "text": "5.2 Normalité des résidus\n\nplot(monmodel,\n     which = 2,\n     labels.id = tab$name,\n     col=\"red\")\n\n\n\nshapiro.test(tab$Yres)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tab$Yres\nW = 0.68437, p-value = 1.163e-08\n\n\n\nCommentaire : La normalité de la distribution des résidus est également une condition importante de validité du modèle de régression linéaire puisqu’elle permet de définir un intervalle de confiance des estimations en se servant de l’écart-type de ces résidus (e.g. + ou - 2 écarts-type pour un intervalle de confiance à 95%). Mais il est clair ici au vu du diragramme QQ plot que la condition de normalité des résidus n’est pas vérifiée, ce que confirme le test de shapiro (p < 0.001)"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#homogénéité-des-résidus",
    "href": "MOD1_Yquanti_exo1.html#homogénéité-des-résidus",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "5.3 Homogénéité des résidus",
    "text": "5.3 Homogénéité des résidus\n\nplot(monmodel,\n     which = 3,\n     labels.id = tab$name,\n     col=\"red\")\n\n\n\nncvTest(monmodel)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 65.47898, Df = 1, p = 5.8737e-16\n\n\n\nCommentaire : En liaison avec ce qui précède, l’analyse de l’homogénéité des résidus permet de vérifier si la variance des résidus est constante et donc si l’intervalle de confiance sera le même pour l’ensemble des valeurs estimées. Ici, ce n’est clairement pas le cas puisque le graphique monrre un net accroissement de la variance des résidus lorsque la valeur à estimer augmente. On peut vérifier l’absence d’homogénéité (appelée hétéroscédasticité) en appliquant le test de Breush-Pagan qui examine l’hypothèse “H0 : la distribution des résidus est homogène”. Dans notre exemple H0 est rejetée (p < 0.001) ce qui signifie que l’hypothèse d’homogénéité est clairement violée."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#absence-de-valeurs-exceptionnellement-influentes",
    "href": "MOD1_Yquanti_exo1.html#absence-de-valeurs-exceptionnellement-influentes",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "5.4 Absence de valeurs exceptionnellement influentes",
    "text": "5.4 Absence de valeurs exceptionnellement influentes\n\nplot(monmodel,\n     which = 4,\n     labels.id = tab$name,\n     col=\"red\")\n\n\n\noutlierTest(monmodel, labels = tab$name)\n\n             rstudent unadjusted p-value Bonferroni p\nSouth Africa 6.034761         3.2546e-07   1.4971e-05\nLibya        4.657740         3.0790e-05   1.4164e-03\n\n\n\nCommentaire : Le dernier test consiste à vérifier si la relation observée est bien le résultat d’un ensemble d’observations indépendante et non pas l’effet de la présence d’une ou deux valeurs exceptionnelles. Plusieurs tests sont ici possibles qui visent au même objectif : déterminer à quel point le retrait d’une valeur unique modifie le résultat de l’analyse, c’est à dire le coefficient de détermination et les paramètres a et b de l’équation Y=aX+b. Le graphique proposé par R utilise la distance de Cook pour mettre en valeur l’influence potentielle des valeurs exceptionnelles et on y retrouve sans surprise la Libye, l’Afrique du Sud et le Bostwana. On peut arriver à un résultat similaire en utilisant le test de Bonferroni qui signale le caractère exceptionellement influent de l’Afrique du Sud et de la Libye."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#tous-les-tests-dun-coup",
    "href": "MOD1_Yquanti_exo1.html#tous-les-tests-dun-coup",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "5.5 Tous les tests d’un coup",
    "text": "5.5 Tous les tests d’un coup\nUne fois que l’on a bien compris les tests précédents, on peut afficher les quatre graphiques correspondant en une seule commande :\n\npar(mfrow=c(2,2))\nplot(monmodel,\n     which = c(1,2,3,4),\n     labels.id = tab$name,\n     col=\"red\")"
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#modèle-linéaire-sans-valeurs-exceptionnelles.",
    "href": "MOD1_Yquanti_exo1.html#modèle-linéaire-sans-valeurs-exceptionnelles.",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "6.1 Modèle linéaire sans valeurs exceptionnelles.",
    "text": "6.1 Modèle linéaire sans valeurs exceptionnelles.\nOn décide de retirer les trois valeurs exceptionellement influentes qui ont été repérées dans la première analyse et de refaire une régression linéaire.\n\nCorrection du tableau\n\ntab2<-tab[!(tab$iso3 %in% c(\"ZAF\",\"BWA\",\"LBY\")),]\n\n\n\nCorrélation\n\ncor.test(tab2$X,tab2$Y, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  tab2$X and tab2$Y\nt = 16.456, df = 41, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8770905 0.9627927\nsample estimates:\n      cor \n0.9319357 \n\ncor.test(tab2$X,tab2$Y, method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  tab2$X and tab2$Y\nS = 1614, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8781335 \n\n\n\n\nRégression\n\nmonmodel2 <- lm(tab2$Y~tab2$X)\nsummary(monmodel2)\n\n\nCall:\nlm(formula = tab2$Y ~ tab2$X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67137 -0.21168 -0.02265  0.12596  1.33042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.452e-01  8.348e-02  -2.937  0.00542 ** \ntab2$X       2.280e-04  1.385e-05  16.456  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3666 on 41 degrees of freedom\nMultiple R-squared:  0.8685,    Adjusted R-squared:  0.8653 \nF-statistic: 270.8 on 1 and 41 DF,  p-value: < 2.2e-16\n\n# Extraction des valeurs estimées et résiduelles\ntab2$Yest <- monmodel2$fitted.values\ntab2$Yres <- monmodel2$residuals\n\n\n\nVisualisation\n\nplot(tab2$X,tab2$Y,\n     cex = 0.6,\n     pch = 19,\n     col = \"red\",\n     xlab = nomX,\n     ylab = nomY,\n     main = titre,\n     sub = note)\ntext(tab2$X, tab2$Y, tab2$iso3,\n     cex = 0.6,\n     col = \"blue\",\n     pos = 1)\nabline(monmodel2, col =\"black\", lwd =2)\n\n\n\n\n\n\nDiagnostics\n\npar(mfrow=c(2,2))\nplot(monmodel2,\n     which = c(1,2,3,4),\n     labels.id = tab2$name,\n     col=\"red\")\n\n\n\ndurbinWatsonTest(monmodel2)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.05326152      1.856255   0.622\n Alternative hypothesis: rho != 0\n\nshapiro.test(tab2$Yres)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tab2$Yres\nW = 0.91406, p-value = 0.003438\n\nncvTest(monmodel2)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 14.98559, Df = 1, p = 0.00010834\n\noutlierTest(monmodel2,labels = tab2$name)\n\n        rstudent unadjusted p-value Bonferroni p\nAlgeria 4.679998          3.263e-05    0.0014031\n\n\n\nCommentaire : Le nouveau modèle affiche une corrélation beaucoup plus élevée (r = = 0.96) et une bien meilleure qualité d’ajustement (r2 = 86.5%). Il demeure une forte autocorrélation des résidus (p >0.60) mais les résidus sont à peu près gaussiens (p >0.05). L’hétroscédasticité demeure élévée (p < 0.001) et on trouve une nouvelle valeur exceptionellement influente (Algérie). Il y a donc d’indéniables progrès mais le modèle n’est pas encore tout à fait satisfaisant."
  },
  {
    "objectID": "MOD1_Yquanti_exo1.html#modèles-non-linéaires",
    "href": "MOD1_Yquanti_exo1.html#modèles-non-linéaires",
    "title": "Exo 1 : PIB et CO2 en Afrique",
    "section": "6.2 Modèles non linéaires",
    "text": "6.2 Modèles non linéaires\nIl est toujours ennuyeux de retirer des valeurs exceptionnelles car on risque d’en trouver des nouvelles et c’est un processus sans fin. Il s’agit en outre d’une démarche criticable si on effectue le retrait des valeurs sans raisons objectives. Il est donc préférable d’essayer de garder toutes les valeurs mais de chercher à transformer les variables X et Y pour construire des fonctions différentes. On utilise classiquement quatre modèles (linéaire, exponentiel, logarithmique, puissance) selon que l’on applique ou non des transformations linéaires à X et Y.\n\nExamen visuel des quatre modèles\n\npar(mfrow=c(2,2))\n\nplot(tab$X,tab$Y, main = \"Linéaire : Y=a.X+b\", pch=20, col=\"red\",cex=0.5)\nplot(tab$X,log(tab$Y), main = \"Exponentiel : log(Y)=a.X+b\", pch=20, col=\"red\",cex=0.5)\nplot(log(tab$X),tab$Y, main = \"Logarithmique : Y = a.log(X)+b\", pch=20, col=\"red\",cex=0.5)\nplot(log(tab$X),log(tab$Y), main = \"Puissance : log(Y) = a.log(X)+b\", pch=20, col=\"red\",cex=0.5)\n\n\n\n\n\nCommentaire : Un simple examen visuel laisse présager que le modèle puissance est celui qui s’ajustera le mieux à une droite et offrira une répartition régulière des résidus conforme aux hypothèses.\n\n\n\nCalcul des coefficients de corrélation\n\npaste(\"Linéaire : \",round(cor(tab$X,tab$Y),3))\n\n[1] \"Linéaire :  0.804\"\n\npaste(\"Exponentiel : \", round(cor(tab$X,log(tab$Y)),3))\n\n[1] \"Exponentiel :  0.85\"\n\npaste(\"Logarithmique : \",round(cor(log(tab$X),tab$Y),3))\n\n[1] \"Logarithmique :  0.722\"\n\npaste(\"Puissance : \", round(cor(log(tab$X),log(tab$Y)),3))\n\n[1] \"Puissance :  0.913\"\n\n\n\nCommentaire : Le calcul des coefficients de corrélation confirme que cette solution donne le meilleur ajustement aux données. Noter bien que ce critère ne suffit pas à lui seul à choisir un modèle. Un modèle qui aurait un meilleur ajustement mais violerait les hypothèses ne devrait pas être retenu face à un modèle ayant un ajustement plus faible mais des résidus mieux distribués.\n\n\n\nPréparation des données\nOn crée un nouveau tableau de données\n\ndon$X<-log(don$PIB)\ndon$Y<-log(don$CO2)\ntab3<-don[,c(\"iso3\",\"name\",\"X\",\"Y\")]\ntab3<-tab3[complete.cases(tab3), ]\nnomXlog <- \"log(PIB en $/hab)\"\nnomYlog <- \"log(CO2 en t./hab)\" \ntitre <- \"Les pays Africains en 2018\"\nnote <- \"Source : Rapport sur le développement humain 2020\"\n\n\n\nRégression\n\nmonmodel3 <- lm(tab3$Y~tab3$X)\nsummary(monmodel3)\n\n\nCall:\nlm(formula = tab3$Y ~ tab3$X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.21875 -0.34992 -0.09064  0.27574  1.38323 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.69651    0.80784  -15.72   <2e-16 ***\ntab3$X        1.45733    0.09822   14.84   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5672 on 44 degrees of freedom\nMultiple R-squared:  0.8334,    Adjusted R-squared:  0.8296 \nF-statistic: 220.2 on 1 and 44 DF,  p-value: < 2.2e-16\n\n# Extraction des valeurs estimées et résiduelles\ntab3$Yest <- monmodel$fitted.values\ntab3$Yres <- monmodel$residuals\n\n\n\nVisualisation\n\nplot(tab3$X,tab3$Y,\n     cex = 0.6,\n     pch = 19,\n     col = \"red\",\n     xlab = nomXlog,\n     ylab = nomYlog,\n     main=titre,\n     sub = note)\ntext(tab3$X, tab3$Y, tab3$iso3,\n     cex = 0.6,\n     col = \"blue\",\n     pos = 1)\nabline(monmodel3, col =\"black\", lwd =2)\n\n\n\n\n\n\nDiagnostics\n\npar(mfrow=c(2,2))\nplot(monmodel3,\n     which = c(1,2,3,4),\n     labels.id = tab3$name,\n     col=\"red\")\n\n\n\ndurbinWatsonTest(monmodel3)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.03349644       1.87894    0.68\n Alternative hypothesis: rho != 0\n\nshapiro.test(tab3$Yres)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tab3$Yres\nW = 0.68437, p-value = 1.163e-08\n\nncvTest(monmodel3)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.248169, Df = 1, p = 0.2639\n\noutlierTest(monmodel3,labels = tab3$name)\n\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n        rstudent unadjusted p-value Bonferroni p\nLesotho 2.628744           0.011838      0.54453\n\nexp(-12.69)\n\n[1] 3.08179e-06\n\n\n\nCommentaires : Outre sa qualité d’ajustement élevée (r2 = 83%), le modèle final respecte beaucoup mieux les hypothèses théoriques d’un modèle de régression linéaire. Il demeure certes une légère autocorrélation des résidus et une disribution qui n’est pas tout à fait gaussienne. Mais les résidus sont désormais homogènes (p > 0.26) et aucune valeur influente n’est plus détectée par le test de Bonferoni. Bref, le modèle est acceptable.\n\n\n\nReprésenter la forme finale du modèle Y = f(X)\nLe modèle ayant été ajusté sous forme bi-logarithmique, il faut en rétablir l’équation sous la forme Y = f(X), ce qui suppose de transformer l’équation de la façon suivante :\n\n\\(log(Y) = a\\times {log(X)}+b <=> Y = e^{b} \\times X^{a}\\)\n\nCe qui nous donne l’équation finale :\n\n\\(log(CO2) = a\\times log(PIB) + b <=> CO2 = e^{-12.696}\\times PIB^{1.47} <=> CO2 = 0.000003\\times PIB^{1.47}\\)\n\nQue l’on peut représenter de la façon suivante :\n\nx<-seq(0,20000,100)\ny<- 0.000003*(x**1.47)\nplot(x,y,\n     type=\"l\",\n     col=\"red\",\n     lwd =2,\n     xlab = \"PIB en $/hab.\",\n     ylab = \"Estimation du CO2 en t./hab\",\n     main = \"Modèle final\") \ngrid()\n\n\n\n\n\nCommentaire : Notre modèle final offre une représentation assez fiable de la relation qui existe entre le PIB par habitant et les émissions de CO2 des pays africains en 2018. La forme de la relation est de type puissance avec un exposant de 1.41 > 1 ce qui indique que l’accroissement des émissions n’est pas linéaire mais de plus en plus rapide lorsque le développement augmente. Un pays dont le revenu est de 5000 $/hab. émettra moins de 1 tonne de CO2 par habiatnt alors qu’un pays dont le revenu est de 10 000 $/hab émettra plus de 2 tonnes et un pays dont le revenu est de 20 000 $ par habiatnt plus de 6 tonnes !"
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html",
    "href": "MOD1_Yquanti_exo2.html",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "",
    "text": "Pour des raisons de cohérence pédagogique, il est important que certains tableaux de données soient présents tout au long des cours qui seront dispensés pendant l’école d’été CIST 2022-2023. Compte-tenu des perspectives différentes qu’implique leur traitement, nous avons besoin d’au moins deux tableaux de nature différente :\nL’objectif de la présente note est de présenter un tableau du second type pouvant être utilisé dans le module Modélisation d’une variable Y quantitative. Il s’agit d’un tableau décrivant les pays africains en 2017-2018 à l’aide d’indicateurs tirés du rapport mondial sur le développement humain de 2020 et complété par quelques variables tirées de la base des pays du Monde du CEPII. Il possède plusieurs avantages :"
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#a.-données",
    "href": "MOD1_Yquanti_exo2.html#a.-données",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "A. Données",
    "text": "A. Données\nLe chargement des données s’effectue à l’aide de trois fichiers qui peuvent être importées dans R mais aussi bien dans d’autres logiciels de statistique (format .csv) ou de cartographie (format .shp).\n\ndon <- read.csv2(\"DEV-AFRIC-2018/data/afrika_don.csv\")\nmeta <- read.csv2(\"DEV-AFRIC-2018/data/afrika_don_meta.csv\")\nmap <- st_read(\"DEV-AFRIC-2018/shp/afrika_map.shp\",quiet = T)\n\n\nListe des variables\n\nkable(meta, caption = \"Défintion et source des variables\")\n\n\nDéfintion et source des variables\n\n\n\n\n\n\n\nCode\nDefinition\nSouce\n\n\n\n\niso3\nISO3 code of country\nHuman Development Report 2020\n\n\nname\nEnglish name of country\nHuman Development Report 2020\n\n\nnom\nFrench name of country\nHuman Development Report 2020\n\n\nPOP\nPopulation (Millions inhabitants)\nHuman Development Report 2020\n\n\nPIB\nGross Domestic Product ($ per capita ppp)\nHuman Development Report 2020\n\n\nIDH\nHuman Development Index\nHuman Development Report 2020\n\n\nADOFEC\nAdolescent birth rate (births per 1,000 women ages 15-19)       \nHuman Development Report 2020\n\n\nCO2HAB\nCarbon Dioxyde Emissions per capita (tonnes)\nHuman Development Report 2020\n\n\nEMPAGR\nEmployment in agriculture (% of total employment)          \nHuman Development Report 2020\n\n\nEMPSER\nEmployment in services (% of total employment)                    \nHuman Development Report 2020\n\n\nINTERN\nInternet users, total (% of population)                            \nHuman Development Report 2020\n\n\nESPVIE\nLife expectancy at birth (years)                         \nHuman Development Report 2020\n\n\nAGEMED\nMedian age (years)                                                \nHuman Development Report 2020\n\n\nTELMOB\nMobile phone subscriptions (per 100 people)                       \nHuman Development Report 2020\n\n\nMORINF\nMortality rate, infant (per 1,000 live births)                    \nHuman Development Report 2020\n\n\nTXMIGR\nNet migration rate (per 1,000 people)                             \nHuman Development Report 2020\n\n\nDVIEUX\nOld-age (65 and older) dependency ratio (per 100 people ages 15-64)\nHuman Development Report 2020\n\n\nTUBERC\nTuberculosis incidence (per 100,000 people)                        \nHuman Development Report 2020\n\n\nURBANI\nUrban population (%)                                              \nHuman Development Report 2020\n\n\nDJEUNE\nYoung age (0-14) dependency ratio (per 100 people ages 15-64)\nHuman Development Report 2020\n\n\nSUBREG\nUnited Nation subregions in Africa\nHuman Development Report 2020\n\n\nLOCKED\nLandlocked country (0/1)\nCEPII\n\n\nCOLFRA\nColonisation by France (0/1)\nCEPII\n\n\nCOLGBR\nColonisation by United Kingdom (0/1)\nCEPII\n\n\nLANGFR\nFrench official langage (0/1)\nCEPII\n\n\nLANGEN\nEnglish official langage(0/1)\nCEPII\n\n\n\n\n\n\n\nSélection d’un sous-tableau\nA titre d’exemple, nous allons extraitre un petit tableau utile pour le module de modélisation d’une variable Y quantitative ainsi qu’un nombre limité de variable auquel on donnera un préfixe pour mieux repérer leur rôle dans les modèles statistiques qui seront développés :\n\nW : variable de pondération\nY : variable dépendante (à expliquer) de type quantitatif continu\nX : variable indépendante (explicative) de type quantitatif continu\nQ : variable indépendante (explicative) de type qualitatif ou quantitatif discret\n\n\nsel<-don %>% select(   iso3,  \n                       nom,\n                       W_POP = POP,\n                       Y_TMI = MORINF,\n                       Y_FEC = ADOFEC,\n                       X_PIB = PIB,\n                       X_URB = URBANI,\n                       X_JEU = DJEUNE,\n                       Q_ACC = LOCKED,\n                       Q_FRA = COLFRA,\n                       Q_REG = SUBREG) %>%\n                mutate(Q_ACC = as.factor(Q_ACC),\n                       Q_FRA = as.factor(Q_FRA),\n                       Q_REG = as.factor(as.character(Q_REG))) \n levels(sel$Q_ACC)  <-c(\"Pays côtier\",\"Pays enclavé\")\n levels(sel$Q_FRA)  <-c(\"Col. Fr. Non\",\"Col. Fr. Oui\") \n\n# levels(sel$Q_REG) <- c(\"Centre\", \"Ouest\")\n sel<-sel[complete.cases(sel),]\n \nkable(sel, caption = \"Tableau de données retenu\", digits=1)\n\n\nTableau de données retenu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3\nnom\nW_POP\nY_TMI\nY_FEC\nX_PIB\nX_URB\nX_JEU\nQ_ACC\nQ_FRA\nQ_REG\n\n\n\n\n1\nAGO\nAngola\n31.3\n51.6\n152.6\n6793.7\n65.8\n91.5\nPays côtier\nCol. Fr. Non\nMiddle Africa\n\n\n2\nBDI\nBurundi\n11.4\n41.0\n56.2\n756.6\n13.2\n87.0\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n3\nBEN\nBénin\n11.6\n60.5\n87.4\n3224.0\n47.6\n77.8\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n4\nBFA\nBurkina Faso\n20.0\n49.0\n105.7\n2160.6\n29.7\n84.9\nPays enclavé\nCol. Fr. Oui\nWestern Africa\n\n\n5\nBWA\nBotswana\n2.3\n30.0\n46.3\n17700.3\n69.8\n54.9\nPays enclavé\nCol. Fr. Non\nSouthern Africa\n\n\n6\nCAF\nRep. Centrafricaine\n4.7\n84.5\n130.2\n939.0\n41.6\n83.1\nPays enclavé\nCol. Fr. Oui\nMiddle Africa\n\n\n7\nCIV\nCôte d’Ivoire\n25.4\n59.4\n118.5\n5133.6\n51.0\n75.6\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n8\nCMR\nCameroun\n25.5\n50.6\n107.7\n3628.1\n56.7\n77.6\nPays côtier\nCol. Fr. Oui\nMiddle Africa\n\n\n9\nCOD\nCongo, Rép. dém. du\n85.4\n68.2\n124.9\n1091.9\n44.8\n90.5\nPays côtier\nCol. Fr. Non\nMiddle Africa\n\n\n10\nCOG\nCongo\n5.3\n36.2\n113.0\n3356.2\n67.2\n74.8\nPays côtier\nCol. Fr. Oui\nMiddle Africa\n\n\n11\nDJI\nDjibouti\n1.0\n49.8\n19.2\n5366.7\n77.8\n44.5\nPays côtier\nCol. Fr. Oui\nEastern Africa\n\n\n12\nDZA\nAlgérie\n42.6\n20.1\n10.3\n11414.6\n72.9\n48.0\nPays côtier\nCol. Fr. Oui\nNorthern Africa\n\n\n13\nEGY\nEgypte\n99.4\n18.1\n54.1\n11564.8\n42.7\n55.5\nPays côtier\nCol. Fr. Non\nNorthern Africa\n\n\n15\nETH\nEthiopie\n110.7\n39.1\n68.1\n2161.6\n21.0\n72.5\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n16\nGAB\nGabon\n2.1\n32.7\n97.5\n14806.6\n89.6\n62.5\nPays côtier\nCol. Fr. Oui\nMiddle Africa\n\n\n17\nGHA\nGhana\n30.1\n34.9\n67.3\n5303.5\n56.4\n63.1\nPays côtier\nCol. Fr. Non\nWestern Africa\n\n\n18\nGIN\nGuinée\n12.6\n64.9\n136.6\n2531.2\n36.3\n81.7\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n19\nGMB\nGambie\n2.3\n39.0\n79.8\n2175.6\n61.6\n83.0\nPays côtier\nCol. Fr. Non\nWestern Africa\n\n\n20\nGNB\nGuinée-Bissau\n1.9\n54.0\n105.2\n1969.3\n43.6\n76.9\nPays côtier\nCol. Fr. Non\nWestern Africa\n\n\n21\nGNQ\nGuinée équatoriale\n1.3\n62.6\n157.0\n19458.9\n72.3\n61.2\nPays côtier\nCol. Fr. Non\nMiddle Africa\n\n\n22\nKEN\nKenya\n52.0\n30.6\n76.5\n4266.8\n27.2\n67.9\nPays côtier\nCol. Fr. Non\nEastern Africa\n\n\n23\nLBR\nLibéria\n4.9\n53.5\n136.3\n1462.4\n51.4\n73.4\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n24\nLBY\nLibye\n6.7\n10.2\n5.8\n15096.1\n80.2\n41.8\nPays côtier\nCol. Fr. Non\nNorthern Africa\n\n\n25\nLSO\nLesotho\n2.1\n65.7\n92.3\n2758.1\n28.4\n52.2\nPays enclavé\nCol. Fr. Non\nSouthern Africa\n\n\n26\nMAR\nMaroc\n36.3\n19.2\n31.4\n7476.2\n62.8\n41.2\nPays côtier\nCol. Fr. Oui\nNorthern Africa\n\n\n27\nMDG\nMadagascar\n26.6\n38.2\n110.9\n1629.7\n37.5\n71.8\nPays côtier\nCol. Fr. Oui\nEastern Africa\n\n\n28\nMLI\nMali\n19.4\n62.0\n170.3\n2305.2\n42.8\n94.7\nPays enclavé\nCol. Fr. Oui\nWestern Africa\n\n\n29\nMOZ\nMozambique\n29.9\n54.0\n149.9\n1285.0\n36.2\n84.6\nPays côtier\nCol. Fr. Non\nEastern Africa\n\n\n30\nMRT\nMauritanie\n4.5\n51.5\n72.0\n5119.7\n54.1\n70.3\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n31\nMWI\nMalawi\n18.4\n35.3\n134.1\n1051.1\n17.0\n81.4\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n32\nNAM\nNamibie\n2.5\n29.0\n65.5\n9784.6\n50.5\n62.1\nPays côtier\nCol. Fr. Non\nSouthern Africa\n\n\n33\nNER\nNiger\n22.9\n48.0\n188.3\n1207.8\n16.4\n105.1\nPays enclavé\nCol. Fr. Oui\nWestern Africa\n\n\n34\nNGA\nNigéria\n198.4\n75.7\n108.5\n5145.3\n50.8\n81.9\nPays côtier\nCol. Fr. Non\nWestern Africa\n\n\n35\nRWA\nRwanda\n12.5\n27.0\n39.3\n2157.4\n17.2\n69.8\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n36\nSDN\nSoudan\n42.3\n42.1\n66.0\n4059.5\n34.8\n71.9\nPays côtier\nCol. Fr. Non\nNorthern Africa\n\n\n38\nSEN\nSénégal\n16.1\n31.8\n74.1\n3354.8\n47.5\n79.6\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n39\nSLE\nSierra Leone\n7.7\n78.5\n114.5\n1690.8\n42.3\n72.8\nPays côtier\nCol. Fr. Non\nWestern Africa\n\n\n41\nSWZ\nSwaziland\n1.1\n43.0\n77.1\n8647.1\n23.9\n65.5\nPays enclavé\nCol. Fr. Non\nSouthern Africa\n\n\n42\nTCD\nTchad\n15.7\n71.4\n163.2\n1578.0\n23.2\n92.9\nPays enclavé\nCol. Fr. Oui\nMiddle Africa\n\n\n43\nTGO\nTogo\n8.0\n47.4\n89.4\n1574.2\n42.0\n73.6\nPays côtier\nCol. Fr. Oui\nWestern Africa\n\n\n44\nTUN\nTunisie\n11.6\n14.6\n7.8\n10759.7\n69.1\n35.9\nPays côtier\nCol. Fr. Oui\nNorthern Africa\n\n\n45\nTZA\nTanzanie\n57.2\n37.6\n119.0\n2625.2\n34.1\n82.3\nPays côtier\nCol. Fr. Non\nEastern Africa\n\n\n46\nUGA\nOuganda\n43.5\n33.8\n120.5\n2151.7\n24.1\n91.0\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n47\nZAF\nAfrique du Sud\n58.2\n28.5\n68.4\n12556.3\n66.7\n44.2\nPays côtier\nCol. Fr. Non\nSouthern Africa\n\n\n48\nZMB\nZambie\n17.6\n40.4\n122.2\n3500.5\n43.8\n84.0\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n49\nZWE\nZimbabwe\n14.5\n33.9\n89.0\n2983.0\n32.2\n77.2\nPays enclavé\nCol. Fr. Non\nEastern Africa\n\n\n\n\n\n\n\nParamètres principaux\nOn résume rapidement les variables retenues ce qui permet de constater qu’elles sont toutes complètes sauf la variable X1_PIB pour laquelle il y a trois valeurs manquantes.\n\nsummary(sel)\n\n     iso3               nom                W_POP              Y_TMI      \n Length:46          Length:46          Min.   :  0.9662   Min.   :10.24  \n Class :character   Class :character   1st Qu.:  4.9867   1st Qu.:32.98  \n Mode  :character   Mode  :character   Median : 15.8938   Median :41.57  \n                                       Mean   : 27.3386   Mean   :44.55  \n                                       3rd Qu.: 31.0113   3rd Qu.:54.02  \n                                       Max.   :198.4191   Max.   :84.46  \n     Y_FEC             X_PIB             X_URB           X_JEU       \n Min.   :  5.814   Min.   :  756.6   Min.   :13.20   Min.   : 35.93  \n 1st Qu.: 67.476   1st Qu.: 2014.9   1st Qu.:32.69   1st Qu.: 62.66  \n Median : 94.866   Median : 3289.4   Median :43.70   Median : 74.21  \n Mean   : 94.131   Mean   : 5168.8   Mean   :46.04   Mean   : 72.09  \n 3rd Qu.:121.756   3rd Qu.: 6437.0   3rd Qu.:60.38   3rd Qu.: 82.86  \n Max.   :188.314   Max.   :19458.9   Max.   :89.55   Max.   :105.10  \n          Q_ACC             Q_FRA                Q_REG   \n Pays côtier :31   Col. Fr. Non:26   Eastern Africa :12  \n Pays enclavé:15   Col. Fr. Oui:20   Middle Africa  : 8  \n                                     Northern Africa: 6  \n                                     Southern Africa: 5  \n                                     Western Africa :15  \n                                                         \n\n\n\n\nFonds de carte\nNous disposons d’un fonds de carte permettant d’effectuer des jointures avec les données (via la variable iso3) ce qui permettra de procéder à des analyses cartographiques des variables dépendantes ou indépendantes ainsi que des résidus des modèles. Ces résidus sont souvent intéressant pour suggérer la présence de variables latentes oubliéesdans l’analyse."
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#b.-variable-dépendant",
    "href": "MOD1_Yquanti_exo2.html#b.-variable-dépendant",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "B. Variable dépendant",
    "text": "B. Variable dépendant\nOn se propose d’expliquer la variable mortalité infantile. On va donc analyser rapidement ses caractéristiques statistiques\n\nForme de la distribution\n\nY<-sel$Y_TMI\nlabelY<-\"Tx de mort. infantile (p.1000)\"\n\nhist(Y, main=labelY)\n\n\n\n\nLa distribution est unimodale et globalement symétrique.\n\n\nValeurs exceptionnelles\n\nboxplot(Y, main=labelY, horizontal=T)\n\n\n\n\nLa distribution ne comporte pas de valeurs exceptionnelles.\n\n\nNormalité\n\nshapiro.test(Y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Y\nW = 0.98461, p-value = 0.7953\n\n\nLa distribution est gaussienne avec une probabilité très élevée."
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#c.-analyse-de-variance",
    "href": "MOD1_Yquanti_exo2.html#c.-analyse-de-variance",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "C. Analyse de variance",
    "text": "C. Analyse de variance\nOn va tester tour à tour chacune de nos trois variables indépendantes qualitatives\n\nmodèle 1 : La mortalité infantile varie-t-elle selon les régions ?\n\nQ1<-sel$Q_REG\nlabelQ1 = \"Region\"\n\nplot(Y~Q1,cex.axis=0.6, xlab=labelQ1, ylab=labelY)\n\n\n\nmod<-lm(Y~Q1)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.517  -6.656  -1.160   6.246  27.250 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        38.4057     3.7693  10.189 8.44e-13 ***\nQ1Middle Africa    18.8085     5.9598   3.156  0.00300 ** \nQ1Northern Africa -17.6609     6.5286  -2.705  0.00990 ** \nQ1Southern Africa   0.8209     6.9502   0.118  0.90656    \nQ1Western Africa   15.6041     5.0570   3.086  0.00363 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.06 on 41 degrees of freedom\nMultiple R-squared:  0.4864,    Adjusted R-squared:  0.4363 \nF-statistic: 9.708 on 4 and 41 DF,  p-value: 1.282e-05\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: Y\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nQ1         4 6620.2 1655.06  9.7076 1.282e-05 ***\nResiduals 41 6990.1  170.49                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nModèle 2 : La mortalité infantile varie-t-elle selon l’accès à la mer\n\nQ2<-sel$Q_ACC\nlabelQ2 = \"Accès à la mer\"\n\nplot(Y~Q2,cex.axis=0.6, xlab=labelQ2, ylab=labelY)\n\n\n\nmod<-lm(Y~Q2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33.15 -12.48  -4.14  10.62  37.52 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      43.393      3.144  13.802   <2e-16 ***\nQ2Pays enclavé    3.550      5.506   0.645    0.522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.51 on 44 degrees of freedom\nMultiple R-squared:  0.009362,  Adjusted R-squared:  -0.01315 \nF-statistic: 0.4158 on 1 and 44 DF,  p-value: 0.5224\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: Y\n          Df  Sum Sq Mean Sq F value Pr(>F)\nQ2         1   127.4  127.42  0.4158 0.5224\nResiduals 44 13483.0  306.43               \n\n\n\n\nModèle 3 : La mortalité infantile varie-t-elle selon l’héritage colonial\n\nQ3<-sel$Q_FRA\nlabelQ3 = \"Héritage colonial\"\n\nplot(Y~Q3,cex.axis=0.6, xlab=labelQ3, ylab=labelY)\n\n\n\nmod<-lm(Y~Q3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.661 -11.649  -0.894  11.557  37.197 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      42.461      3.416  12.430 5.42e-16 ***\nQ3Col. Fr. Oui    4.806      5.181   0.928    0.359    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.42 on 44 degrees of freedom\nMultiple R-squared:  0.01918,   Adjusted R-squared:  -0.003111 \nF-statistic: 0.8604 on 1 and 44 DF,  p-value: 0.3587\n\n\n\n\nAnalyse de variance à plusieurs facteurs : Y = f(Q1,Q2,Q3)\n\nmod<-lm(Y~Q1+Q2+Q3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q1 + Q2 + Q3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.1534  -7.2555  -0.6981   6.8263  24.3054 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        35.7507     4.6881   7.626 3.01e-09 ***\nQ1Middle Africa    21.8452     6.5632   3.328  0.00191 ** \nQ1Northern Africa -13.6335     7.2992  -1.868  0.06932 .  \nQ1Southern Africa   0.2744     7.0270   0.039  0.96905    \nQ1Western Africa   19.0220     5.8946   3.227  0.00254 ** \nQ2Pays enclavé      5.3356     4.6465   1.148  0.25784    \nQ3Col. Fr. Oui     -2.7451     4.5460  -0.604  0.54944    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.12 on 39 degrees of freedom\nMultiple R-squared:  0.5066,    Adjusted R-squared:  0.4306 \nF-statistic: 6.673 on 6 and 39 DF,  p-value: 6.481e-05\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n             Sum Sq Df F value    Pr(>F)    \n(Intercept) 10014.1  1 58.1531 3.010e-09 ***\nQ1           6445.0  4  9.3567 2.117e-05 ***\nQ2            227.1  1  1.3186    0.2578    \nQ3             62.8  1  0.3646    0.5494    \nResiduals    6715.9 39                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn retient les deux variables qui avaient des effets isolés significatifs\n\nY<-sel$Y_TMI\nQ1<-sel$Q_REG\nQ2<-sel$Q_ACC\nQ3<-sel$Q_FRA\n\n\nmod<-lm(Y~Q1+Q2+Q3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q1 + Q2 + Q3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.1534  -7.2555  -0.6981   6.8263  24.3054 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        35.7507     4.6881   7.626 3.01e-09 ***\nQ1Middle Africa    21.8452     6.5632   3.328  0.00191 ** \nQ1Northern Africa -13.6335     7.2992  -1.868  0.06932 .  \nQ1Southern Africa   0.2744     7.0270   0.039  0.96905    \nQ1Western Africa   19.0220     5.8946   3.227  0.00254 ** \nQ2Pays enclavé      5.3356     4.6465   1.148  0.25784    \nQ3Col. Fr. Oui     -2.7451     4.5460  -0.604  0.54944    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.12 on 39 degrees of freedom\nMultiple R-squared:  0.5066,    Adjusted R-squared:  0.4306 \nF-statistic: 6.673 on 6 and 39 DF,  p-value: 6.481e-05\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n             Sum Sq Df F value    Pr(>F)    \n(Intercept) 10014.1  1 58.1531 3.010e-09 ***\nQ1           6445.0  4  9.3567 2.117e-05 ***\nQ2            227.1  1  1.3186    0.2578    \nQ3             62.8  1  0.3646    0.5494    \nResiduals    6715.9 39                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#d.-régression-linéaire",
    "href": "MOD1_Yquanti_exo2.html#d.-régression-linéaire",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "D. Régression linéaire",
    "text": "D. Régression linéaire\nOn reprend la même variable dépendante (Y = fécondité des femmes de 15-19 ans) et on la confronte cette fois-ci à trois variables indépendantes de type quantitatif continu.\n\nModèle 1 : La richesse par habitant (log) fait-elle diminuer la mortalité infantile ?\n\nX1<-log10(sel$X_PIB)\nlabelX1 <-\"log10(PIB/hab)\"\nN<-sel$iso3\n\nmod<-lm(Y~X1)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.410 -12.346  -2.527  11.737  34.657 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  123.762     21.985   5.629 1.18e-06 ***\nX1           -22.295      6.155  -3.622 0.000752 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.44 on 44 degrees of freedom\nMultiple R-squared:  0.2297,    Adjusted R-squared:  0.2122 \nF-statistic: 13.12 on 1 and 44 DF,  p-value: 0.0007516\n\nplot(X1,Y, cex=1, col=\"black\",pch=20, xlab=labelX1,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X1),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\ntext(X1,Y,N,cex=0.6,pos = 1, col=\"blue\")\n\n\n\n\n\n\nModèle 2 : L’urbanisation fait-elle diminuer la mortalité infantile ?\n\nY<-sel$Y_TMI\nX2<-sel$X_URB\nlabelX2<-\"Taux d'urbanisation (%)\"\nN<-sel$iso3\n\n\n\nmod<-lm(Y~X2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.348 -12.294  -3.534  11.561  39.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  53.9180     6.6916   8.058 3.33e-10 ***\nX2           -0.2035     0.1346  -1.512    0.138    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.15 on 44 degrees of freedom\nMultiple R-squared:  0.04939,   Adjusted R-squared:  0.02778 \nF-statistic: 2.286 on 1 and 44 DF,  p-value: 0.1377\n\nplot(X2,Y, cex=1, col=\"black\",pch=20, xlab=labelX2,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X2),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\ntext(X2,Y,N,cex=0.6,pos = 1, col=\"blue\")\n\n\n\n\n\n\nModèle 3 : La mortalité infantile est-elle plus forte au début de la transition démographique ?\nLe taux de dépendance des jeunes (Jeunes/Adulte) est un bon proxi de l’avancement dans la transition démographique. Très fort au début de celle-ci, il diminue ensuite réguièrement.\n\nY<-sel$Y_TMI\nX3<-sel$X_JEU\nlabelX3<-\"Jeunes/adultes (%)\"\nN<-sel$iso3\n\n\n\nmod<-lm(Y~X3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.830 -11.397  -4.335   8.137  33.492 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3106    10.3009   0.224 0.823552    \nX3            0.5860     0.1396   4.197 0.000129 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.86 on 44 degrees of freedom\nMultiple R-squared:  0.2859,    Adjusted R-squared:  0.2696 \nF-statistic: 17.61 on 1 and 44 DF,  p-value: 0.0001294\n\nplot(X3,Y, cex=1, col=\"black\",pch=20, xlab=labelX3,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X3),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\ntext(X3,Y,N,cex=0.6,pos = 1, col=\"blue\")\n\n\n\n\n\n\nRégression linéaire multiple : Y = f(X1,X2,X3, …)\n\nmod<-lm(Y~X1+X2+X3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.489 -10.752  -2.656   7.905  33.218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  59.8527    41.7027   1.435   0.1586  \nX1          -17.2938     9.6954  -1.784   0.0817 .\nX2            0.2614     0.1624   1.610   0.1149  \nX3            0.4731     0.1974   2.396   0.0211 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.56 on 42 degrees of freedom\nMultiple R-squared:  0.3455,    Adjusted R-squared:  0.2988 \nF-statistic: 7.391 on 3 and 42 DF,  p-value: 0.0004385\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n            Sum Sq Df F value Pr(>F)  \n(Intercept)  436.9  1  2.0599 0.1586  \nX1           674.8  1  3.1816 0.0817 .\nX2           549.6  1  2.5913 0.1149  \nX3          1217.8  1  5.7419 0.0211 *\nResiduals   8907.8 42                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#modeles-hybrides",
    "href": "MOD1_Yquanti_exo2.html#modeles-hybrides",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "Modeles hybrides",
    "text": "Modeles hybrides\nOn aurait enfin pu tenter de combiner les variables continues et discrètes dans un même modèle.\n\nEnsemble des variables\n\nmod<-lm(Y~X1+X2+X3+Q1+Q2+Q3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + Q1 + Q2 + Q3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.135  -6.914  -1.917   8.809  25.596 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       110.8534    38.4525   2.883  0.00661 ** \nX1                -17.4380     9.4626  -1.843  0.07360 .  \nX2                 -0.1194     0.2052  -0.582  0.56416    \nX3                 -0.1298     0.2458  -0.528  0.60076    \nQ1Middle Africa    27.8628     7.7229   3.608  0.00093 ***\nQ1Northern Africa  -5.7272     8.0850  -0.708  0.48327    \nQ1Southern Africa  10.1633     8.4388   1.204  0.23631    \nQ1Western Africa   20.6276     6.2928   3.278  0.00232 ** \nQ2Pays enclavé     -0.1326     5.4793  -0.024  0.98082    \nQ3Col. Fr. Oui     -3.1760     4.5795  -0.694  0.49242    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.37 on 36 degrees of freedom\nMultiple R-squared:  0.595, Adjusted R-squared:  0.4937 \nF-statistic: 5.876 on 9 and 36 DF,  p-value: 5.08e-05\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n            Sum Sq Df F value   Pr(>F)   \n(Intercept) 1272.6  1  8.3109 0.006609 **\nX1           520.0  1  3.3961 0.073598 . \nX2            51.9  1  0.3388 0.564160   \nX3            42.7  1  0.2787 0.600763   \nQ1          3375.8  4  5.5116 0.001444 **\nQ2             0.1  1  0.0006 0.980825   \nQ3            73.7  1  0.4810 0.492420   \nResiduals   5512.4 36                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nvif(mod)\n\n       GVIF Df GVIF^(1/(2*Df))\nX1 3.678123  1        1.917843\nX2 4.464910  1        2.113033\nX3 4.471582  1        2.114612\nQ1 7.306947  4        1.282233\nQ2 1.982002  1        1.407836\nQ3 1.548242  1        1.244284\n\n\n\n\nVariables les plus pertinentes\n\nY<-sel$Y_TMI\n\nmod<-lm(Y~X1+Q1)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X1 + Q1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.931  -6.845  -1.999   7.007  27.227 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        102.017     21.046   4.847 1.92e-05 ***\nX1                 -19.073      6.226  -3.064 0.003904 ** \nQ1Middle Africa     23.352      5.629   4.148 0.000170 ***\nQ1Northern Africa   -5.571      7.139  -0.780 0.439706    \nQ1Southern Africa   12.416      7.378   1.683 0.100183    \nQ1Western Africa    17.217      4.638   3.712 0.000625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.9 on 40 degrees of freedom\nMultiple R-squared:  0.584, Adjusted R-squared:  0.532 \nF-statistic: 11.23 on 5 and 40 DF,  p-value: 8.325e-07\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n            Sum Sq Df F value    Pr(>F)    \n(Intercept) 3325.8  1 23.4967 1.923e-05 ***\nX1          1328.4  1  9.3854  0.003904 ** \nQ1          4822.1  4  8.5171 4.541e-05 ***\nResiduals   5661.7 40                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInteractions\n\nmod<-lm(Y~X1:Q1)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X1:Q1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.396  -7.070  -1.586   7.579  26.540 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           113.952     22.287   5.113 8.25e-06 ***\nX1:Q1Eastern Africa   -22.549      6.728  -3.352 0.001764 ** \nX1:Q1Middle Africa    -15.987      6.244  -2.560 0.014343 *  \nX1:Q1Northern Africa  -23.544      5.736  -4.104 0.000194 ***\nX1:Q1Southern Africa  -19.095      5.786  -3.301 0.002036 ** \nX1:Q1Western Africa   -17.465      6.555  -2.664 0.011063 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.95 on 40 degrees of freedom\nMultiple R-squared:  0.5802,    Adjusted R-squared:  0.5278 \nF-statistic: 11.06 on 5 and 40 DF,  p-value: 9.891e-07\n\nAnova(mod, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Y\n            Sum Sq Df F value    Pr(>F)    \n(Intercept) 3734.0  1  26.143 8.254e-06 ***\nX1:Q1       7897.1  5  11.058 9.891e-07 ***\nResiduals   5713.3 40                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MOD1_Yquanti_exo2.html#exercices",
    "href": "MOD1_Yquanti_exo2.html#exercices",
    "title": "Exo 2 : Mortalité infantile en Afrique",
    "section": "EXERCICES",
    "text": "EXERCICES\nRefaire l’analyse avec la variable Y = Fécondité des 15-19 ans"
  },
  {
    "objectID": "MOD1_Yquanti_exo3.html",
    "href": "MOD1_Yquanti_exo3.html",
    "title": "Exo 3 : Revenus des ménages du Bénin",
    "section": "",
    "text": "L’objectif de la présente est d’analyser un tableau de données d’enquête permettant de décrire les revenus des ménages et un certain nombre d’indicateurs de confort en fonction d’attributs du chef de ménage ou du ménage proprement dit.\nNadège : (…)"
  },
  {
    "objectID": "MOD1_Yquanti_exo3.html#a.-données",
    "href": "MOD1_Yquanti_exo3.html#a.-données",
    "title": "Exo 3 : Revenus des ménages du Bénin",
    "section": "A. Données",
    "text": "A. Données\nNadège : (…)\n\nImportation\nOn importe les données depuis Excel\n\ndon <- read_excel(\"FIN-BENIN-2018/data/select.xls\",sheet = \"data\")\nkable(head(don), caption = \"Premières lignes du tableau\")\n\n\nPremières lignes du tableau\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhou_wgt\nhou_mbr\nhou_roo\nhou_inc\nhou_agr\nhea\nhea_age\nhea_sex\nloc_dep\nloc_urb\nloc_rur\n\n\n\n\n117.5442\n8\n1\n20000\nOnly\nTRUE\n65\nFemale\nALIBORI\nFALSE\nTRUE\n\n\n116.5713\n4\n1\n20000\nOnly\nTRUE\n40\nMale\nALIBORI\nFALSE\nTRUE\n\n\n116.5713\n9\n3\n125000\nOnly\nTRUE\n35\nMale\nALIBORI\nFALSE\nTRUE\n\n\n207.5032\n7\n3\n20000\nOnly\nTRUE\n28\nMale\nALIBORI\nFALSE\nTRUE\n\n\n207.5032\n7\n4\n20000\nOnly\nTRUE\n20\nMale\nALIBORI\nFALSE\nTRUE\n\n\n566.2480\n7\n5\n20000\nOnly\nTRUE\n25\nMale\nALIBORI\nTRUE\nFALSE\n\n\n\n\n\n\n\nListe des variables\n\nmeta <- read_excel(\"FIN-BENIN-2018/data/select.xls\",sheet = \"meta\")\nkable(meta,caption = \"Liste des variables\")\n\n\nListe des variables\n\n\nCode\nDef\n\n\n\n\nhou_wgt\npoids statistique du ménage\n\n\nhou_mbr\nnombre de membres du ménage\n\n\nhou_roo\nnombre de pièces servant de chambre à coucher\n\n\nhou_inc\nrevenu global du ménage en CFA\n\n\nhou_agr\nrevenu d’activité agricole (exclusive, partielle, absente)\n\n\nhea\nle répondant est chef du ménage (TRUE/FALSE)\n\n\nhea_age\nâge du chef de ménage\n\n\nhea_sex\nsexe du chef de ménage\n\n\nloc_dep\ndépartement de résidence du ménage\n\n\nloc_urb\nmilieu urbain (TRUE/FALSE)\n\n\nloc_rur\nmilieu rural (TRUE/FALSE)\n\n\n\n\n\nN.B.1: Nous avons éliminé du tableau initial les individus dont le revenu par personne se situait dans les 5% les plus élevés et les 5% les plus faibles, ainsi que les données incomplètes. Il ne reste donc plus que 4685 observations contre un peu plus de 6000 dans le tableau initial. La pondération n’est donc plus correcte pour effectuer des redressements et elle ne sera pas utilisée.\nN.B.2: Le revenu total du ménage a été obtenu soit de façon absolue (la personne donne un chiffre unique), soit par tranche selon une grille de réponse. Dans ce dernier cas, nous avons pris comme valeur le centre de la tranche. Pour les tranches extrêmes nous avons alloué la valeur 20000 pour la modalité “moins de 40000” et la valeur 1000000 popur la modalité “plus de 750000”. Ceci entraîne donc une concentration artificielle des valeurs autour de 20000.\nNadège (…)\n\n\nAjout de nouvelles variables\nNous construisons à partir du tableau deux nouvelles variables :\n\nhou_inc_cap : revenu moyen par habitant d’un ménage\nhou_roo_occ : nombre d’individu par pièces de couchage\n\n\ndon$hou_inc_cap <- don$hou_inc/don$hou_mbr\ndon$hou_roo_occ <- don$hou_mbr/don$hou_roo\n\n\n\nSélection\nOn n’opère aucune sélection dans l’immédiat\n\nsel<-don\n\nnadège ? (possibilité de réduire l’échantillon ?)\n\n\nParamètres principaux\nOn résume rapidement les variables retenues\n\nsummary(sel)\n\n    hou_wgt           hou_mbr          hou_roo          hou_inc       \n Min.   :  29.49   Min.   : 1.000   Min.   : 1.000   Min.   :   2000  \n 1st Qu.: 187.91   1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.:  20000  \n Median : 259.77   Median : 5.000   Median : 2.000   Median :  20000  \n Mean   : 296.17   Mean   : 5.612   Mean   : 2.555   Mean   :  45208  \n 3rd Qu.: 373.05   3rd Qu.: 7.000   3rd Qu.: 3.000   3rd Qu.:  75000  \n Max.   :1374.73   Max.   :45.000   Max.   :22.000   Max.   :1000000  \n   hou_agr              hea               hea_age        hea_sex         \n Length:4865        Length:4865        Min.   :15.00   Length:4865       \n Class :character   Class :character   1st Qu.:28.00   Class :character  \n Mode  :character   Mode  :character   Median :37.00   Mode  :character  \n                                       Mean   :39.92                     \n                                       3rd Qu.:50.00                     \n                                       Max.   :95.00                     \n   loc_dep            loc_urb            loc_rur           hou_inc_cap   \n Length:4865        Length:4865        Length:4865        Min.   : 1818  \n Class :character   Class :character   Class :character   1st Qu.: 4000  \n Mode  :character   Mode  :character   Mode  :character   Median : 6667  \n                                                          Mean   : 9333  \n                                                          3rd Qu.:12500  \n                                                          Max.   :30769  \n  hou_roo_occ    \n Min.   : 0.250  \n 1st Qu.: 1.500  \n Median : 2.000  \n Mean   : 2.444  \n 3rd Qu.: 3.000  \n Max.   :14.000  \n\n\n\n\nFonds de carte\nNous disposons d’un fonds de carte (source : IPUMS) permettant de cartographier éventuellement les résultats de certaines analyses par département.\n\nmap<-st_read(\"FIN-BENIN-2018/shp/geo1_bj2013.shp\", quiet=T)\n\nIl permet d’ores et déjà de repérer la position des départements\n\nmf_map(map, type=\"typo\",var=\"ADMIN_NAME\")\nmf_label(map,var = \"DEPT2013\" )\nmf_layout(title = \"Les 12 départements du Bénin\",credits = \"IPUMS\",arrow = F,frame =T)"
  },
  {
    "objectID": "MOD1_Yquanti_exo3.html#modélisation-du-revenu-total-des-ménages",
    "href": "MOD1_Yquanti_exo3.html#modélisation-du-revenu-total-des-ménages",
    "title": "Exo 3 : Revenus des ménages du Bénin",
    "section": "Modélisation du revenu total des ménages",
    "text": "Modélisation du revenu total des ménages\nOn se propose d’expliquer la variable hou_inc qui mesure le revenu total des ménages en fonction du nombre de membres du ménage.\n\nForme de la distribution\n\nY<-sel$hou_inc\nlabelY<-\"Revenu total \"\n\nhist(Y, main=labelY,breaks = 12)\n\n\n\nhist(log10(Y), main=labelY, xlab = \"logarithme décimal\", breaks=12)\n\n\n\n\nLa distribution comporte une forte anomalie statistique liée au fait que nous avons attribué la valeur 20000 CFA à tous les ménages gagnant “moins de 40000 CFA”.\n\n\nValeurs exceptionnelles\n\nboxplot(Y, main=labelY, horizontal=T)\n\n\n\nboxplot(log(Y), main=labelY, horizontal=T, xlab = \"Logarithme décimal\")\n\n\n\n\nLa distribution comporte des valeurs exceptionnelles mais celles-ci disparaissent pour la plupart après transformation logarithmique.\n\n\nNormalité\n\nshapiro.test(Y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Y\nW = 0.65732, p-value < 2.2e-16\n\nshapiro.test(log(Y))\n\n\n    Shapiro-Wilk normality test\n\ndata:  log(Y)\nW = 0.88535, p-value < 2.2e-16\n\n\nLa distribution n’est pas gaussienne et elle ne le devient pas non plus après transformation logarithmique en raison de la cocnentration des valeurs autour de 20000. On décide de poursuivre l’analyse en utilisant désormais le log. du revenu.\n\n\nModèle 1 : Ajustement linéaire\nOn commence par un modèle linéaire de type Y = aX+b\n\nY<-sel$hou_inc\nlabelY <- \"revenu total\"\nX2<-sel$hou_mbr\nlabelX2 <-\"nombre de membres du ménage\"\n\nmod<-lm(Y~X2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-114018  -21744   -8073   14585  731681 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  13422.2     1030.5   13.03   <2e-16 ***\nX2            5664.4      156.8   36.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37390 on 4863 degrees of freedom\nMultiple R-squared:  0.2115,    Adjusted R-squared:  0.2113 \nF-statistic:  1304 on 1 and 4863 DF,  p-value: < 2.2e-16\n\nplot(X2,Y, cex=1, col=\"black\",pch=20, xlab=labelX2,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X2),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\n\n\n\n\nLa relation est assez forte (r2= 21.1%) mais très significative (p < 0.001). Toutefois elle est visiblement influencée par une valeur exceptionnelle correspondant à un ménage de plus de 45 personnes déclarant un revenu d’un million de CFA. On élimine ce cas exceptionnel (outlier) avant de refaire le calcul :\n\n\nModèle 1-bis : Ajustement linéaire sans outlier\n\nsel<-sel %>% filter(hou_inc < 500000)\n\nY<-sel$hou_inc\nlabelY <- \"revenu total\"\nX2<-sel$hou_mbr\nlabelX2 <-\"nombre de membres du ménage\"\n\nmod<-lm(Y~X2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-100621  -21908   -7194   13377  316949 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16194.4      997.1   16.24   <2e-16 ***\nX2            5142.8      152.5   33.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35840 on 4862 degrees of freedom\nMultiple R-squared:  0.1896,    Adjusted R-squared:  0.1894 \nF-statistic:  1138 on 1 and 4862 DF,  p-value: < 2.2e-16\n\nplot(X2,Y, cex=1, col=\"black\",pch=20, xlab=labelX2,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X2),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\n\n\n\n\nLa relation est un peu moins forte (r2 =19%) mais toujours très significative (p < 0.001). Toutefois elle est incorrecte sur le plan statistique car elle comporte visiblement une forte hétéroscédasticité (i.e. variance des résidus non homogène). Il faut donc procéder à des transformations logarithmiques sur X et/ou Y.\n\n\nModèle 2 : Ajustement exponentiel\nOn passe à un modèle exponentiel de type log(Y) = aX+b\n\nY<-log(sel$hou_inc)\nlabelY <- \"revenu total (log)\"\nX2<-sel$hou_mbr\nlabelX2 <-\"nombre de membres du ménage\"\n\nmod<-lm(Y~X2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3847 -0.4673 -0.1379  0.5362  1.7580 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.88931    0.01824  542.12   <2e-16 ***\nX2           0.09630    0.00279   34.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6558 on 4862 degrees of freedom\nMultiple R-squared:  0.1969,    Adjusted R-squared:  0.1967 \nF-statistic:  1192 on 1 and 4862 DF,  p-value: < 2.2e-16\n\nplot(X2,Y, cex=1, col=\"black\",pch=20, xlab=labelX2,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X2),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\n\n\n\n\nCe nouveau modèle demeure significatif (p<0.001), explique un peu plus de variance (r2 = 20%) et élimine l’effet de l’hétéroscédasticité. Mais on voit que le nuage de point affiche désormais une courbure ce qui crée une autocorrélation des résidus indiquant que le modèle n’est toujours pas correct d’un point de vue statistique.\n\n\nModèle 3 : Ajustement puissance\nOn passe à un modèle exponentiel de type log(Y) = a.log(X)+b\n\nY<-log(sel$hou_inc)\nlabelY <- \"revenu total (log)\"\nX2<-log(sel$hou_mbr)\nlabelX2 <-\"nombre de membres du ménage (log)\"\n\nmod<-lm(Y~X2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.02346 -0.56251 -0.08335  0.51347  1.93356 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.62437    0.02391  402.48   <2e-16 ***\nX2           0.52293    0.01432   36.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6483 on 4862 degrees of freedom\nMultiple R-squared:  0.2152,    Adjusted R-squared:  0.2151 \nF-statistic:  1333 on 1 and 4862 DF,  p-value: < 2.2e-16\n\nplot(X2,Y, cex=1, col=\"black\",pch=20, xlab=labelX2,ylab=labelY)\nabline(mod,col=\"red\")\nabline(v=mean(X2),lty=2, lwd=2)\nabline(h=mean(Y),lty=2)\n\n\n\nexp(9.624)\n\n[1] 15123.42\n\n\nLe pouvoir explicatif du modèle est toujours très significatif un peu plus élevé que précédemment (r2 = 21.5%) et la plupart des défauts statistiques ont été corrigés même s’ils demeurent des imperfections liées au fait qu’on a utilisé des centres de tranches de revenus comportant une certaine imprécision. L’équation du modèle s’écrit donc sous la forme\n\\(log(Revenu) = 0.523.log(NBmembres) + 9.624\\)\nCe que l’on peut transformer en :\n\\(Revenu = exp[0.523.log(NBmembres) + 9.624]\\)\nd’où\n\\(Revenu = exp(9.624) . (NBmembres)^{0.523}\\)\net finalement\n\\(Revenu = 15123 . (NBmembres)^{0.523}\\)\nCette équation signifie que le revenu total est en théorie de 15123 CFA pour un ménage d’une personne et qu’il augmente ensuite de façon à peu près proportionnelle à la racine carrée du nombre de membres du ménage (en effet \\(X^{0.5} = \\sqrt{X}\\)). On peut construire un tableau montrant le revenu attendu des ménages en fonction du nombre de personnes :\n\nnb<-c(1:20)\ninc<-15123*nb**0.523\ndf<-data.frame(nb,inc)\nkable(df, \n      caption = \"Revenu des ménages en fonction de leur taille\",\n      digits = 0,\n      col.names = c(\"Taille du ménage\",\"Revenu attendu\")\n      )\n\n\nRevenu des ménages en fonction de leur taille\n\n\nTaille du ménage\nRevenu attendu\n\n\n\n\n1\n15123\n\n\n2\n21731\n\n\n3\n26864\n\n\n4\n31226\n\n\n5\n35091\n\n\n6\n38602\n\n\n7\n41843\n\n\n8\n44870\n\n\n9\n47721\n\n\n10\n50424\n\n\n11\n53001\n\n\n12\n55469\n\n\n13\n57840\n\n\n14\n60126\n\n\n15\n62335\n\n\n16\n64475\n\n\n17\n66552\n\n\n18\n68572\n\n\n19\n70538\n\n\n20\n72456\n\n\n\n\n\nOu bien sous forme graphique :\n\nnb<-c(1:40)\ninc<-15123*nb**0.523 / 1000\nplot(nb,inc,\n     type=\"l\",\n     xlab = \"Taille du ménage (nb. membres)\",\n     ylab = \"Revenu du ménage (1000 CFA)\",\n     main = \"Modélisation du revenu des ménages en fonction de leur taille\",\n     col=\"red\")\ngrid()\n\n\n\n\n\n\nDiscussion\nCompte-tenu du caractère non-linéaire de la relation entre revenu et taille du ménage, il serait peu judicieux de calculer un indicateur tel que le revenu moyen par personne. Il semble plus judicieux de prendre comme indicateur de richesse relative du ménage le rapport entre la valeur observée et la valeur théorique selon le modèle défini ci-dessus.\n\nsel$hou_rich <- sel$hou_inc/(15123*(sel$hou_mbr**0.523))"
  },
  {
    "objectID": "MOD1_Yquanti_exo3.html#c.-analyse-des-déterminants-de-la-richesse-relative-du-ménage",
    "href": "MOD1_Yquanti_exo3.html#c.-analyse-des-déterminants-de-la-richesse-relative-du-ménage",
    "title": "Exo 3 : Revenus des ménages du Bénin",
    "section": "C. Analyse des déterminants de la richesse relative du ménage",
    "text": "C. Analyse des déterminants de la richesse relative du ménage\nOn se propose d’expliquer la variable hou_rich qui a été créée à l’étape précédente et qui mesure sa richesse relative, toutes choses égales quant à sa taille.\n\nForme de la distribution\n\nY<-sel$hou_rich\nlabelY<-\"Richesse relative \"\n\nhist(Y, main=labelY,breaks = 12)\n\n\n\nhist(log10(Y), main=labelY, xlab = \"logarithme décimal\", breaks=12)\n\n\n\n\nLa distribution est unimodale mais asymétrique à gauche. Après transformation logarithmique elle apparaît plus symétrique.\n\n\nValeurs exceptionnelles\n\nboxplot(Y, main=labelY, horizontal=T)\n\n\n\nboxplot(log(Y), main=labelY, horizontal=T, xlab = \"Logarithme décimal\")\n\n\n\n\nLa distribution comporte des valeurs exceptionnelles mais celles-ci disparaissent pratiquement après transformation logarithmique.\n\n\nNormalité\n\nshapiro.test(Y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Y\nW = 0.84285, p-value < 2.2e-16\n\nshapiro.test(log(Y))\n\n\n    Shapiro-Wilk normality test\n\ndata:  log(Y)\nW = 0.97933, p-value < 2.2e-16\n\n\n\nCommentaire : La distribution n’est pas gaussienne et elle ne le devient pas non plus après transformation logarithmique en raison de la cocnentration des valeurs autour de 20000 (Cf. introduction). On décide de poursuivre l’analyse en utilisant désormais le log. du revenu\n\n\nY<-log(sel$hou_rich)\nlabelY<-\"Richesse relative (log)\"\n\nOn va tester l’effet de variables indépendantes qualitatives\n\n\nModèle 1 : Les ménages urbains sont-ils plus riches ?\n\nQ1<-as.factor(sel$loc_urb)\nlevels(Q1)<-c(\"Rural\",\"Urbain\")\nlabelQ1 = \"Milieu urbain ou rural\"\n\nplot(Y~Q1,cex.axis=0.6, xlab=labelQ1, ylab=labelY)\n\n\n\nmod<-lm(Y~Q1)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1191 -0.5415 -0.0233  0.4751  2.0118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.07806    0.01242  -6.284 3.58e-10 ***\nQ1Urbain     0.17410    0.01852   9.403  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6424 on 4862 degrees of freedom\nMultiple R-squared:  0.01786,   Adjusted R-squared:  0.01766 \nF-statistic: 88.41 on 1 and 4862 DF,  p-value: < 2.2e-16\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: Y\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nQ1           1   36.49  36.489  88.407 < 2.2e-16 ***\nResiduals 4862 2006.72   0.413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCommentaire : les ménages urbains sont légèrement plus riches que les ménages ruraux. La relation est significative mais son pouvoir explicatif est faible. La différence relative est égale à 0.174 en log soit un rapport de \\(exp(0.174) = 1.19\\) qui implique une richesse supérieure d’environ 19% pour les ménages urbains.\n\n\n\nModèle 2 : Les ménages dont le chef est un homme sont-ils plus riches ?\n\nQ2<-as.factor(sel$hea_sex)\nlevels(Q2)\n\n[1] \"Female\" \"Male\"  \n\nlevels(Q2)<-c(\"Femme\",\"Homme\")\nlabelQ2 = \"Sexe du chef de ménage\"\n\nplot(Y~Q2,cex.axis=0.6, xlab=labelQ2, ylab=labelY)\n\n\n\nmod<-lm(Y~Q2)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.07604 -0.49849 -0.00969  0.46290  1.93225 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.07331    0.01432  -5.118 3.21e-07 ***\nQ2Homme      0.12629    0.01876   6.730 1.89e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6453 on 4862 degrees of freedom\nMultiple R-squared:  0.009231,  Adjusted R-squared:  0.009027 \nF-statistic:  45.3 on 1 and 4862 DF,  p-value: 1.888e-11\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: Y\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nQ2           1   18.86 18.8602  45.298 1.888e-11 ***\nResiduals 4862 2024.35  0.4164                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCommentaire : les ménages dont le chef est un homme sont légèrement plus riches que les ménages dont le chef est une femme . La relation est significative mais son pouvoir explicatif est faible. La différence relative est égale à 0.126 en log soit un rapport de \\(exp(0.126) = 1.13\\) qui implique une richesse supérieure d’environ 13% pour les ménages dont le chef est un homme.\n\n\n\nModèle 3 : La richesse des ménages varie-t-elle selon les départements\n\nQ3<-as.factor(substr(sel$loc_dep,1,3))\nlabelQ3 <- \"Département\"\n\nplot(Y~Q3,cex.axis=0.6, xlab=labelQ3, ylab=labelY)\n\n\n\nmod<-lm(Y~Q3)\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ Q3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.02674 -0.50092 -0.04339  0.46192  1.94100 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.009793   0.034441  -0.284  0.77617    \nQ3ATA       -0.006997   0.046921  -0.149  0.88147    \nQ3ATL       -0.005519   0.049791  -0.111  0.91175    \nQ3BOR        0.151092   0.046324   3.262  0.00112 ** \nQ3COL        0.176282   0.044632   3.950 7.94e-05 ***\nQ3COU       -0.254332   0.045433  -5.598 2.29e-08 ***\nQ3DON        0.016994   0.047645   0.357  0.72135    \nQ3LIT        0.208135   0.046921   4.436 9.38e-06 ***\nQ3MON       -0.146867   0.045895  -3.200  0.00138 ** \nQ3OUE        0.013460   0.045086   0.299  0.76531    \nQ3PLA        0.087189   0.045872   1.901  0.05740 .  \nQ3ZOU       -0.155016   0.049229  -3.149  0.00165 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6341 on 4852 degrees of freedom\nMultiple R-squared:  0.0451,    Adjusted R-squared:  0.04293 \nF-statistic: 20.83 on 11 and 4852 DF,  p-value: < 2.2e-16\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: Y\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nQ3          11   92.15  8.3768  20.832 < 2.2e-16 ***\nResiduals 4852 1951.06  0.4021                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MOD1_Yquanti_intro.html",
    "href": "MOD1_Yquanti_intro.html",
    "title": "Introduction : Les précipitations en Californie",
    "section": "",
    "text": "L’objectif de ce chapitre introductif est de présenter de façon non technique les objectifs du module en s’appuyant sur un célèbre exemple pédagogique mis au point par le géographe P.J. Taylor en 1980. Bien qu’il ne concerne pas directement les pays africains ou la France, cet exemple a été retenu pour l’EE CIST 2023 car l’expérience montre qu’il est particulièrement efficace pour l’apprentissage des mécanismes logiques de construction d’un modèle de régression multiple. La publication originale étant difficile à trouver, nous avons créé un lien sur le site de l’EE pour son téléchargement mais nous en rappelons ici la référence complète :\n\nP. J. Taylor, 1980, A Pedagogic Application of Multiple Regression Analysis: Precipitation in California, Geography ,Vol. 65, No. 3, pp. 203-212 Published by: Geographical Association, https://www.jstor.org/stable/40569273\n\nToute en reprenant l’essentiel de la démonstration de l’auteur, nous allons toutefois adapter sa démarche sur plusieurs points :\n\ntraduction en français (!)\ndéveloppement plus important de la partie univariée\nreprise de la cartographie initiale de l’article\nréalisation de tous les calculs avec R"
  },
  {
    "objectID": "MOD1_Yquanti_intro.html#a.-introduction",
    "href": "MOD1_Yquanti_intro.html#a.-introduction",
    "title": "Introduction : Les précipitations en Californie",
    "section": "A. Introduction",
    "text": "A. Introduction\n\nObjectifs de la régression multiple\nLa régression multiple est une méthode permettant de décrire comment les variations d’une phénomène sont reliés aux variations d’un ou plusieur autres phénomènes. On suppose en général que le premier phénomène est la variable à expliquer (qui sera notée \\(Y\\)) et que les autres phénomènes sont des variables explicatives (qui seront notées \\(X_1...X_n\\)). On suppose donc que les variations de ces variables explicatives “produisent”, “déterminent” ou même “causent” les variations de la variable à expliquer.\n\n\nCorrélation et causalité\nDans l’exemple qui va suivre, les relations de causalité sont assez faciles à démontrer car elles reposent sur des phénomènes physiques bien établis tels que le refroidissement de la température avec l’altitude ou la direction générale de la circulation des masses d’air d’ouest en est dans les zones de moyenne latitude. Mais il faut d’emblée souligner que dans les exemples qui seront analysés par la suite de ce chapitre on sera confronté à des situations moins simples et pour lesquelles il n’est pas toujours évident d’identifier de façon absolue le sens de la relation de causalité entre la variable \\(Y\\) et les autres variables \\(X_1...X_n\\). Dans certains cas, on est plutôt en situation de covariation ce qui signifie que l’onpeut prédire la valeur d’un indicateur en fonction d’un autre sans pour autant pouvoir affirmer avec certitude lequel est la cause et l’autre la conséquence.\n\n\nUne approche non technique\nLe but de ce chapitre d’introduction est de présenter le principe de la régression multiple de façon volontairement non technique afin de permettre aux étudiants de comprendre les rudiments de la méthode sans être immédiatement forcés d’en saisir les détails dans toute leur sophistication. Dit autrement , il s’agit de leur donner envie d’apprendre la technique en leur faisant saisir de façon intuitive tout son intérêt et toute sa puissance.\n\n\nUn problème exemplaire\nBien que l’exemple retenu par P.J. Taylor n’ait pas d’ambition scientifique (il concerne des faits bien connus des géographes et des climatologues), il est suffisamment exemplaire pour apparaître d’emblée intéressant à l’ensemble des géographes et - nous l’espérons - des chercheurs d’autres disciplines. Les prédictions météorologiques sont en effet un souci quotidien dans l’ensemble des pays du monde et les précipitations ont une importance de plus en plus vitale dans une période d’accélération du changement climatique.\nIl serait d’ailleurs intéressant de voir si on peut au cours de l’EE CIST 2023 actualiser les données de Taylor à près d’un demi-siècle de distance. Il fait en effet peu de doute que les préciptations en Californie ont certainement beaucoup changé depuis les années 1970…"
  },
  {
    "objectID": "MOD1_Yquanti_intro.html#b.-données",
    "href": "MOD1_Yquanti_intro.html#b.-données",
    "title": "Introduction : Les précipitations en Californie",
    "section": "B. Données",
    "text": "B. Données\nDans cette section, nous présentons les variables contenues dans le tableau de données, la localisatiion géographique des stations et enfin les hypothèses concernant les relations de causalité ou de covariations.\n\nLe tableau de données\nLe tableau original de Taylor (1980) a été légèrement modifié afin de le rendre plus familier pour des lecteurs francophones. Il s’agit uniquement de transformation des unités de mesure afin que les précipitations soient désormais en millimètres (et non pas en pouces), les altitudes en mètres (et non pas en pieds), les distances à la mer en (kilomètres) et non pas en miles. Ces transformations ne modifient normalement pas les résultats mais facilitent la transmission pour un public moins familier des mesures anglo-saxonnes.\n\n\n\nLes précipitations en Californie vers 1960\n\n\nCODE\nNOM\nPRE\nALT\nDIS\nABR\nLAT\nLNG\n\n\n\n\n1\nEUREKA\n1010\n10\n2\nNON\n40.8\n-124.2\n\n\n2\nRED-BLUFF\n590\n100\n155\nOUI\n40.2\n-122.2\n\n\n3\nTHERMAL\n460\n1270\n112\nOUI\n33.8\n-116.2\n\n\n4\nFORT-BRAGG\n950\n20\n2\nNON\n39.4\n-123.8\n\n\n5\nSODA-SPRING\n1250\n2060\n240\nNON\n39.3\n-120.4\n\n\n6\nSAN-FRANCISCO\n550\n20\n8\nNON\n37.8\n-122.5\n\n\n7\nSACRAMENTO\n460\n10\n128\nOUI\n38.5\n-121.5\n\n\n8\nSAN-JOSE\n360\n30\n45\nOUI\n37.4\n-121.9\n\n\n9\nGIANT-FOREST\n1080\n1940\n232\nNON\n36.6\n-118.7\n\n\n10\nSALINAS\n350\n20\n19\nOUI\n36.7\n-121.6\n\n\n11\nFRESNO\n240\n100\n182\nOUI\n36.7\n-119.8\n\n\n12\nPt-PIEDRAS\n490\n20\n2\nNON\n35.7\n-121.3\n\n\n13\nPASA-ROBLES\n400\n230\n50\nOUI\n35.7\n-120.7\n\n\n14\nBAKERSFIELD\n150\n150\n120\nOUI\n35.4\n-119.0\n\n\n15\nBISHOP\n150\n1250\n317\nOUI\n37.3\n-118.4\n\n\n16\nMINERAL\n1210\n1480\n227\nNON\n40.4\n-121.6\n\n\n17\nSANTA-BARBARA\n460\n40\n2\nNON\n34.4\n-119.7\n\n\n18\nSUSANVILLE\n460\n1270\n317\nOUI\n40.3\n-120.6\n\n\n19\nTULE-LAKE\n250\n1230\n224\nOUI\n41.9\n-121.5\n\n\n20\nNEEDLES\n120\n280\n307\nOUI\n34.8\n-114.6\n\n\n21\nBURBANK\n370\n210\n75\nNON\n34.2\n-118.3\n\n\n22\nLOS-ANGELES\n380\n100\n26\nNON\n34.1\n-118.3\n\n\n23\nLONG-BEACH\n310\n20\n19\nNON\n33.8\n-118.2\n\n\n24\nLOS-BANOS\n210\n40\n118\nOUI\n37.8\n-118.1\n\n\n25\nBLYTHE\n100\n80\n248\nOUI\n33.6\n-114.6\n\n\n26\nSAN-DIEGO\n250\n10\n8\nNON\n32.7\n-117.2\n\n\n27\nDAGGET\n110\n640\n136\nOUI\n34.1\n-116.9\n\n\n28\nDEATH-VALLEY\n40\n-50\n310\nOUI\n36.5\n-116.4\n\n\n29\nCRESCENT-CITY\n1900\n10\n2\nNON\n41.7\n-124.2\n\n\n30\nCOLUSA\n410\n20\n146\nOUI\n39.2\n-122.0\n\n\n\n\n\n\nCODE : Code de la station\nNOM : Nom de la station\nPRE : Précipitations annuelles (en millimètres)\nALT : Altitude (en mètres)\nDIS : Distance à la mer (en kilomètres)\nABR : Situation d’abri par rapport aux vents dominants\nLAT : Latitude (en degrés décimaux de latitude nord)\nLNG : Longitude (en dégrés décimaux)\n\n\n\nLocalisation\nLes différentes stations météorologiques peuvent être localisées sur une carte interactive si on dispose d’une connexion internet et si le document est enregistré au format .html. Il suffit alors de cliquer sur une station pour retrouver l’ensemble des informations du tableau. On aplacé en fonds d’écran un fonds de carte indiquant le relief ce qui permet de mieux comprendre la signification de la variable ABR qui indique si la station est face au vent d’ouest (ABR = “Non”) ou si elle se trouve sur un versant qui tourne le dos à la mer et reçois alors moins de précipitations (ABR = “Oui”).\n\n\n\n\n\n\nDans le cas où l’on ne dispose pas de connexion internet, on se contentera d’une carte statique indiquant juste la position des stations avec leur code.\n\n\n\n\n\nCode des stations météorologiques\n\n\n\n\n\n\nCarte des précipitations\nLa carte des précipitations ne montrent pas de schéma spatial d’organisation très évident de prime abord. La station n° 29 qui affiche les précipitations maximales (Crescent City, 1900 mm) se situe sur la côte au nord-ouest tandis que la station n°28 qui affiche les précipitations les plus faibles se situe à l’intérieur au sud-est (Death Valley, 40 mm). Entre ces deux extrêmes, on trouve de fortes différences entre stations parfois très proches dans l’espace. Ainsi, la station n°9 (Giant Forest) reçoit 1080 mm de pluie par an alors que la station n°11 (Fresno) qui est située à moins de 100 km ne reçoit que 240 mm par an.\n\n\n\n\n\ncarte des précipitations"
  },
  {
    "objectID": "MOD1_Yquanti_intro.html#c.-hypothèses-bivariées",
    "href": "MOD1_Yquanti_intro.html#c.-hypothèses-bivariées",
    "title": "Introduction : Les précipitations en Californie",
    "section": "C. Hypothèses bivariées",
    "text": "C. Hypothèses bivariées\nDans le cadre de l’exemple des précipitations en Californie, on peut émettre un certain nombre d’hypothèses causales sur les déterminants des précipitations qui obéissent aux règles du cycle de l’eau dans les régions de moyenne latitude soumises à une circulation dominante des masses d’airs d’ouest en est. L’exemple serait évidemment différent si l’on analysait les précipitations dans des régions de hautes ou basses latitudes.\n\n\n\n\n\nSource : manuel scolaire\n\n\n\nLe cycle de l’eau dans les régions de moyenne latitude\n\n\nH1 : Les précipitations augmentent avec l’altitude**\nLe phénomène s’explique par la diminution de la température de -0.6°C tous les 100 mètres. Une masse d’air chargée d’une certaine quantité d’humidité va se transformer en pluie lorsque la température sera descendue en dessous d’un certain seuil. Cette hypothèse est-elle vérifiée sur le plan statistique\n\n\n\n\n\nOn décèle en effet une légère tendance à l’accroissement des précipitations avec l’altitude, mais la relation observée n’est pas très forte et comporte de nombreuses exceptions. Au total, on ne peut mettre en évidence un effet significatif.\n\n\nH2 : Les précipitations augmentent avec la latitude\nDans le cas de la Californie qui est très étirée dans le sens Nord-Sud, les précipitations seront en moyenne plus importante au Nord qu’au Sud car les dépressions d’Ouest y circulent plus fréquemment. Cette hypothèse est-elle vérifiée sur le plan statistique ?\n\n\n\n\n\nNous obtenons une relation beaucoup plus forte que précédemment qui est nettement significative (r = 0.58, p <0.001). Il demeure certes des résidus important et l’on peut être par exemple intrigué par le cas des stations n°19 (Tule Lake) et n°29 (Crescent City) qui affichent des différences considérables de précipitations alors qu’elles sont situées pratiquement à la même latitude.\n\n\nH3 : les précipitations diminuent avec le distance à la mer\nLes masses d’airs circulant d’Ouest en Est vont perdre progressivement leur charge en humidité sur les premiers reliefs qu’elles vont rencontrer. Les chaînes de montagne éloignées de la côte recevront moins de précipitations à altitude égale.\n\n\n\n\n\nLa relation est bien négative comme nous l’avions prévu mais elle demeure nettement trop faible pour être significative (r = -0.21, p = 0.27). En dehors de l’exception remarquable constituée par la station n°29 (Crescent City), on remarque le groupe des stations n°5 (Soda Springs), 9 (Giant Forest) et 16 (Mineral) qui sont toutes caractérisées par un fort niveau de précipitation alors même qu’elles sont éloignées de plus de 200 km de la mer. Un retour au tableau montre que toutes ces stations ont pour point commun d’être localisée à une altitude supérieure à 1000 mètres. On peut alors commencer à deviner que si l’altitude et la distancd à la mer ne so,nt pas significatives de façon isolée, elles vont peut-être le devenir si on les combine ensemble.\n\n\nH4 : Les stations en situation d’abri reçoivent moins de précipitation\nEn effet les nuages tendent à déverser leur pluie sur le premier versant tournée vers la mer et arrosent moins les versants qui lui tournent le dos ou les dépressions.\n\n\n\n\n\nCette dernière hypothèse est un peu différente des précédentes puisqu’elle met en jeu une variable qualitative de type binaire, mais on peut comparer ses résultats avec les précédents en utilisant une astuce statistique consistant à transformer les modalités “OUI” en 1 et les modalités “NON” en 0. Malgré son caractère sommaire, cette variable dichotomique obtient le plus fort niveau de corrélation associé à une très forte significativité (r = +0.60, p < 0.001)\n\n\nDiscussion\nA s’en tenir aux résultats de l’analyse bivariée, on pourrait supposer que seules deux des quatre variables explicatives jouent un rôle dans la prévision et l’explication du niveau des précipitations. ce qui nous conduirait à formuler un modèle PRE = f(LAT,ABR) dans lequel on aurait retiré les variables ALT et DIS en raison de leur absence de significativité lorsqu’elles sont introduites de façon isolée.\nOn devine cependant que ces différents facteurs ne sont pas indépendant les uns des autres et qu’ils doivent être prise en compte simultanément si l’on veut rendre compte correctement du phénomène étudié. Deux cas intéressant sur le plan théorique peuvent d’ailleurs se produire :\n\nUne variable explicative dont l’effet est significatif lorsqu’elle est utilisée de façon isolée devient non-significative lorsqu’on la combine avec d’autres variables explicatives.\nUne variable explicative dont l’effet est non-significatif lorsqu’elle est utilisée de façon isolée devient significative lorsqu’on la combine avec d’autres variables explicatives.\n\nCe n’est donc qu’en introduisant simultanément l’ensemble des variables dans un même modèle qu’on pourra véritablement évaluer leurs pouvoirs explicatifs respectifs."
  },
  {
    "objectID": "MOD1_Yquanti_intro.html#d.-modélisation",
    "href": "MOD1_Yquanti_intro.html#d.-modélisation",
    "title": "Introduction : Les précipitations en Californie",
    "section": "D. Modélisation",
    "text": "D. Modélisation\nConformément aux objectifs de cette introduction, nous n’allons pas aborder immédiatement les aspects proprement statistiques de la régression linéaire. Nous allons plutôt suivre une démarche hypothético-déductive de construction progressive d’un modèle explicatif selon le schéma proposé par F. Durand-Dastès (1992) dans le texte intitulé “Le particulier et le général en géographie” et dont la logique est présentée ci-dessous.\n\n\n\n\n\nSource : manuel scolaire\n\n\n\nLa méthode hypothetico déductive (Durand-Dastès F., 1990)\n\n\nModèle 1 : Latitude + Altitude + Distance\nDans ce premier modèle nous introduisons les trois variables explicatives correspondant aux hypothèses H1, H2 et H3 que nous avons discuté dans la section précédente. Les résultats de la modélisation statsistique apparaissent sous la forme d’un tableau de prime abord complexe à comprendre pour le non spécialiste.\n\n\n\nParamètres du modèle 1\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPrécipitations (en mm)\n\n\n\n\n\n\n\n\nLatitude (degrés N)\n\n\n87.893***\n\n\n\n\n\n\n(20.175)\n\n\n\n\n\n\n\n\n\n\nAltitude (m)\n\n\n0.339***\n\n\n\n\n\n\n(0.101)\n\n\n\n\n\n\n\n\n\n\nDistance à la mer (km)\n\n\n-2.265***\n\n\n\n\n\n\n(0.577)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2,609.336***\n\n\n\n\n\n\n(741.251)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n30\n\n\n\n\nR2\n\n\n0.600\n\n\n\n\nAdjusted R2\n\n\n0.554\n\n\n\n\nResidual Std. Error\n\n\n281.769 (df = 26)\n\n\n\n\nF Statistic\n\n\n12.997*** (df = 3; 26)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nOn peut traduire les résultats sous la forme de l’équation suivante qui donne les paramètres de prédiction \\(PRE_i\\) d’une station \\(i\\) en fonction de sa latitude \\(LAT_i\\), de son altitude \\(ALT_i\\) et de sa distance à la mer \\(DIS_i\\) avec une erreur égale à \\(\\epsilon_i\\) :\n\\(PRE_i = -2609 + (87.9 \\times LAT_i) + (0.101 \\times ALT_i) - (2.27 \\times DIS_i) + \\epsilon_i\\)\nPar exemple, la station n°1 (Eureka) est située à latitude de 40.8 degrés avvec une altitude de 10 mètres et une distance à la mer de 2 kilomètres. Le modèle prévoit donc que ses précipitations devraient être approximativement égales à :\n\\(PRE_1 = -2609 + (87.9 \\times 40.8) + (0.101 \\times 10) - (2.27 \\times 2) = 975.6\\)\nLa valeur observée est légèrement plus forte que la prédiction (1010) ce quui donne une erreur résiduelle \\(\\epsilon_1\\) égale à 1010-975.6 = +34.4. Comme dans un modèle de régression simple, on peut établir un tableau de résultat présentant les valeurs observées, les valeurs estimées et les résidus qui correspondent à la différence entre les deux.\n\n\n\nValeurs estimées et résiduelles du modèle 1\n\n\nCode\nNom\nValeur observée\nValeur estimée\nrésidu\n\n\n\n\n1\nEUREKA\n1010\n975.6\n34.4\n\n\n2\nRED-BLUFF\n590\n606.8\n-16.8\n\n\n3\nTHERMAL\n460\n538.7\n-78.7\n\n\n4\nFORT-BRAGG\n950\n855.9\n94.1\n\n\n5\nSODA-SPRING\n1250\n1000.2\n249.8\n\n\n6\nSAN-FRANCISCO\n550\n701.7\n-151.7\n\n\n7\nSACRAMENTO\n460\n488.0\n-28.0\n\n\n8\nSAN-JOSE\n360\n586.1\n-226.1\n\n\n9\nGIANT-FOREST\n1080\n740.3\n339.7\n\n\n10\nSALINAS\n350\n580.1\n-230.1\n\n\n11\nFRESNO\n240\n238.0\n2.0\n\n\n12\nPt-PIEDRAS\n490\n530.7\n-40.7\n\n\n13\nPASA-ROBLES\n400\n493.2\n-93.2\n\n\n14\nBAKERSFIELD\n150\n281.2\n-131.2\n\n\n15\nBISHOP\n150\n375.1\n-225.1\n\n\n16\nMINERAL\n1210\n929.5\n280.5\n\n\n17\nSANTA-BARBARA\n460\n423.2\n36.8\n\n\n18\nSUSANVILLE\n460\n645.6\n-185.6\n\n\n19\nTULE-LAKE\n250\n983.3\n-733.3\n\n\n20\nNEEDLES\n120\n-151.1\n271.1\n\n\n21\nBURBANK\n370\n298.0\n72.0\n\n\n22\nLOS-ANGELES\n380\n362.9\n17.1\n\n\n23\nLONG-BEACH\n310\n325.2\n-15.2\n\n\n24\nLOS-BANOS\n210\n459.3\n-249.3\n\n\n25\nBLYTHE\n100\n-190.8\n290.8\n\n\n26\nSAN-DIEGO\n250\n250.0\n0.0\n\n\n27\nDAGGET\n110\n296.9\n-186.9\n\n\n28\nDEATH-VALLEY\n40\n-120.4\n160.4\n\n\n29\nCRESCENT-CITY\n1900\n1054.7\n845.3\n\n\n30\nCOLUSA\n410\n512.1\n-102.1\n\n\n\n\n\nEn comparant la variance des résidus à la variance initiale de la variable dépendante, on peut calculer le pouvoir explicatif de notre modèle qui est le carré du coefficient de corrélation multiple noté en abrégé \\(R^2\\). Dans notre exemple ce pouvoir explicatif est environ de 60% ce qui signifie que près de 40% des différences de précipitations entre les stations demeurent à ce stade inexpliquée par les trois variables que nous avons introduites dans le modèle. Si certaines stations bénéficient de prédiction quasi pafaites comme Fresno (n°11) ou San Diego (n°26), d’autres affichent des erreurs considérables d’estimation. C’est notamment le cas des deux stations situées le plus au nord : Tule Lake (n°19) et Crescent City (n°29).\nUne cartographie des résidus peut alors être entreprise afin de vérifier si les erreurs se distribuent au hasard dans l’espace où si elles se regroupent dans certaines situations particulières pouvant révéler l’existence d’une autre variable explicative que l’on n’aurait pas aperçue de prime abord.\n\n\n\n\n\nRésidus du modèle 1\n\n\n\n\nOn remarque sur la carte que les résidus positifs et négatifs ne se distribuent pas au hasard mais semblent liés aux différentes chaînes de montagnes qui traversent la Californie. On remarque plus précisément que les stations qui sont tournées vers l’ouest face à l’Océan ont en général des résidus positifs ce qui signifie que notre modèle a sous-estime leur quantité de pluie. Inversement, les stations tournées vers l’est et situées en position d’abri ont en général des résidus négtifs, ce qui signifie que notre modèle a surestimé leurs précipitations. Même si la règle comporte des exceptions (certaines stations en situation d’abri ont des résidus positifs), elle semble suffisamement générale pour justifier l’introduction d’une nouvelle variable dans le modèle.\n\n\nModèle 2 : Latitude + Altitude + Distance + Abri\nLa variable situation d’abri étant qualitative, elle est transformée en une variable quantitative binaire (dummy variable) où la modalité 1 correspond à la présence du phénomène d’abri et la modalité 0 à son absence. Le modèle va donc rajouter un paramètre supplémentaire dans l’équation pour indiqiuer de combien il faut accroître ou réduire les précipitations en cas de situation d’abri.\n\n\n\nParamètres du modèle 2\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPrécipitations (en mm)\n\n\n\n\n\n\n\n\nLatitude (degrés N)\n\n\n87.883***\n\n\n\n\n\n\n(16.682)\n\n\n\n\n\n\n\n\n\n\nAltitude (m)\n\n\n0.183*\n\n\n\n\n\n\n(0.094)\n\n\n\n\n\n\n\n\n\n\nDistance à la mer (km)\n\n\n-0.852\n\n\n\n\n\n\n(0.617)\n\n\n\n\n\n\n\n\n\n\nAbri (Oui/Non)\n\n\n-401.351***\n\n\n\n\n\n\n(111.186)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2,493.660***\n\n\n\n\n\n\n(613.735)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n30\n\n\n\n\nR2\n\n\n0.737\n\n\n\n\nAdjusted R2\n\n\n0.695\n\n\n\n\nResidual Std. Error\n\n\n232.978 (df = 25)\n\n\n\n\nF Statistic\n\n\n17.516*** (df = 4; 25)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nLa nouvelle équation du modèle peut maintenant s’écrire :\n\\(PRE_i = -2493 + (87.9 \\times LAT_i) + (0.183 \\times ALT_i) - (0.85 \\times DIS_i) - (401 \\times ABR_i)+ \\epsilon_i\\)\nLe paramètre de la variable \\(ABR\\) indique que les stations situées en position d’abri ont des précipitations plus faible d’environ 400 mm que les stations tournées vers l’ouest face aux vents dominants. L’ajout de ce paramètre augmente considérablement le pouvoir explicatif du modèle puisque son coefficient de détermination \\(R^2\\) passe de 60.0% dans le modèle 1 à 73.7% dans le modèle 2.\nMais l’ajout de cette nouvelle variable modifie aussi le rôle des trois variables précédentes. Si l’effet de la latitude ne change pas et demeure très significatif, il n’en va pas de même pour la variable altitude. Alors que le modèle 1 suggérait un accroissement des pluies de 0.33 mm de pluies chaque fois que l’altitude auglente d’un mètre, le modèle 2 indique un effet plus limité de 0.183 mm de pluie par mètre d’altitude. Cettevariable demeure significative mais moins que précédemment. Quand à la variable distance à la mer, elle apparaît désormais non significative et pourrait être retiré du modèle.\nCes modifications suggèrent que l’effet d’abri est corrélé avec les deux variables altitude et distance à la mer ce que confirme l’observation empirique. Leur pouvoir explicatif est donc diminué car il correspondait - au moins en partie - à l’effet d’une variable au pouvoir explicatif plus puissant.\n\n\n\nValeurs estimées et résiduelles du modèle 2\n\n\nCode\nNom\nValeur observée\nValeur estimée\nrésidu\n\n\n\n\n1\nEUREKA\n1010\n1092.1\n-82.1\n\n\n2\nRED-BLUFF\n590\n524.0\n66.0\n\n\n3\nTHERMAL\n460\n211.9\n248.1\n\n\n4\nFORT-BRAGG\n950\n970.9\n-20.9\n\n\n5\nSODA-SPRING\n1250\n1131.8\n118.2\n\n\n6\nSAN-FRANCISCO\n550\n825.1\n-275.1\n\n\n7\nSACRAMENTO\n460\n381.2\n78.8\n\n\n8\nSAN-JOSE\n360\n358.9\n1.1\n\n\n9\nGIANT-FOREST\n1080\n879.5\n200.5\n\n\n10\nSALINAS\n350\n317.7\n32.3\n\n\n11\nFRESNO\n240\n193.5\n46.5\n\n\n12\nPt-PIEDRAS\n490\n645.7\n-155.7\n\n\n13\nPASA-ROBLES\n400\n241.8\n158.2\n\n\n14\nBAKERSFIELD\n150\n141.2\n8.8\n\n\n15\nBISHOP\n150\n341.2\n-191.2\n\n\n16\nMINERAL\n1210\n1133.7\n76.3\n\n\n17\nSANTA-BARBARA\n460\n535.1\n-75.1\n\n\n18\nSUSANVILLE\n460\n608.5\n-148.5\n\n\n19\nTULE-LAKE\n250\n821.0\n-571.0\n\n\n20\nNEEDLES\n120\n-47.2\n167.2\n\n\n21\nBURBANK\n370\n486.4\n-116.4\n\n\n22\nLOS-ANGELES\n380\n499.2\n-119.2\n\n\n23\nLONG-BEACH\n310\n464.2\n-154.2\n\n\n24\nLOS-BANOS\n210\n333.7\n-123.7\n\n\n25\nBLYTHE\n100\n-138.9\n238.9\n\n\n26\nSAN-DIEGO\n250\n375.1\n-125.1\n\n\n27\nDAGGET\n110\n102.8\n7.2\n\n\n28\nDEATH-VALLEY\n40\n39.4\n0.6\n\n\n29\nCRESCENT-CITY\n1900\n1171.2\n728.8\n\n\n30\nCOLUSA\n410\n429.2\n-19.2\n\n\n\n\n\n\n\n\n\n\nRésidus du modèle 2\n\n\n\n\nL’examen du tableau et de la carte des résidus montre une amélioration générale des prédictions, même si dans certains cas une station peut voir son erreur augmenter lorsqu’on passe du modèle 1 au modèle 2. Il n’en demeure pas moins que les deux stations du nord de la Californie continuent à afficher des résidus exceptionnellement forts, l’un positif (Crescent City) et l’autre négatif (Tule Lake) ce qui laisse penser qu’elles relèvent d’un autre modèle climatique que celui des autres stations de Californie.\nCeci conduit à proposer un troisième modèle où l’on ne conserve que 28 stations en excluant les deux stations septentrionales.\n\n\nModèle 3 : idem mais sans les deux stations du nord.\nCe dernier modèle possède un pouvoir explicatif remarquable puisque son coefficient de détermination \\(R^2\\) est de 89% ce qui signifie qu’à peine 10% des variations de précipitations entre les stations demeurent inexpliquées. On note de plus que les quatre variables sont désormais toutes très significatives, ce qui n’était pas le cas dans le modèle 2 où les variables altitude et distance à la mer avaient vu leur significativité fortement réduite par rapport au modèle 1. On peut donc conclure que nous disposons désormais d’un modèle d’excellente qualité pour prévoir les précipitations en Californie (exception faite de sa partie la plus septentrionale).\nNotre modèle permet de voir l’effet de chacune de nos quatre variables explicatives, non plus de façon isolée mais toutes choses égales quant à l’effet des trois autres :\n\nLatitude : les précipitations augmentent en moyenne de 78 mm chaque fois que l’on se déplace vers le nord d’un degré.\nAltitude : les précipitations augmentent en moyenne de 27.5 mm chaque fois que l’altitude augmente de 100 mètre.\nDistance à la mer : les précipitations diminuent 9 mm chaque fois que l’on s’éloigne de la côte Pacifique de 10 km .\nSituation d’abri : les précipitations sont en moyenne plus faible de 285 mm dans les stations situées à l’abri des vents d’ouest.\n\nSur la base de ces résultats on pourrait théoriquement produire des prévisions relativement fiable d’un point quelconque de Californie qui ne disposerait pas de station météorologique mais où l’on connaîtrait chacun des paramètres.\n\n\n\nParamètres du modèle 3\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPrécipitations (en mm)\n\n\n\n\n\n\n\n\nLatitude (degrés N)\n\n\n77.836***\n\n\n\n\n\n\n(10.155)\n\n\n\n\n\n\n\n\n\n\nAltitude (m)\n\n\n0.275***\n\n\n\n\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\nDistance à la mer (km)\n\n\n-0.899**\n\n\n\n\n\n\n(0.330)\n\n\n\n\n\n\n\n\n\n\nAbri (Oui/Non)\n\n\n-284.514***\n\n\n\n\n\n\n(60.871)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2,229.328***\n\n\n\n\n\n\n(369.080)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28\n\n\n\n\nR2\n\n\n0.887\n\n\n\n\nAdjusted R2\n\n\n0.867\n\n\n\n\nResidual Std. Error\n\n\n123.545 (df = 23)\n\n\n\n\nF Statistic\n\n\n44.959*** (df = 4; 23)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\nSynthèse et discussion\nOn peut résumer les étapes de notre démarche hypothético-déductive à l’aide d’un tableau final qui regroupe les paramètres des trois modèles et permet de voir comment la qualité générale de l’ajustement augmente et comment les paramètres de chacune des variables explicatives évolue en fonction de la présence d’autres variables ou de l’exclusion de valeurs exceptionnelles. Nous procédons ce faisant à un test de stabilité des résultats qui permet de repérer les facteurs explicatifs dont l’effet est indiscutable (latitude, effet d’abri) et ceux dont la significativité est plus variable et dont les paramètres peuvent se modifier plus ou moins fortement (altitude, distance à la mer).\n\n\n\nSynthèse des modèles\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPrécipitations (en mm)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nLatitude (degrés N)\n\n\n87.893***\n\n\n87.883***\n\n\n77.836***\n\n\n\n\n\n\n(20.175)\n\n\n(16.682)\n\n\n(10.155)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAltitude (m)\n\n\n0.339***\n\n\n0.183*\n\n\n0.275***\n\n\n\n\n\n\n(0.101)\n\n\n(0.094)\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistance à la mer (km)\n\n\n-2.265***\n\n\n-0.852\n\n\n-0.899**\n\n\n\n\n\n\n(0.577)\n\n\n(0.617)\n\n\n(0.330)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbri (Oui/Non)\n\n\n\n\n-401.351***\n\n\n-284.514***\n\n\n\n\n\n\n\n\n(111.186)\n\n\n(60.871)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2,609.336***\n\n\n-2,493.660***\n\n\n-2,229.328***\n\n\n\n\n\n\n(741.251)\n\n\n(613.735)\n\n\n(369.080)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n30\n\n\n30\n\n\n28\n\n\n\n\nR2\n\n\n0.600\n\n\n0.737\n\n\n0.887\n\n\n\n\nAdjusted R2\n\n\n0.554\n\n\n0.695\n\n\n0.867\n\n\n\n\nResidual Std. Error\n\n\n281.769 (df = 26)\n\n\n232.978 (df = 25)\n\n\n123.545 (df = 23)\n\n\n\n\nF Statistic\n\n\n12.997*** (df = 3; 26)\n\n\n17.516*** (df = 4; 25)\n\n\n44.959*** (df = 4; 23)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nConcernant la stratégie adoptée dans le modèle 3 (retrait de deux valeurs exceptionnelles) il est important de préciser qu’elle fait l’objet de vives controverses entre les spécialistes de modélisation. Certains auteurs considèrent comme sacrilège de retirer la moindre valeur du tableau initial, estimant qu’il s’agit d’une forme de “tricherie” scientifique. D’autres auteurs y voient au contraire une nécessité dès lors que les valeurs exceptionnelles perturbent l’ensemble des résultats et mettent en péril les conclusions qu’on pourrait tirer. Laissons sur ce point la parole à l’auteur de l’étude sur les précipitations en Californie :\n\n“We argue that our variables behave differently in the extreme north and so we cannot expect to model that region within the same analysis as the rest of the state. Hence we will omit these two stations and proceed with a third model incorporating just 28 stations. Such a strategy is certainly controversial, for a researcher should not pick and choose what observations include in this way other than in exceptional circumstances. The residuals for stations and 29 are exceptional. (A useful next stage would be to collect data for Washington and Oregon and see whether these two stations fit consistently into a new “north-west region” multiple regression equation.” (P.J. Taylor, 1980)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modélisation d’une variable quantitative",
    "section": "",
    "text": "L’introduction du cours utilise un exemple pédagogique simple mais efficace, celui des précipitations en Californie. Sans entrer immédiatement dans les détails statistiques, cet exemple inventé par le géographe Peter Taylor possède l’avantage de décrire des relations déterministes où il est facile de comprendre intuitivement le sens des relations causales qui relient la variable dépendante Y et les différentes variables indépendantes X1…Xn.\nLe cours d’économétrie repris d’un enseignement de L2 dispensé à l’ENSPD de Parakou par Nagège Gbetoton fournit l’ensemble des bases statistiques et mathématiques nécessaires pour les personnes souhaitant approfondir les élémenrs donnés en introduction.\nLes exercices d’applications sous R utilisent quant à eux les fonctions de base du logiciel pour ne pas perdre le débutant. Très peu de packages supplémentaires sont moblisés pour que l’on se concentre sur l’apprentissage de la statistique plutôt que sur celui des graphiques. Les deux premiers portent sur des données spatialement aggrégées, les deux suivants sur des données individuelle d’enquête et de recensement.\nN’hésitez pas à faire part de vos remarques ou critiques aux auteurs :\nClaude Grasland : claude.grasland@parisgeo.cnrs.fr Nadège Gbetoton : nadeged2001@yahoo.fr"
  },
  {
    "objectID": "MOD1_Yquanti_exo4.html",
    "href": "MOD1_Yquanti_exo4.html",
    "title": "Exo 4 : Equipement des ménages du Bénin",
    "section": "",
    "text": "L’objectif de la présente analyse est d’analyser un tableau de données individuelles permettant de décrire le niveau de vie des ménages en fonction d’un certain nombre d’indicateurs endogènes ou exogènes."
  },
  {
    "objectID": "MOD1_Yquanti_exo4.html#a.-donnees",
    "href": "MOD1_Yquanti_exo4.html#a.-donnees",
    "title": "Exo 4 : Equipement des ménages du Bénin",
    "section": "A. DONNEES",
    "text": "A. DONNEES\nLe jeu de données est un échantillon de variables relatives aux ménages du Bénin lors du RP 2013. Il s’agit plus précisément d’une extraction par tirage au sort de 100 ménages dans chacun des 546 arrondissements du Bénin. Le tirage au sort a été effectué en se limitant aux ménages composés d’au moins deux personnes, dont le chef de ménage est marié ou en union libre et dont les variables chosies ne comportent pas de valeurs manquantes ou non renseignés. La base de sondage se compose donc d’1.305 millions de ménages sur les 2 millions que compte le Bénin à cette date. L’échantillon comporte exactement 54600 lignes (100 pour chacun des 546 arrondissements).\nLes variables ont été anonymisées en supprimant le numéro d’identification des ménages du recensement et en le remplaçant par le code de l’arrondissement suivi d’un chiffre de 001 à 100. Les variables les plus discriminantes ont vu leur nombre de modalité réduit.\n\nImportation\nOn importe les données\n\ndon <- readRDS(\"BEN-MEN-2013/BEN-MEN-2013-SAMPLE.RDS\")\nkable(head(don), caption = \"Premières lignes du tableau\")\n\n\nPremières lignes du tableau\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nWGT\nDEP\nDEP_NAME\nCOM\nCOM_NAME\nARR\nARR_NAME\nURBA\nDEP_CHEF\nCOM_CHEF\nCOM_CHEF_DIST\nDEP_CHEF_DIST\nNAT_CHEF_DIST\nstatoc\ntailmen\nlocnais\ndatnais\nage\nsexe\netud5\nradio\ntelev\nhifi\nparab\nmagne\ncddvd\nfrigo\ncuisi\nfoyam\nferre\nclima\nventi\nlit\nmatel\nfaumo\nordi\ninter\nelgen\nbicyc\nmotoc\nvoitu\nbarqu\ntelfix\ntelmob\nequip_tot\nequip_acp\n\n\n\n\n7\n01101_001\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropFam\n4\n011\n1987\n26\nM\n1.Aucun\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n1\n-1.1496591\n\n\n39\n01101_002\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropFam\n7\n011\n1978\n35\nM\n1.Aucun\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n3\n-0.6338360\n\n\n45\n01101_003\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropFam\n6\n011\n1982\n31\nM\n1.Aucun\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\n4\n-0.3559969\n\n\n76\n01101_004\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropInd\n9\n011\n1970\n43\nM\n1.Aucun\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n5\n-0.0904709\n\n\n91\n01101_005\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropFam\n5\n011\n1983\n30\nM\n1.Aucun\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n0\n-1.1499543\n\n\n115\n01101_006\n37.58\n01\nALIBORI\n011\nBANIKOARA\n01101\nFOUNOUNGO\n0\n0\n0\n23.30885\n55.21384\n572.8043\nPropFam\n8\n011\n1988\n25\nM\n1.Aucun\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n5\n-0.2158661\n\n\n\n\nsum(don$WGT)\n\n[1] 1305188\n\n\n\n\nListe des variables\nLe tableau comporte 47 variables\n\nSTRATES\nID : identifiant anonyme du ménage\nWGT : poids du ménage\nPOSITION GEOGRAPHIQUE\nDEP : code du département\nDEP_NAME : nom du département\nCOM : code de la commune\nCOM_NAME : nom de la commune\nARR : code de l’arrondissement\nARR_NAME : nom de l’arrondissement\nURBA : statut de l’arrondissement (0 = rural, 1 = urbain)\nDEP_CHEF : arrondissement situé au chef-lieu du département (0 = non, 1 = oui)\nCOM_CHEF : arrondissement situé au chef-lieu de la commune (0 = non, 1 = oui)\nCOM_CHEF_DIST : distance de l’arrondissement au chef-lieu de sa commune (en km)\nDEP_CHEF_DIST : distance de l’arrondissement au chef-lieu de son département (en km)\nNAT_CHEF_DIST : distance de l’arrondissement à l’aéroport de Cotonu (en km)\nSTRUCTURE DU MENAGE\nstatoc : statut d’occupation (Propriété individuelle ou familiale, locataire, autre)\ntailmen : nombre de personnes recensées dans le ménage\nATTRIBUTS DU CHEF DE MENAGE\nlocnais : code de la commune de naissance (ou du département ou du pays étranger)\ndatnais : année de naissance\nage : âge en différence de millésime\nsexe : sexe\netud5 : niveau d’étude maximal (ramené à 5 modalités)\nEQUIPEMENT DU MENAGE (présence ou absence) \nradio : poste de radio\ntelev : poste de télévision\nhifi : chaîne HIFI\n\nparab : parabole\nmagne : magnetoscope\ncddvd : lecteur de CD-DVD\n\nfrigo : réfrigérateur\n\ncuisi : cuisinière\n\nfoyam : foyer aménagé\n\nferre : fer à repasser\n\nclima : climatiseur\n\nventi : betilateur\n\nlit : lit\n\nmatel : matelas\n\nfaumo : fauteuril moderne\n\nordi : ordinateur\n\ninter : accès internet\n\nelgen : générateur électrique\n\nbicyc : bicyclette\n\nmotoc : motcyclette\n\nvoitu : coiture\n\nbarqu : barque, pirogue\n\ntelfix: téléphone fixe (au moins une ligne)\n\ntelmob: téléphone mobile (au moins un abonnement)\nSYNTHESE DES EQUIPEMENTS\nequip_tot : nombre total d’équipements différents (0 à 24)\nequip_acp : coordonnées sur le 1er axe factoriel d’une ACP sur les 24 équipements.\n\n\n\nFonds de carte\nNous disposons de quatre fonds de carte correspondant au découpage du Bénin à chacune des échelles d’analyse\n\nBEN0<-readRDS(\"BEN-MEN-2013/BEN-ADM0.RDS\")\nBEN1<-readRDS(\"BEN-MEN-2013/BEN-ADM1-DIST.RDS\")\nBEN2<-readRDS(\"BEN-MEN-2013/BEN-ADM2-DIST.RDS\")\nBEN3<-readRDS(\"BEN-MEN-2013/BEN-ADM3-DIST.RDS\")\n\nIl permet d’ores et déjà de repérer la position des départements, communes et arrondissements.\n\nmf_map(BEN1, type=\"typo\",var=\"DEP_NAME\")\nmf_map(BEN3, type=\"base\",col=NA,border=\"white\",lwd=0.1,add=T)\nmf_map(BEN2, type=\"base\",col=NA,border=\"black\",lwd=0.5,add=T)\nmf_map(BEN1, type=\"base\",col=NA,border=\"black\",lwd=2,add=T)\nmf_layout(title = \"Carte administrative du Bénin en 2013\",\n          credits = \"Source : EE CIST 2023\",frame = T)"
  },
  {
    "objectID": "MOD1_Yquanti_exo4.html#b.-analyse-du-benin",
    "href": "MOD1_Yquanti_exo4.html#b.-analyse-du-benin",
    "title": "Exo 4 : Equipement des ménages du Bénin",
    "section": "B. ANALYSE DU BENIN",
    "text": "B. ANALYSE DU BENIN\nOn choisit de retenir l’ensemble des ménages de l’échantillon et de prendre comme variable dépendante (Y) le nombre total d’équipements dont ils disposent. La valeur minimale est de 0 et la valeur maximale de 24. La distribution est fortement dissymétrique à gauche ce qui est logique\n\nsel<-don\nsel$equip<-sel$equip_tot\nhist(sel$equip, \n     breaks=seq(0,24,1),\n     col=\"lightyellow\",\n     probability = T,\n     main=\"Equipements des ménages du Bénin en 2013\",\n     sub = \"Source : RP2013, INS Bénin, échantillon stratifié par arrondissements\",\n     xlab=\"Nombre d'équipements\",\n     ylab= \"Probabilité\"\n     )\n\n\n\nsummary(sel$equip)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   3.000   4.049   5.000  24.000 \n\n\n\nCommentaire : le nombre moyen d’équipement des ménages est environ de 4 mais la médiane est de 3 et le mode de 0. La distribution est donc très fortement dissymétrique à gauche avec un grand nombre de ménages n’ayant que des équipements de base (radio, bicyclette, téléphone mobile,) et une très petite minorité accédant à tous les équipements y compris les plus rares et les plus coûteux (connexion internet, ordinateur, …)\n\n\nSTRUCTURE DU MENAGE\nOn peut modéliser tout d’abord l’effet de variables structurelles liées à la taille du ménage et au statut d’occupation du logement.\n\nTaille du ménage\n\nX<-log(sel$tailmen)\nY<-sel$equip\nnameX<-\"Taille du ménage (log)\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nplot(X,Y ,\n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY)\nabline(mod, col=\"red\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5649 -2.2868 -0.7923  1.2188 19.8085 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.80348    0.04569   61.35   <2e-16 ***\nX            0.71331    0.02492   28.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.264 on 54598 degrees of freedom\nMultiple R-squared:  0.01479,   Adjusted R-squared:  0.01477 \nF-statistic: 819.6 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : Relation significative mais à très faible pouvoir explicatif. Le nombre d’équipement augmente de 0.7 chaque fois que le logarithme du nombre de personnes présentes augmente de 1. Un ménage de 10 personnes devrait donc avoir 2.8 + 0.7*log(10) = 4.4 équipements.\n\n\n2.8+0.7*log(10)\n\n[1] 4.41181\n\n\n\n\nStatut d’occupation\n\nX<-sel$statoc\nY<-sel$equip\nnameX<-\"Statut d'occupation\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4322 -2.3494 -0.5342  1.4658 19.6506 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.34941    0.02382  182.56  < 2e-16 ***\nXPropFam    -0.81522    0.03009  -27.10  < 2e-16 ***\nXAutre      -0.48134    0.07372   -6.53 6.65e-11 ***\nXLocat       2.08282    0.05362   38.84  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.191 on 54596 degrees of freedom\nMultiple R-squared:  0.05882,   Adjusted R-squared:  0.05877 \nF-statistic:  1137 on 3 and 54596 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : Avantage significatif des locataires (p <0.001) qui ont en moyenne 2 équipements de plus que les propriétaires individuels. Mais cela révèle sans doute indirectement un effet urbain-rural.\n\n\n\n\nATTRIBUTS DU CHEF DE MENAGE\nUne seconde série de caractéristique concernant les attributs du chef de ménage permet de tester d’autres reltions.\n\nSexe\n\nX<-sel$sexe\nY<-sel$equip\nnameX<-\"Sexe\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3115 -2.3115 -0.6331  1.3669 20.3669 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.31151    0.01506  286.31   <2e-16 ***\nXF          -1.67845    0.03807  -44.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.232 on 54598 degrees of freedom\nMultiple R-squared:  0.03437,   Adjusted R-squared:  0.03435 \nF-statistic:  1943 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : Les ménages dont le chef est une femme ont en moyenne 1.7 équipements de moins que les ménages dont le chef est un homme (4.3). La différence est significative.\n\n\n\nClasse d’âge\n\nsel$age5<-cut(sel$age, breaks=c(18,29,39,49,59,100),include.lowest = T)\nlevels(sel$age5)<-c(\"18-29\", \"30-39\",\"40-49\",\"50-59\",\"60+\")\nX<-sel$age5\nY<-sel$equip\nnameX<-\"Classe d'âge\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.286 -2.218 -0.752  1.493 19.796 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.75199    0.03470 108.134   <2e-16 ***\nX30-39       0.46621    0.04292  10.862   <2e-16 ***\nX40-49       0.53391    0.04509  11.840   <2e-16 ***\nX50-59       0.45242    0.05097   8.876   <2e-16 ***\nX60+        -0.24495    0.05008  -4.891    1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.276 on 54595 degrees of freedom\nMultiple R-squared:  0.007785,  Adjusted R-squared:  0.007713 \nF-statistic: 107.1 on 4 and 54595 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : Relation significative mais à très faible pouvoir explicatif. Elle met en valeur le fait que les ménages les moins bien équipés sont ceux dont le chef est très jeune (moins de 30 ans) ou très âgé (plus de 60 ans).\n\n\n\nNiveau d’étude\n\nX<-sel$etud5\nY<-sel$equip\nnameX<-\"Niveau d'étude\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5163 -2.1405 -0.3599  1.4995 19.8595 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    3.14052    0.01590  197.47   <2e-16 ***\nX2.Primaire    1.35999    0.03236   42.03   <2e-16 ***\nX3.Secondaire  3.21940    0.03682   87.44   <2e-16 ***\nX4.Supérieur   6.37579    0.07843   81.30   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.946 on 54596 degrees of freedom\nMultiple R-squared:  0.1974,    Adjusted R-squared:  0.1974 \nF-statistic:  4477 on 3 and 54596 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : Relation très significative et à très fort pouvoir explicatif comparée aux précédentes (r2 = 19.7%). Il s’agit du facteur le plus déterminant parmi ceux qui ont été retenus. Le niveau d’étude sert évidemment de proxy à d’autres variables telles que le revenu, la situation professionnelle, etc.\n\n\n\n\nPOSISTION GEOGRAPHIQUE\nNous proposons maintenant une suite d’analyse portant sur les déterminants géographiques en distinguant deux cas selon que la variable explicative est catégorielle ou selon qu’elle correspond à un effect continu d’éloignement.\n\nDéterminants territoriaux\nLes déterminants territoriaux correspondent au cas où la variable explicative est catégorielle et où l’effet sur le ménage est lié à son appartenance à un type d’espace. Ceci nous permet d’identifier trois variables :\n\nURBA : l’arrondissement où le ménage est situé est-il déclaré “rural” ou “urbain” par l’état ?\nDEF_CHEF : l’arrondissement où le ménage est situé est-il le siège d’un chef-lieu de département\nCOM_CHEF : l’arrondissement où le ménage est situé est-il le siège d’un chef-lieu de commune\n\n\nmap0<-BEN0\nmap1<-BEN1\nmap2<-BEN2\nmap3<-BEN3\nmap3$URBA<-as.factor(map3$URBA)\nlevels(map3$URBA)<-c(\"Rural\",\"Urbain\")\nmap2ctr<-st_centroid(map3[map3$COM_CHEF==1 & map3$DEP_CHEF==0,])\nmap1ctr<-st_centroid(map3[ map3$DEP_CHEF==1,])\n\nmf_map(map3,type=\"typo\",var=\"URBA\",pal=c(\"lightyellow\",\"orange\"), border=\"gray\",lwd=0.3,leg_title = \"Type d'espace\")\nmf_map(map2,type=\"base\",col=NA, border=\"black\",lwd=1, add=T)\nmf_map(map1,type=\"base\",col=NA, border=\"black\",lwd=2, add=T)\nmf_map(map2ctr,type=\"base\",pch=20,col=\"blue\",add=T,cex=0.4)\n#mf_label(map2ctr,var = \"COM_NAME\", col=\"blue\",pos=1, cex=0.4, overlap=F)\nmf_map(map1ctr,type=\"base\",pch=15,col=\"red\",add=T, cex=0.6)\nmf_label(map1ctr,var = \"COM_NAME\", col=\"red\",pos=1,cex = 0.5,overlap = F)\nmf_layout(title = \"Communes et arrondissements du Borgou\",\n          credits = \"Source : EE CIST 2023\",frame = T)\n\n\n\n\n\n\nUrbain/Rural\nL’attribution du caractère urbain ou rural à un arrondissement est le résultat d’une décision administrative de l’état. Elle se traduit dans le recensement de 2013 par l’ajout d’un code “5” en quatrième position du code de l’arrondissement. Il peut y avoir plusieurs arrondissements “urbains” à l’intérieur d’une même commune. Il peut également arriver que l’ensemble d’une commune soit composée d’arrondissements urbain comme c’est le cas pour Parakou. Les arrondissements urbains contigus portent en général le même nom suivi d’un numéro (PARAKOU 01, PARAKOU 02, PARAKOU 03, PARAKOU 04) et leur réunion forment implicitement une “ville” dans une logique administrative. Mais les contours de ces villes sont très différents de ceux que l’on pourrait obtenir par des sources non administratives telles que les images de télédétection utilisées par la base Africapolis où des données détaillées de mobilité à l’échelon indiviuel (non accessibles pour des raisons de secret statistique).\nQu’elle reflète ou non une réalité morphologique (continuité du bâti) ou fonctionnelle (solde migratoire positif) n’est cependant pas ici la question principale. En revanche il peut être intéressant de se demander si les arrondissements urbains bénéficient d’un traitement privilégié de l’état, conduisant par exemple celui-ci à mettre en place des politiques publiques qui privilégient ces lieux en matière de déploiement d’infrastructures (écoles, lycées, cliniques, …) ce qui peut attirer des populations plus riches ou plus éduquées, dotées d’un meilleur équipement.\n\nsel$URBA<-as.factor(sel$URBA)\nlevels(sel$URBA)<-c(\"Rural\",\"Urbain\")\nX<-sel$URBA\nY<-sel$equip\nnameX<-\"Catégorisation administrative de l'arrondissement\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5341 -2.5341 -0.6171  1.3829 19.3829 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.61709    0.01551  233.22   <2e-16 ***\nXUrbain      1.91705    0.03268   58.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.19 on 54598 degrees of freedom\nMultiple R-squared:  0.0593,    Adjusted R-squared:  0.05928 \nF-statistic:  3442 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : La relation obtenue est très significative (p<0.001) et son pouvoir explicatif est plutôt élevé (r2 = 6%) bien qu’inférieur à celui du nveau d’étude.\n\n\n\nFonction administrative\nDans une logique légèrement différente, on peut se demander si les arrondissements qui remplissent des fonction de centralité administrative bénéficient d’un traitement privilégié de la part de l’état. Dans le cas du Bénin, tous les arrondissements qui sont centre de commune sont automatiquement déclarés urbains. Mais l’inverse n’est pas vrai et des arrondissements urbains peuvent ne pas être le siège d’un chef-lieu de commune ou de département.\n\nsel$CHEF<-as.factor(sel$COM_CHEF+sel$DEP_CHEF)\nlevels(sel$CHEF)<-c(\"Aucune\",\"Centre de commune\", \"Centre de département\")\nX<-sel$CHEF\nY<-sel$equip\nnameX<-\"Fonction administraive\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nboxplot(Y~X, \n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY,\n        col=\"lightyellow\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2175 -1.9261 -0.9261  1.0739 20.0739 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             3.92614    0.01509  260.27   <2e-16 ***\nXCentre de commune      0.60863    0.04324   14.08   <2e-16 ***\nXCentre de département  2.29136    0.09551   23.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.267 on 54597 degrees of freedom\nMultiple R-squared:  0.01335,   Adjusted R-squared:  0.01331 \nF-statistic: 369.4 on 2 and 54597 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : La relation obtenue est très significative (p<0.001) même si son pouvoir explicatif est plus limité (r2 = 1.3%) que celui de l’opposition urbain-rural . On note un net accroissement du niveau d’équipement des ménages avec le niveau administraif de l’arrondissement où ils sont localisés (+0.6 pour les ménages résidant dans un chef lieu de commune et +2.3 pourceux qui sont localisés dans un chef lieu de département)\n\n\n\nAccessibilité\nLes déterminants spatiaux correspondent au cas où la variable explicative est de type quantitatif continu et où l’effet sur le ménage est lié à son accessibilité à une ressource c’est-à-dire à l’influence de la distance.\n\npar(mfrow=c(1,3))\nmypal<-brewer.pal(8,\"RdYlGn\")\n\n## Distance à la commune\nmybreaks=c(0,1,2,4,8,16,32,64,128)\nmf_map(map3,\n       type=\"choro\",\n       var=\"COM_CHEF_DIST\",\n       pal=mypal,\n       breaks=mybreaks,\n        border=\"gray\",\n       lwd=0.3,\n       leg_title = \"Distance(km)\",\n       leg_val_rnd = 0)\n\n\nmf_map(map2,type=\"base\",col=NA, border=\"black\",lwd=2, add=T)\nmf_map(map2ctr,type=\"base\",pch=20,col=\"black\",add=T)\nmf_label(map2ctr,var = \"COM_NAME\", col=\"black\",pos=1,cex=0.6)\n#mf_map(map1ctr,type=\"base\",pch=20,col=\"black\",add=T, cex=1)\n#mf_label(map1ctr,var = \"DEP_NAME\", col=\"black\",pos=1)\nmf_layout(title = \"Accessibilité Communale\",\n          credits = \"Source : EE CIST 2023\",frame = T)\n\n\n## Distance au dept\nmybreaks=c(0,4,8,16,32,64,128,256,512)\nmf_map(map3,\n       type=\"choro\",\n       var=\"DEP_CHEF_DIST\",\n       pal=mypal,\n       breaks=mybreaks,\n        border=\"gray\",\n       lwd=1,\n       leg_title = \"Distance (km)\",\n       leg_val_rnd = 0)\n\n#mf_map(map2,type=\"base\",col=NA, border=\"black\",lwd=1, add=T)\nmf_map(map1,type=\"base\",col=NA, border=\"black\",lwd=2, add=T)\nmf_map(map1ctr,type=\"base\",pch=15,col=\"black\",add=T, cex=1)\nmf_label(map1ctr,var = \"DEP_NAME\", col=\"black\",pos=1)\nmf_layout(title = \"Accessibilité Départementale\",\n          credits = \"Source : EE CIST 2023\",frame = T)\n\n## Distance à la capitale nationale\nmap0ctr<-st_centroid(map3[map3$NAT_CHEF_DIST==0,])\n\n\nmybreaks=c(0,8,16,32,64,128,256,512,1024)\nmf_map(map3,\n       type=\"choro\",\n       var=\"NAT_CHEF_DIST\",\n       pal=mypal,\n       breaks=mybreaks,\n        border=\"gray\",\n       lwd=1,\n       leg_title = \"Distance (km)\",\n       leg_val_rnd = 0)\n\n#mf_map(map2,type=\"base\",col=NA, border=\"black\",lwd=1, add=T)\nmf_map(map0,type=\"base\",col=NA, border=\"black\",lwd=2, add=T)\nmf_map(map0ctr,type=\"base\",pch=15,col=\"black\",add=T, cex=2)\nmf_label(map0ctr,var = \"COM_NAME\", col=\"black\",pos=1)\nmf_layout(title = \"Accessibilité Nationale\",\n          credits = \"Source : EE CIST 2023\",frame = T)\n\n\n\n\n\n\n\nAccessibilité communale\nOn utilise une fonction racine carré pour modéliser l’effet de la distance. Pour une distance égale à zéro, la constante donnera l’effet du chefl-lieu de commune.\n\nsel$ACC_COM<-sqrt(sel$COM_CHEF_DIST)\nX<-sel$ACC_COM\nY<-sel$equip\nnameX<-\"Distance au centre de la commune (racine carrée)\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nplot(X,Y ,\n     pch=20,\n     cex=0.4,\n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY)\nabline(mod, col=\"red\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.838 -2.245 -0.773  1.417 20.247 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.838526   0.028328  170.81   <2e-16 ***\nX           -0.257530   0.008043  -32.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.258 on 54598 degrees of freedom\nMultiple R-squared:  0.01843,   Adjusted R-squared:  0.01842 \nF-statistic:  1025 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : La relation obtenue est très significative (p<0.001) même si son pouvoir explicatif est plus limité (r2 = 1.8%). Ceci s’explique tout d’abord par l’incertitude de mesure : la distance utilisée est l’éloignement entre les arrondissements et non pas entre l’individu et le centre de sa commune. Mais aussi par la diversité des ménages en matière d’équipement quelle que soit la distance au centre.\n\n\n\nAccessibilité départementale\n\nsel$ACC_DEP<-sqrt(sel$DEP_CHEF_DIST)\nX<-sel$ACC_DEP\nY<-sel$equip\nnameX<-\"Distance au centre du département (racine carrée)\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nplot(X,Y ,\n     pch=20,\n     cex=0.4,\n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY)\nabline(mod, col=\"red\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8445 -2.2733 -0.7454  1.4512 19.7786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.844517   0.035941  134.79   <2e-16 ***\nX           -0.147858   0.006152  -24.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.272 on 54598 degrees of freedom\nMultiple R-squared:  0.01047,   Adjusted R-squared:  0.01045 \nF-statistic: 577.6 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : La relation obtenue demeure très significative (p<0.001) mais son pouvoir explicatif demeure limité (r2 = 1.0%).\n\n\n\nAccessibilité Nationale\n\nsel$ACC_NAT<-sqrt(sel$NAT_CHEF_DIST)\nX<-sel$ACC_NAT\nY<-sel$equip\nnameX<-\"Distance à la capitale nationale (racine carrée)\"\nnameY<-\"Nombre d'équipements (0 à 24) \"\ntitre<-\"Equipement des ménages du Bénin en 2013\"\nsource<-\"Source : RP2013, INS Bénin\"\nmod<-lm(Y~X)\n\nplot(X,Y ,\n     pch=20,\n     cex=0.4,\n        main=titre,\n        sub = source,\n        xlab=nameX,\n        ylab=nameY)\nabline(mod, col=\"red\")\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4562 -2.2186 -0.7569  1.3006 20.1477 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.45623    0.03081  144.66   <2e-16 ***\nX           -0.03313    0.00223  -14.86   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.282 on 54598 degrees of freedom\nMultiple R-squared:  0.004025,  Adjusted R-squared:  0.004007 \nF-statistic: 220.7 on 1 and 54598 DF,  p-value: < 2.2e-16\n\n\n\nCommentaire : La relation obtenue est très significative (p<0.001) même si son pouvoir explicatif est très limité (r2 = 0.4%). Il faut toutefois examiner le rôle de ces variables d’accessibilité lorsqu’on les combine avec les autres variables relatives aux ménages.\n\n\n\nSYNTHESE\n\nVariables endogène\nOn en retient d’abord que les variables relatives à la structure du ménage et aux attributs du chef de ménage.\n\nmod1<-lm(data=sel, equip~log(tailmen)+statoc+sexe+age5+etud5)\n\n\nsummary(mod1)\n\n\nCall:\nlm(formula = equip ~ log(tailmen) + statoc + sexe + age5 + etud5, \n    data = sel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0754  -1.8455  -0.4329   1.3331  20.6140 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.67591    0.05409  30.982  < 2e-16 ***\nlog(tailmen)       0.88914    0.02415  36.820  < 2e-16 ***\nstatocPropFam     -0.55877    0.02692 -20.756  < 2e-16 ***\nstatocAutre       -0.42373    0.06566  -6.454 1.10e-10 ***\nstatocLocat        1.14917    0.05016  22.910  < 2e-16 ***\nsexeF             -0.83280    0.03520 -23.660  < 2e-16 ***\nage530-39          0.36677    0.03797   9.659  < 2e-16 ***\nage540-49          0.37145    0.04077   9.111  < 2e-16 ***\nage550-59          0.50248    0.04589  10.949  < 2e-16 ***\nage560+            0.28754    0.04506   6.381 1.78e-10 ***\netud52.Primaire    1.32687    0.03163  41.944  < 2e-16 ***\netud53.Secondaire  3.05137    0.03697  82.528  < 2e-16 ***\netud54.Supérieur   6.00856    0.07763  77.400  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.829 on 54587 degrees of freedom\nMultiple R-squared:  0.2604,    Adjusted R-squared:  0.2603 \nF-statistic:  1602 on 12 and 54587 DF,  p-value: < 2.2e-16\n\nAnova(mod1, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: equip\n             Sum Sq    Df F value    Pr(>F)    \n(Intercept)    7680     1  959.89 < 2.2e-16 ***\nlog(tailmen)  10847     1 1355.71 < 2.2e-16 ***\nstatoc        11767     3  490.21 < 2.2e-16 ***\nsexe           4479     1  559.80 < 2.2e-16 ***\nage5           1136     4   35.50 < 2.2e-16 ***\netud5         91359     3 3806.05 < 2.2e-16 ***\nResiduals    436762 54587                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCommentaire : la combinaison des déterminants sociaux aboutit à un modèle satisfaisant (r2=26%) où toutes les variables apportent une contribution significative au modèle. L’analyse de variance de type III (toutes choses égales par ailleurs) confirme le rôle central du niveau de diplôme du chef de ménage qui est de lion la variable qui contribue le plus à l’explication du niveau d’équipement. La taille du ménage joue également un rôle important ainsi que le statut d’occupation qui montre un meilleur équipement des locataires.L’influence de l’âge et du sexe est plus limitée.\n\n\n\nVariables contextuelles\n\nmod2<-lm(data=sel, equip~URBA+ACC_COM+ACC_DEP+ACC_NAT)\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = equip ~ URBA + ACC_COM + ACC_DEP + ACC_NAT, data = sel)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3603 -2.3168 -0.6018  1.4378 19.8409 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.636340   0.047536   76.50   <2e-16 ***\nURBAUrbain   2.428247   0.046287   52.46   <2e-16 ***\nACC_COM      0.221386   0.012150   18.22   <2e-16 ***\nACC_DEP     -0.077063   0.007326  -10.52   <2e-16 ***\nACC_NAT     -0.032419   0.002778  -11.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.174 on 54595 degrees of freedom\nMultiple R-squared:  0.06866,   Adjusted R-squared:  0.06859 \nF-statistic:  1006 on 4 and 54595 DF,  p-value: < 2.2e-16\n\nAnova(mod2, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: equip\n            Sum Sq    Df F value    Pr(>F)    \n(Intercept)  58954     1 5851.74 < 2.2e-16 ***\nURBA         27726     1 2752.11 < 2.2e-16 ***\nACC_COM       3345     1  331.99 < 2.2e-16 ***\nACC_DEP       1115     1  110.65 < 2.2e-16 ***\nACC_NAT       1372     1  136.15 < 2.2e-16 ***\nResiduals   550021 54595                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCommentaire :  le mélange des variables contextuelles est difficile à interpréter en raison des fortes redondances qu’il y a entre certaines variables explicatives. En effet la variable URBA est corrélée avec le fait d’être ou pas un centre de commune ou de département. MAlgré son pouvoir explicatif limité (r2=6.9%) le modèle met bien en valeur la combinaison des différents effets d’urbanisation et d’accessibilité. On note une inversion de l’effet d’accessibilité au chef lieu de commune qui s’explique par l’ajout des deux autres variables d’accessibilité.\n\n\n\nVariables endogènes et contextuelles\nOn construit un modèle final reprenant l’ensemble des variables endogènes mais en leur ajoutant les deux mesures d’accessibilité a au centre de commune ou de district.\n\nmod3<-lm(data=sel, equip~log(tailmen)+statoc+sexe+age5+etud5+URBA+ACC_COM+ACC_DEP+ACC_NAT)\nsummary(mod3)\n\n\nCall:\nlm(formula = equip ~ log(tailmen) + statoc + sexe + age5 + etud5 + \n    URBA + ACC_COM + ACC_DEP + ACC_NAT, data = sel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2365  -1.8215  -0.3934   1.3575  20.6885 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.605216   0.066453  24.155  < 2e-16 ***\nlog(tailmen)       0.988396   0.024501  40.342  < 2e-16 ***\nstatocPropFam     -0.585553   0.026798 -21.851  < 2e-16 ***\nstatocAutre       -0.537254   0.064720  -8.301  < 2e-16 ***\nstatocLocat        0.707877   0.050480  14.023  < 2e-16 ***\nsexeF             -0.922920   0.034873 -26.465  < 2e-16 ***\nage530-39          0.292758   0.037436   7.820 5.37e-15 ***\nage540-49          0.259819   0.040264   6.453 1.11e-10 ***\nage550-59          0.362171   0.045334   7.989 1.39e-15 ***\nage560+            0.147358   0.044516   3.310 0.000933 ***\netud52.Primaire    1.143840   0.031894  35.864  < 2e-16 ***\netud53.Secondaire  2.769215   0.037352  74.137  < 2e-16 ***\netud54.Supérieur   5.556467   0.077327  71.857  < 2e-16 ***\nURBAUrbain         1.496127   0.042144  35.500  < 2e-16 ***\nACC_COM            0.149466   0.010709  13.957  < 2e-16 ***\nACC_DEP           -0.056129   0.006468  -8.678  < 2e-16 ***\nACC_NAT           -0.028252   0.002533 -11.153  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.784 on 54583 degrees of freedom\nMultiple R-squared:  0.2837,    Adjusted R-squared:  0.2835 \nF-statistic:  1351 on 16 and 54583 DF,  p-value: < 2.2e-16\n\nAnova(mod3, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: equip\n             Sum Sq    Df  F value    Pr(>F)    \n(Intercept)    4522     1  583.488 < 2.2e-16 ***\nlog(tailmen)  12613     1 1627.460 < 2.2e-16 ***\nstatoc         7867     3  338.377 < 2.2e-16 ***\nsexe           5428     1  700.421 < 2.2e-16 ***\nage5            686     4   22.123 < 2.2e-16 ***\netud5         70895     3 3049.204 < 2.2e-16 ***\nURBA           9767     1 1260.251 < 2.2e-16 ***\nACC_COM        1510     1  194.785 < 2.2e-16 ***\nACC_DEP         584     1   75.309 < 2.2e-16 ***\nACC_NAT         964     1  124.385 < 2.2e-16 ***\nResiduals    423023 54583                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCommentaire : Par rapport au modèle purement endogène de modélistion des comportements individuels par les attributs des ménages, le gain est assez faible en terme de qualité d’ajustement (passage de 26% à 28.3%). C’est plutôt au niveau de l’interprétation que le nouveau modèle est intéressant car il révèle que le niveau d’étude se combine étroitement dans l’explication avec l’accessibilité aux centres de commune ou de districts. Ce qui laisse soupçonner une interaction entre les deux facteurs"
  }
]